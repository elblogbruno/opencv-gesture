<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- base href="http://historical.ncstrl.org/litesite-data/brown_cs/cs99-11.ps.Z" --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="Producer" content="GNU Ghostscript 7.07"><meta name="CreationDate" content="Mon Jun 21 10:11:11 US/Eastern 1999"><meta name="Creator" content="pscover 1.0"><title>A Survey of Hand Posture and Gesture Recognition Techniques and Technology</title></head><body bgcolor="#ffffff" vlink="blue" link="blue"><div style="border: 1px solid rgb(153, 153, 153); margin: -1px -1px 0pt; padding: 0pt; background: rgb(255, 255, 255) none repeat scroll 0% 0%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;"><div style="border: 1px solid rgb(153, 153, 153); margin: 12px; padding: 8px; background: rgb(221, 221, 221) none repeat scroll 0% 0%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial; font-family: arial,sans-serif; font-style: normal; font-variant: normal; font-size: 13px; line-height: normal; font-size-adjust: none; font-stretch: normal; -x-system-font: none; color: rgb(0, 0, 0); font-weight: normal; text-align: left;">这是 <a href="http://historical.ncstrl.org/litesite-data/brown_cs/cs99-11.ps.Z" style="text-decoration: underline; color: rgb(0, 0, 204);">http://historical.ncstrl.org/litesite-data/brown_cs/cs99-11.ps.Z</a> 的 HTML 档。<br> <b><font color="#0039b6">G</font> <font color="#c41200">o</font> <font color="#f3c518">o</font> <font color="#0039b6">g</font> <font color="#30a72f">l</font> <font color="#c41200">e</font></b> 在网路漫游时会自动将档案转换成 HTML 网页来储存。</div></div><div style="position: relative;">








<table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="1"><b>Page 1</b></a></font></td></tr></tbody></table><font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 660px; left: 295px;"><nobr>A Survey of Hand Posture and Gesture</nobr></div>
<div style="position: absolute; top: 678px; left: 293px;"><nobr>Recognition Techniques and Technology</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 723px; left: 388px;"><nobr>Joseph J. LaViola Jr.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 10px; font-family: Times;">
<div style="position: absolute; top: 768px; left: 361px;"><nobr>Department of Computer Science</nobr></div>
<div style="position: absolute; top: 786px; left: 413px;"><nobr>Brown University</nobr></div>
<div style="position: absolute; top: 804px; left: 365px;"><nobr>Providence, Rhode Island 02912</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 11px; font-family: Times;">
<div style="position: absolute; top: 846px; left: 436px;"><nobr>CS-99-11</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 10px; font-family: Times;">
<div style="position: absolute; top: 865px; left: 437px;"><nobr>June 1999</nobr></div>
</span></font>

<div style="position: absolute; top: 1363px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="2"><b>Page 2</b></a></font></td></tr></tbody></table></div>
<div style="position: absolute; top: 1363px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="3"><b>Page 3</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 1814px; left: 257px;"><nobr>A Survey of Hand Posture and Gesture</nobr></div>
<div style="position: absolute; top: 1847px; left: 248px;"><nobr>Recognition Techniques and Technology</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 1920px; left: 384px;"><nobr>Joseph J. LaViola Jr.</nobr></div>
<div style="position: absolute; top: 1941px; left: 395px;"><nobr>Brown University</nobr></div>
<div style="position: absolute; top: 1962px; left: 314px;"><nobr>NSF Science and Technology Center for</nobr></div>
<div style="position: absolute; top: 1983px; left: 287px;"><nobr>Computer Graphics and Scientific Visualization</nobr></div>
<div style="position: absolute; top: 2004px; left: 320px;"><nobr>Box 1910, Providence, RI 02912 USA</nobr></div>
<div style="position: absolute; top: 2025px; left: 395px;"><nobr>jjl@cs.brown.edu</nobr></div>
</span></font>

<div style="position: absolute; top: 2551px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="4"><b>Page 4</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 2959px; left: 431px;"><nobr><b>Abstract</b></nobr></div>
<div style="position: absolute; top: 2992px; left: 223px;"><nobr>This paper surveys the use of hand postures and gestures as a mechanism for inter-</nobr></div>
<div style="position: absolute; top: 3010px; left: 201px;"><nobr>action with computers, describing both the various techniques for performing accurate</nobr></div>
<div style="position: absolute; top: 3028px; left: 201px;"><nobr>recognition and the technological aspects inherent to posture- and gesture-based inter-</nobr></div>
<div style="position: absolute; top: 3046px; left: 201px;"><nobr>action. First, the technological requirements and limitations for using hand postures</nobr></div>
<div style="position: absolute; top: 3064px; left: 201px;"><nobr>and gestures are described by discussing both glove-based and vision-based recogni-</nobr></div>
<div style="position: absolute; top: 3082px; left: 201px;"><nobr>tion systems along with advantages and disadvantages of each. Second, the various</nobr></div>
<div style="position: absolute; top: 3100px; left: 201px;"><nobr>types of techniques used in recognizing hand postures and gestures are compared and</nobr></div>
<div style="position: absolute; top: 3118px; left: 201px;"><nobr>contrasted. Third, the applications that have used hand posture and gesture interfaces</nobr></div>
<div style="position: absolute; top: 3136px; left: 201px;"><nobr>are examined. The survey concludes with a summary and a discussion of future re-</nobr></div>
<div style="position: absolute; top: 3154px; left: 201px;"><nobr>search directions.</nobr></div>
</span></font>

<div style="position: absolute; top: 3739px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="5"><b>Page 5</b></a></font></td></tr></tbody></table></div><font size="5" face="Times"><span style="font-size: 35px; font-family: Times;">
<div style="position: absolute; top: 4025px; left: 201px;"><nobr><b>Contents</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 4139px; left: 201px;"><nobr><b>1 Introduction</b></nobr></div>
<div style="position: absolute; top: 4139px; left: 709px;"><nobr><b>3</b></nobr></div>
<div style="position: absolute; top: 4172px; left: 201px;"><nobr><b>2 Hand Posture and Gesture Recognition Technology</b></nobr></div>
<div style="position: absolute; top: 4172px; left: 709px;"><nobr><b>5</b></nobr></div>
<div style="position: absolute; top: 4190px; left: 223px;"><nobr>2.1 Data Collection for Hand Postures and Gestures . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4190px; left: 708px;"><nobr>5</nobr></div>
<div style="position: absolute; top: 4208px; left: 223px;"><nobr>2.2 Data Collection Using Instrumented Gloves and</nobr></div>
<div style="position: absolute; top: 4226px; left: 257px;"><nobr>Trackers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4226px; left: 708px;"><nobr>6</nobr></div>
<div style="position: absolute; top: 4244px; left: 257px;"><nobr>2.2.1 Tracking Devices . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4244px; left: 708px;"><nobr>6</nobr></div>
<div style="position: absolute; top: 4262px; left: 257px;"><nobr>2.2.2 Instrumented Gloves . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4262px; left: 708px;"><nobr>8</nobr></div>
<div style="position: absolute; top: 4280px; left: 223px;"><nobr>2.3 Vision-Based Technology . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4280px; left: 701px;"><nobr>16</nobr></div>
<div style="position: absolute; top: 4298px; left: 223px;"><nobr>2.4 Advantages and Disadvantages of Glove- and Vision-Based Data Col-</nobr></div>
<div style="position: absolute; top: 4316px; left: 257px;"><nobr>lection Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4316px; left: 701px;"><nobr>17</nobr></div>
<div style="position: absolute; top: 4348px; left: 201px;"><nobr><b>3 Algorithmic Techniques for Recognizing Hand Postures and Gestures</b></nobr></div>
<div style="position: absolute; top: 4348px; left: 701px;"><nobr><b>20</b></nobr></div>
<div style="position: absolute; top: 4367px; left: 223px;"><nobr>3.1 Feature Extraction, Statistics, and Models . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4367px; left: 701px;"><nobr>20</nobr></div>
<div style="position: absolute; top: 4385px; left: 257px;"><nobr>3.1.1 Template Matching . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4385px; left: 701px;"><nobr>21</nobr></div>
<div style="position: absolute; top: 4403px; left: 257px;"><nobr>3.1.2 Feature Extraction and Analysis . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4403px; left: 701px;"><nobr>23</nobr></div>
<div style="position: absolute; top: 4420px; left: 257px;"><nobr>3.1.3 Active Shape Models . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4420px; left: 701px;"><nobr>25</nobr></div>
<div style="position: absolute; top: 4438px; left: 257px;"><nobr>3.1.4 Principal Component Analysis . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4438px; left: 701px;"><nobr>26</nobr></div>
<div style="position: absolute; top: 4456px; left: 257px;"><nobr>3.1.5 Linear Fingertip Models . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4456px; left: 701px;"><nobr>27</nobr></div>
<div style="position: absolute; top: 4474px; left: 257px;"><nobr>3.1.6 Causal Analysis . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4474px; left: 701px;"><nobr>28</nobr></div>
<div style="position: absolute; top: 4492px; left: 223px;"><nobr>3.2 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4492px; left: 701px;"><nobr>30</nobr></div>
<div style="position: absolute; top: 4510px; left: 257px;"><nobr>3.2.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4510px; left: 701px;"><nobr>30</nobr></div>
<div style="position: absolute; top: 4528px; left: 257px;"><nobr>3.2.2 Hidden Markov Models . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4528px; left: 701px;"><nobr>33</nobr></div>
<div style="position: absolute; top: 4546px; left: 257px;"><nobr>3.2.3 Instance-Based Learning . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4546px; left: 701px;"><nobr>38</nobr></div>
<div style="position: absolute; top: 4564px; left: 223px;"><nobr>3.3 Miscellaneous Techniques . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4564px; left: 701px;"><nobr>40</nobr></div>
<div style="position: absolute; top: 4582px; left: 257px;"><nobr>3.3.1 The Linguistic Approach . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4582px; left: 701px;"><nobr>41</nobr></div>
<div style="position: absolute; top: 4600px; left: 257px;"><nobr>3.3.2 Appearance-Based Motion Analysis . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4600px; left: 701px;"><nobr>41</nobr></div>
<div style="position: absolute; top: 4618px; left: 257px;"><nobr>3.3.3 Spatio-Temporal Vector Analysis . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4618px; left: 701px;"><nobr>43</nobr></div>
<div style="position: absolute; top: 4650px; left: 201px;"><nobr><b>4 Applications Areas That Use Hand Postures and Gestures</b></nobr></div>
<div style="position: absolute; top: 4650px; left: 701px;"><nobr><b>45</b></nobr></div>
<div style="position: absolute; top: 4668px; left: 223px;"><nobr>4.1 Sign Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4668px; left: 701px;"><nobr>45</nobr></div>
<div style="position: absolute; top: 4686px; left: 223px;"><nobr>4.2 Gesture-to-Speech . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4686px; left: 701px;"><nobr>46</nobr></div>
<div style="position: absolute; top: 4704px; left: 223px;"><nobr>4.3 Presentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4704px; left: 701px;"><nobr>46</nobr></div>
<div style="position: absolute; top: 4722px; left: 223px;"><nobr>4.4 Virtual Environments . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 4722px; left: 701px;"><nobr>46</nobr></div>
<div style="position: absolute; top: 4779px; left: 455px;"><nobr>1</nobr></div>
</span></font>

<div style="position: absolute; top: 4927px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="6"><b>Page 6</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 5116px; left: 223px;"><nobr>4.5 3D Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5116px; left: 701px;"><nobr>47</nobr></div>
<div style="position: absolute; top: 5133px; left: 223px;"><nobr>4.6 Multimodal Interaction . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5133px; left: 701px;"><nobr>47</nobr></div>
<div style="position: absolute; top: 5151px; left: 223px;"><nobr>4.7 Human/Robot Manipulation and Instruction . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5151px; left: 701px;"><nobr>48</nobr></div>
<div style="position: absolute; top: 5169px; left: 223px;"><nobr>4.8 Television Control . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5169px; left: 701px;"><nobr>49</nobr></div>
<div style="position: absolute; top: 5201px; left: 201px;"><nobr><b>5 Conclusions</b></nobr></div>
<div style="position: absolute; top: 5201px; left: 701px;"><nobr><b>50</b></nobr></div>
<div style="position: absolute; top: 5234px; left: 201px;"><nobr><b>A Anatomy of the Human Hand</b></nobr></div>
<div style="position: absolute; top: 5234px; left: 701px;"><nobr><b>53</b></nobr></div>
<div style="position: absolute; top: 5253px; left: 223px;"><nobr>A.1 Hand and Finger Joints . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5253px; left: 701px;"><nobr>53</nobr></div>
<div style="position: absolute; top: 5271px; left: 223px;"><nobr>A.2 Hand Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5271px; left: 701px;"><nobr>54</nobr></div>
<div style="position: absolute; top: 5289px; left: 223px;"><nobr>A.3 Muscles and Tendons in the Hand . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5289px; left: 701px;"><nobr>54</nobr></div>
<div style="position: absolute; top: 5307px; left: 223px;"><nobr>A.4 Importance of the Hand’s Anatomy . . . . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5307px; left: 701px;"><nobr>54</nobr></div>
<div style="position: absolute; top: 5325px; left: 223px;"><nobr>A.5 Hand Models Used in Posture and Gesture Recognition . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5325px; left: 701px;"><nobr>55</nobr></div>
<div style="position: absolute; top: 5357px; left: 201px;"><nobr><b>B Hand Posture and Gesture Classification</b></nobr></div>
<div style="position: absolute; top: 5357px; left: 701px;"><nobr><b>59</b></nobr></div>
<div style="position: absolute; top: 5375px; left: 223px;"><nobr>B.1 Sturman’s Whole Hand Input Taxonomy . . . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5375px; left: 701px;"><nobr>59</nobr></div>
<div style="position: absolute; top: 5393px; left: 223px;"><nobr>B.2 Nespoulous and Lecours’ Gesture Taxonomy . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5393px; left: 701px;"><nobr>61</nobr></div>
<div style="position: absolute; top: 5411px; left: 223px;"><nobr>B.3 MIT AHIG’s Gesture Classification System . . . . . . . . . . . . . .</nobr></div>
<div style="position: absolute; top: 5411px; left: 701px;"><nobr>62</nobr></div>
<div style="position: absolute; top: 5967px; left: 455px;"><nobr>2</nobr></div>
</span></font>

<div style="position: absolute; top: 6115px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="7"><b>Page 7</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 6418px; left: 201px;"><nobr><b>1</b></nobr></div>
</span></font>
<font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 6423px; left: 239px;"><nobr><b>Introduction</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 6521px; left: 201px;"><nobr>The purpose of this document is to provide a broad introductionto the field of hand pos-</nobr></div>
<div style="position: absolute; top: 6548px; left: 201px;"><nobr>ture and gesture recognition as a mechanism for interaction with computers. It presents</nobr></div>
<div style="position: absolute; top: 6575px; left: 201px;"><nobr>an extensive survey of all the problems and issues relevant to using hand postures and</nobr></div>
<div style="position: absolute; top: 6602px; left: 201px;"><nobr>gestures in user interfaces, consolidating existing information in the field and organiz-</nobr></div>
<div style="position: absolute; top: 6629px; left: 201px;"><nobr>ing it in a clear and efficient manner. It also gives a critical review of the information</nobr></div>
<div style="position: absolute; top: 6656px; left: 201px;"><nobr>presented so as to point out the general advantages and disadvantages of the various</nobr></div>
<div style="position: absolute; top: 6682px; left: 201px;"><nobr>recognition techniques and systems.</nobr></div>
<div style="position: absolute; top: 6709px; left: 223px;"><nobr>Although other surveys have been written on various subsets of hand posture and</nobr></div>
<div style="position: absolute; top: 6736px; left: 201px;"><nobr>gesture recognition[99][112], this one is more comprehensive and up-to-date. It is</nobr></div>
<div style="position: absolute; top: 6763px; left: 201px;"><nobr>intended to be a starting point for anyone interested in using hand postures and gestures</nobr></div>
<div style="position: absolute; top: 6790px; left: 201px;"><nobr>in their interfaces and to give researchers a starting point for exploring the many open</nobr></div>
<div style="position: absolute; top: 6817px; left: 201px;"><nobr>research issues.</nobr></div>
<div style="position: absolute; top: 6844px; left: 223px;"><nobr>Although hand postures and gestures are often considered identical, there are dis-</nobr></div>
<div style="position: absolute; top: 6871px; left: 201px;"><nobr>tinctions between them. A hand posture is defined as a static movement. For exam-</nobr></div>
<div style="position: absolute; top: 6898px; left: 201px;"><nobr>ple, making a fist and holding it in a certain position is considered a posture. With</nobr></div>
<div style="position: absolute; top: 6925px; left: 201px;"><nobr>a simple posture, each of the fingers is either extended or flexed but not in between;</nobr></div>
<div style="position: absolute; top: 6951px; left: 201px;"><nobr>for example a fist, pointing, and thumb’s up. With a complex posture, the fingers can</nobr></div>
<div style="position: absolute; top: 6978px; left: 201px;"><nobr>be bent at angles other than zero or ninety degrees. Complex postures include various</nobr></div>
<div style="position: absolute; top: 7005px; left: 201px;"><nobr>forms of pinching, the “okay” sign and many of the postures used in finger spelling[77].</nobr></div>
<div style="position: absolute; top: 7032px; left: 223px;"><nobr>A gesture is defined as a dynamic movement, such as waving good-bye. Simple</nobr></div>
<div style="position: absolute; top: 7059px; left: 201px;"><nobr>gestures are made in two ways. The first way involves a simple or complex posture</nobr></div>
<div style="position: absolute; top: 7086px; left: 201px;"><nobr>and change in the position or orientation of the hand, such as making a pinching pos-</nobr></div>
<div style="position: absolute; top: 7155px; left: 455px;"><nobr>3</nobr></div>
</span></font>

<div style="position: absolute; top: 7303px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="8"><b>Page 8</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 7492px; left: 201px;"><nobr>ture and changing the hand’s position. The second way entails moving the fingers in</nobr></div>
<div style="position: absolute; top: 7518px; left: 201px;"><nobr>some way with no change in the position and orientation of the hand, for example,</nobr></div>
<div style="position: absolute; top: 7545px; left: 201px;"><nobr>moving the index and middle finger back and forth to urge someone to move closer. A</nobr></div>
<div style="position: absolute; top: 7572px; left: 201px;"><nobr>complex gesture is one that includes finger movement, wrist movement and changes in</nobr></div>
<div style="position: absolute; top: 7599px; left: 201px;"><nobr>the hand’s position and orientation. Many of the signs in American Sign Language are</nobr></div>
<div style="position: absolute; top: 7626px; left: 201px;"><nobr>examples of this type of gesture.</nobr></div>
<div style="position: absolute; top: 7653px; left: 223px;"><nobr>An important criterion in discussing this interaction paradigm is the number of</nobr></div>
<div style="position: absolute; top: 7680px; left: 201px;"><nobr>postures and gestures that a given recognition system or algorithmic technique can</nobr></div>
<div style="position: absolute; top: 7706px; left: 201px;"><nobr>accurately recognize</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 7705px; left: 323px;"><nobr>1</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 7706px; left: 329px;"><nobr>. In this paper, 1 to 15 postures and gestures is considered a small</nobr></div>
<div style="position: absolute; top: 7733px; left: 201px;"><nobr>set, 15 to 25 is medium-sized, and anything over 25 is considered large.</nobr></div>
<div style="position: absolute; top: 7760px; left: 223px;"><nobr>The remainder of this survey is divided into five main parts. The first part, Section</nobr></div>
<div style="position: absolute; top: 7787px; left: 201px;"><nobr>2, talks about the various aspects of hand posture and gesture recognition technology.</nobr></div>
<div style="position: absolute; top: 7814px; left: 201px;"><nobr>It discusses a number of current glove-based input devices and the advantages and dis-</nobr></div>
<div style="position: absolute; top: 7841px; left: 201px;"><nobr>advantages of each. It also describes aspects of vision-based recognition systems and</nobr></div>
<div style="position: absolute; top: 7868px; left: 201px;"><nobr>compares and contrasts both vision- and glove-based systems. Section 3 describes the</nobr></div>
<div style="position: absolute; top: 7895px; left: 201px;"><nobr>various algorithms used in hand posture and gesture recognition and discusses the ad-</nobr></div>
<div style="position: absolute; top: 7922px; left: 201px;"><nobr>vantages and disadvantages of each. Section 4 describes the many applications that</nobr></div>
<div style="position: absolute; top: 7949px; left: 201px;"><nobr>have used hand postures and gestures in their interfaces. Section 5 presents conclu-</nobr></div>
<div style="position: absolute; top: 7975px; left: 201px;"><nobr>sions and areas of future research. As an addendum, two appendices are also provided</nobr></div>
<div style="position: absolute; top: 8002px; left: 201px;"><nobr>which describe the basic anatomical structure of the hand and the various classification</nobr></div>
<div style="position: absolute; top: 8029px; left: 201px;"><nobr>systems and taxonomies used in describing hand postures and gestures. The purpose of</nobr></div>
<div style="position: absolute; top: 8056px; left: 201px;"><nobr>these appendices is to provide the reader with some terminology that frequently occurs</nobr></div>
<div style="position: absolute; top: 8083px; left: 201px;"><nobr>in the literature.</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 8236px; left: 217px;"><nobr>1</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 8237px; left: 222px;"><nobr>Unfortunately, accuracyis a relative measure. One definition of how accuratea recognition system is can</nobr></div>
<div style="position: absolute; top: 8259px; left: 201px;"><nobr>be entirely different from another. For the purposes of this paper, techniques and algorithms with accuracy</nobr></div>
<div style="position: absolute; top: 8280px; left: 201px;"><nobr>measures over 90% can generally be considered accurate. However, whether such a system is usable is</nobr></div>
<div style="position: absolute; top: 8301px; left: 201px;"><nobr>subject to debate.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 8343px; left: 455px;"><nobr>4</nobr></div>
</span></font>

<div style="position: absolute; top: 8491px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="9"><b>Page 9</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 8794px; left: 201px;"><nobr><b>2</b></nobr></div>
</span></font>
<font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 8799px; left: 239px;"><nobr><b>Hand Posture and Gesture Recognition</b></nobr></div>
<div style="position: absolute; top: 8849px; left: 201px;"><nobr><b>Technology</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 8946px; left: 223px;"><nobr>This section discusses the requirements for hand posture and gesture recognition. It</nobr></div>
<div style="position: absolute; top: 8973px; left: 201px;"><nobr>describes the twomain solutionsfor collecting the required data to perform recognition,</nobr></div>
<div style="position: absolute; top: 9000px; left: 201px;"><nobr>the glove-based solution and the camera- or vision-based solution, and looks at the</nobr></div>
<div style="position: absolute; top: 9027px; left: 201px;"><nobr>advantages and disadvantages of each.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 9060px; left: 201px;"><nobr><b>2.1 Data Collection for Hand Postures and Gestures</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 9110px; left: 201px;"><nobr>The first step in using hand posture and gestures in computer applications is gathering</nobr></div>
<div style="position: absolute; top: 9137px; left: 201px;"><nobr>raw data. This raw data is then analyzed by using various recognition algorithms (see</nobr></div>
<div style="position: absolute; top: 9163px; left: 201px;"><nobr>Section 3) to extract meaning or context from the data in order to perform tasks in the</nobr></div>
<div style="position: absolute; top: 9190px; left: 201px;"><nobr>application. Raw data is collected in three ways. The first is to use input devices worn</nobr></div>
<div style="position: absolute; top: 9217px; left: 201px;"><nobr>by the user. This setup usually consists of one or two instrumented gloves that measure</nobr></div>
<div style="position: absolute; top: 9244px; left: 201px;"><nobr>the various jointangles of the hand and a six degree of freedom (6 DOF) tracking device</nobr></div>
<div style="position: absolute; top: 9271px; left: 201px;"><nobr>that gathers hand position and orientation data. The second way to collect raw hand</nobr></div>
<div style="position: absolute; top: 9298px; left: 201px;"><nobr>data is to use a computer-vision-based approach by which one or more cameras collect</nobr></div>
<div style="position: absolute; top: 9325px; left: 201px;"><nobr>images of the user’s hands. The cameras grab an arbitrary number of images per second</nobr></div>
<div style="position: absolute; top: 9352px; left: 201px;"><nobr>and send them to image processing routines to perform posture and gesture recognition</nobr></div>
<div style="position: absolute; top: 9379px; left: 201px;"><nobr>as well as 3D triangulationto find the hands’ position in space. The third way to collect</nobr></div>
<div style="position: absolute; top: 9406px; left: 201px;"><nobr>raw hand data is to combine the previous two methods in a hybrid approach with the</nobr></div>
<div style="position: absolute; top: 9432px; left: 201px;"><nobr>hope of achieving a more accurate level of recognition by using the two data streams</nobr></div>
<div style="position: absolute; top: 9459px; left: 201px;"><nobr>to reduce each other’s error. Very little work has been done on hybrid tracking for</nobr></div>
<div style="position: absolute; top: 9486px; left: 201px;"><nobr>hand posture and gesture recognition, but this type of tracking has been successful in</nobr></div>
<div style="position: absolute; top: 9531px; left: 455px;"><nobr>5</nobr></div>
</span></font>

<div style="position: absolute; top: 9679px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="10"><b>Page 10</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 9868px; left: 201px;"><nobr>augmented reality systems like Auer[7] and State[98], and could well be applied to</nobr></div>
<div style="position: absolute; top: 9894px; left: 201px;"><nobr>hand posture and gesture recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 9951px; left: 201px;"><nobr><b>2.2 Data Collection Using Instrumented Gloves and</b></nobr></div>
<div style="position: absolute; top: 9992px; left: 249px;"><nobr><b>Trackers</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 10041px; left: 201px;"><nobr>Raw data collection using instrumented gloves and trackers requires users to physically</nobr></div>
<div style="position: absolute; top: 10068px; left: 201px;"><nobr>attach computer input devices to their hands. The instrumented gloves report data</nobr></div>
<div style="position: absolute; top: 10095px; left: 201px;"><nobr>values for the movement of the fingers; the amount of reported data values depends on</nobr></div>
<div style="position: absolute; top: 10122px; left: 201px;"><nobr>the type of glove worn. The trackers are attached to the back of the hand or the upper</nobr></div>
<div style="position: absolute; top: 10148px; left: 201px;"><nobr>wrist, depending on the type of glove worn, and give back data on the position and</nobr></div>
<div style="position: absolute; top: 10175px; left: 201px;"><nobr>orientation of the hand in 3D space.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 10225px; left: 201px;"><nobr><b>2.2.1 Tracking Devices</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 10266px; left: 201px;"><nobr>A number of different tracking technologies are available to track hand position and</nobr></div>
<div style="position: absolute; top: 10292px; left: 201px;"><nobr>orientation. This survey touches on the most common; for a detailed discussion see</nobr></div>
<div style="position: absolute; top: 10319px; left: 201px;"><nobr>Youngblut’s review of virtual environment interface technology[120], Encarnaç˜ao’s</nobr></div>
<div style="position: absolute; top: 10346px; left: 201px;"><nobr>survey on inputtechnology[31] or Mulder’s survey on human movement technology[75].</nobr></div>
<div style="position: absolute; top: 10373px; left: 201px;"><nobr>These three papers present a very thorough analysis of over 25 different tracking de-</nobr></div>
<div style="position: absolute; top: 10400px; left: 201px;"><nobr>vices on the market today.</nobr></div>
<div style="position: absolute; top: 10427px; left: 223px;"><nobr>Three non-vision-based methods for tracking hand position and orientation are</nobr></div>
<div style="position: absolute; top: 10454px; left: 201px;"><nobr>magnetic, acoustic, and inertial tracking. With magnetic tracking, a transmitting de-</nobr></div>
<div style="position: absolute; top: 10481px; left: 201px;"><nobr>vice emits a low-frequency magnetic field from which a small sensor, the receiver,</nobr></div>
<div style="position: absolute; top: 10508px; left: 201px;"><nobr>determines its position and orientation relative to a magnetic source. The advantages</nobr></div>
<div style="position: absolute; top: 10535px; left: 201px;"><nobr>of these types of systems are that they have good range, anywhere from fifteen to thirty</nobr></div>
<div style="position: absolute; top: 10562px; left: 201px;"><nobr>feet away if some extended range hardware is used, are generally accurate to within</nobr></div>
<div style="position: absolute; top: 10588px; left: 201px;"><nobr>0.1 inches in position and 0.1 degrees in orientation, and are moderately priced [6][86].</nobr></div>
<div style="position: absolute; top: 10615px; left: 201px;"><nobr>Their main disadvantage is that any ferromagnetic or conductive objects present in the</nobr></div>
<div style="position: absolute; top: 10642px; left: 201px;"><nobr>room with the transmitter will distort the magnetic field reducing the accuracy. The</nobr></div>
<div style="position: absolute; top: 10669px; left: 201px;"><nobr>distortion can be handled with filtering algorithms, but doing so introduces a more</nobr></div>
<div style="position: absolute; top: 10719px; left: 455px;"><nobr>6</nobr></div>
</span></font>

<div style="position: absolute; top: 10867px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="11"><b>Page 11</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 11056px; left: 201px;"><nobr>complex computational component and increases latency. The two most commonly</nobr></div>
<div style="position: absolute; top: 11082px; left: 201px;"><nobr>used magnetic trackers today are from Polhemus and Ascension Technology Corpora-</nobr></div>
<div style="position: absolute; top: 11109px; left: 201px;"><nobr>tion.</nobr></div>
<div style="position: absolute; top: 11136px; left: 223px;"><nobr>Acoustic tracking systems or ultrasonic tracking uses high-frequency sound emitted</nobr></div>
<div style="position: absolute; top: 11163px; left: 201px;"><nobr>from a source component that is placed on the hand or area to be tracked. Microphones</nobr></div>
<div style="position: absolute; top: 11190px; left: 201px;"><nobr>placed in the environment receive ultrasonic pings from the source components to de-</nobr></div>
<div style="position: absolute; top: 11217px; left: 201px;"><nobr>termine their location and orientation[99]. In most cases, the microphones are placed in</nobr></div>
<div style="position: absolute; top: 11244px; left: 201px;"><nobr>a triangular array and this region determines the area of tracked space. The advantages</nobr></div>
<div style="position: absolute; top: 11270px; left: 201px;"><nobr>of acoustic tracking systems are that they are relatively inexpensive and lightweight.</nobr></div>
<div style="position: absolute; top: 11297px; left: 201px;"><nobr>However, these devices have a short range and their accuracy suffers if acoustically</nobr></div>
<div style="position: absolute; top: 11324px; left: 201px;"><nobr>reflective surfaces are present in the room. Another disadvantage of acoustic tracking</nobr></div>
<div style="position: absolute; top: 11351px; left: 201px;"><nobr>is that external noises such as jingling keys or a ringing phone can cause interference</nobr></div>
<div style="position: absolute; top: 11378px; left: 201px;"><nobr>in the tracking signal and thus reduce accuracy. Logitech’s acoustic tracking systems</nobr></div>
<div style="position: absolute; top: 11405px; left: 201px;"><nobr>seem to be the most commonly used; however, some newer companies like Freepoint</nobr></div>
<div style="position: absolute; top: 11432px; left: 201px;"><nobr>3D have entered this field[31]. Acoustic tracking has also been incorporated into some</nobr></div>
<div style="position: absolute; top: 11459px; left: 201px;"><nobr>glove-based devices such as the Mattel Power Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 11457px; left: 527px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 11459px; left: 549px;"><nobr>[99] and VPL’s Z-Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 11457px; left: 699px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 11486px; left: 201px;"><nobr>[121], discussed in further detail in Section 2.2.2.</nobr></div>
<div style="position: absolute; top: 11513px; left: 223px;"><nobr>Finally, inertial tracking systems use a variety of inertial measurement devices such</nobr></div>
<div style="position: absolute; top: 11539px; left: 201px;"><nobr>as gyroscopes, servo-accelerometers, and even micromachined quartz tuning forks that</nobr></div>
<div style="position: absolute; top: 11566px; left: 201px;"><nobr>sense angular velocity using the Coriolis principle[31]. The advantages of an iner-</nobr></div>
<div style="position: absolute; top: 11593px; left: 201px;"><nobr>tial tracking system is speed, accuracy and range, but the major problems with these</nobr></div>
<div style="position: absolute; top: 11620px; left: 201px;"><nobr>systems are that they usually only track three degrees of freedom (either positionor ori-</nobr></div>
<div style="position: absolute; top: 11647px; left: 201px;"><nobr>entation data) and they suffer from gyroscopic drift. The most commonly used inertial</nobr></div>
<div style="position: absolute; top: 11674px; left: 201px;"><nobr>tracking systems are InterSense’s IS-300 and IS-600. The IS-300 measures only orien-</nobr></div>
<div style="position: absolute; top: 11701px; left: 201px;"><nobr>tation data but uses gravitometer and compass measurements to prevent accumulation</nobr></div>
<div style="position: absolute; top: 11728px; left: 201px;"><nobr>of gyroscopic drift and employs a motion prediction mechanism that predicts motion</nobr></div>
<div style="position: absolute; top: 11755px; left: 201px;"><nobr>up to 50 milliseconds in the future. The IS-600 tracks both position and orientation but</nobr></div>
<div style="position: absolute; top: 11782px; left: 201px;"><nobr>requires an additional ultrasonic component to acquire the position data[46].</nobr></div>
<div style="position: absolute; top: 11809px; left: 223px;"><nobr>A common problem with these tracking devices is that they do not have perfect ac-</nobr></div>
<div style="position: absolute; top: 11835px; left: 201px;"><nobr>curacy. A promising way of achieving greater accuracy is to use prediction/correction</nobr></div>
<div style="position: absolute; top: 11862px; left: 201px;"><nobr>techniques to filter the position and orientation data. One of the most widely used fil-</nobr></div>
<div style="position: absolute; top: 11907px; left: 455px;"><nobr>7</nobr></div>
</span></font>

<div style="position: absolute; top: 12055px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="12"><b>Page 12</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 12244px; left: 201px;"><nobr>tering techniques is the Kalman filter, a recursive mathematical procedure that uses the</nobr></div>
<div style="position: absolute; top: 12270px; left: 201px;"><nobr>predictor/corrector mechanism for least-squares estimation for linear systems. Welch</nobr></div>
<div style="position: absolute; top: 12297px; left: 201px;"><nobr>and Bishop[114] and Maybeck[70] both provide detailed discussions and mathemat-</nobr></div>
<div style="position: absolute; top: 12324px; left: 201px;"><nobr>ical derivations of the Kalman filter for the interested reader. Kalman filtering can</nobr></div>
<div style="position: absolute; top: 12351px; left: 201px;"><nobr>be applied to tracking devices, vision tracking[10], and hybrid tracking systems as</nobr></div>
<div style="position: absolute; top: 12378px; left: 201px;"><nobr>well[113].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 12428px; left: 201px;"><nobr><b>2.2.2 Instrumented Gloves</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 12468px; left: 201px;"><nobr>Instrumented gloves measure finger movement through various kinds of sensor tech-</nobr></div>
<div style="position: absolute; top: 12495px; left: 201px;"><nobr>nology</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 12494px; left: 243px;"><nobr>1</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 12495px; left: 248px;"><nobr>. These sensors are embedded in a glove or placed on it, usually on the back of</nobr></div>
<div style="position: absolute; top: 12522px; left: 201px;"><nobr>the hand. Glove-based input devices can be broadly divided into those gloves that are</nobr></div>
<div style="position: absolute; top: 12549px; left: 201px;"><nobr>available in the marketplace today and those that are not, either because their respective</nobr></div>
<div style="position: absolute; top: 12576px; left: 201px;"><nobr>companies have gone out of business or because they were never developed commer-</nobr></div>
<div style="position: absolute; top: 12602px; left: 201px;"><nobr>cially. Both Sturman[101] and Kadous[48] discuss both categories of gloves, but their</nobr></div>
<div style="position: absolute; top: 12629px; left: 201px;"><nobr>surveys are know out of date. Encarnaç˜ao[31] and Youngblut’s[120] discussions of</nobr></div>
<div style="position: absolute; top: 12656px; left: 201px;"><nobr>glove input devices deal specifically with those currently available from commercial</nobr></div>
<div style="position: absolute; top: 12683px; left: 201px;"><nobr>vendors. The present survey gives both a historical perspective on these devices by de-</nobr></div>
<div style="position: absolute; top: 12710px; left: 201px;"><nobr>scribing those gloves that are no longer available and a practical guide to those gloves</nobr></div>
<div style="position: absolute; top: 12737px; left: 201px;"><nobr>that are on the market today.</nobr></div>
<div style="position: absolute; top: 12785px; left: 201px;"><nobr><b>Historical Perspective </b>One of the first instrumented gloves described in the litera-</nobr></div>
<div style="position: absolute; top: 12813px; left: 201px;"><nobr>ture was the ‘Sayre Glove’ developed by Thomas Defanti and Daniel Sandin in a 1977</nobr></div>
<div style="position: absolute; top: 12840px; left: 201px;"><nobr>project for the National Endowment of the Arts[26]. This glove used light-based sen-</nobr></div>
<div style="position: absolute; top: 12867px; left: 201px;"><nobr>sors with flexible tubes with a light source at one end and a photocell at the other. As</nobr></div>
<div style="position: absolute; top: 12893px; left: 201px;"><nobr>the fingers were bent, the amount of light that hit the photocells varied thus providing a</nobr></div>
<div style="position: absolute; top: 12920px; left: 201px;"><nobr>measure of finger flexion. The glove could measure the metacarpophalangeal joints of</nobr></div>
<div style="position: absolute; top: 12947px; left: 201px;"><nobr>the four fingers and thumb along with the proximal interphalangeal joints of the index</nobr></div>
<div style="position: absolute; top: 12974px; left: 201px;"><nobr>and middle fingers, for a total of 7 DOF (Figure A.1 shows a diagram of the joints of</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 13003px; left: 217px;"><nobr>1</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 13004px; left: 222px;"><nobr>The exceptionto this definition is the FakespacePinch</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 13003px; left: 479px;"><nobr>TM</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 13004px; left: 495px;"><nobr>Glove. Instead of measuring finger movement,</nobr></div>
<div style="position: absolute; top: 13025px; left: 201px;"><nobr>Pinch gloves detect electrical contact made when the fingertips touch.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 13095px; left: 455px;"><nobr>8</nobr></div>
</span></font>

<div style="position: absolute; top: 13243px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="13"><b>Page 13</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 13432px; left: 201px;"><nobr>the hand). It was designed for multidimensional control of sliders and other 2D wid-</nobr></div>
<div style="position: absolute; top: 13458px; left: 201px;"><nobr>gets and did not have the sophistication or accuracy needed for hand posture or gesture</nobr></div>
<div style="position: absolute; top: 13485px; left: 201px;"><nobr>recognition.</nobr></div>
<div style="position: absolute; top: 13512px; left: 223px;"><nobr>The Digital Data Entry Glove, designed by Gary Grimes at Bell Telephone Labo-</nobr></div>
<div style="position: absolute; top: 13539px; left: 201px;"><nobr>ratories in 1981, was invented specifically for performing manual data entry using the</nobr></div>
<div style="position: absolute; top: 13566px; left: 201px;"><nobr>Single-Hand Manual Alphabet[41]. It used touch or proximity sensors, “knuckle-bend</nobr></div>
<div style="position: absolute; top: 13593px; left: 201px;"><nobr>sensors”, tilt sensors, and inertial sensors to replace a traditional keyboard. The touch</nobr></div>
<div style="position: absolute; top: 13620px; left: 201px;"><nobr>or proximity sensors determined whether the user’s thumb was touching another part</nobr></div>
<div style="position: absolute; top: 13646px; left: 201px;"><nobr>of the hand or fingers. They were made of silver-filled conductive rubber pads that sent</nobr></div>
<div style="position: absolute; top: 13673px; left: 201px;"><nobr>an electrical signal when they made contact. The four knuckle-bend sensors measured</nobr></div>
<div style="position: absolute; top: 13700px; left: 201px;"><nobr>the flexion of the joints in the thumb, index finger, and pinkie finger. The two tilt sen-</nobr></div>
<div style="position: absolute; top: 13727px; left: 201px;"><nobr>sors measured the tilt of the hand in the horizontal plane, and the two inertial sensors</nobr></div>
<div style="position: absolute; top: 13754px; left: 201px;"><nobr>measured the twisting of the forearm and the flexing of the wrist. The drawback of</nobr></div>
<div style="position: absolute; top: 13781px; left: 201px;"><nobr>this glove was that it was developed for a specific task and the recognition of hand</nobr></div>
<div style="position: absolute; top: 13808px; left: 201px;"><nobr>signs was done strictly in hardware. Therefore, it was not generic enough to perform</nobr></div>
<div style="position: absolute; top: 13835px; left: 201px;"><nobr>robust hand posture or gesture recognition in any application other than entry of ASCII</nobr></div>
<div style="position: absolute; top: 13862px; left: 201px;"><nobr>characters.</nobr></div>
<div style="position: absolute; top: 13889px; left: 223px;"><nobr>The DataGlove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 13887px; left: 316px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 13889px; left: 339px;"><nobr>and Z-Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 13887px; left: 417px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 13889px; left: 441px;"><nobr>developed by VPL Research, were first pre-</nobr></div>
<div style="position: absolute; top: 13915px; left: 201px;"><nobr>sented at the Human Factors in Computing Systems and Graphics Interface conference</nobr></div>
<div style="position: absolute; top: 13942px; left: 201px;"><nobr>in 1987[121]. Both gloves were designed to be general-purpose interface devices for</nobr></div>
<div style="position: absolute; top: 13969px; left: 201px;"><nobr>applications that required direct object manipulation with the hand, finger spelling,</nobr></div>
<div style="position: absolute; top: 13996px; left: 201px;"><nobr>evaluation of hand impairment, and the like. Both gloves came equipped with five to</nobr></div>
<div style="position: absolute; top: 14023px; left: 201px;"><nobr>fifteen sensors (usually ten) that measured the flexion of both the metacarpophalangeal</nobr></div>
<div style="position: absolute; top: 14050px; left: 201px;"><nobr>joints and proximal interphalangeal joints of the four fingers and thumb for a total of 10</nobr></div>
<div style="position: absolute; top: 14077px; left: 201px;"><nobr>DOF. In some cases abduction sensors were used to measure angles between adjacent</nobr></div>
<div style="position: absolute; top: 14104px; left: 201px;"><nobr>fingers. Both gloves used optical goniometer sensor technology patented by Zimmer-</nobr></div>
<div style="position: absolute; top: 14131px; left: 201px;"><nobr>man in 1985. These sensors were made up of flexible tubes with a reflective interior</nobr></div>
<div style="position: absolute; top: 14158px; left: 201px;"><nobr>wall, a light source at one end and a photosensitive detector at the other that detected</nobr></div>
<div style="position: absolute; top: 14185px; left: 201px;"><nobr>both direct light rays and reflected light rays. Depending on the bending of the tubes,</nobr></div>
<div style="position: absolute; top: 14211px; left: 201px;"><nobr>the detector would change its electrical resistance as a function of light intensity[122].</nobr></div>
<div style="position: absolute; top: 14238px; left: 201px;"><nobr>The gloves also provided tactile feedback by putting piezoceramic benders underneath</nobr></div>
<div style="position: absolute; top: 14283px; left: 455px;"><nobr>9</nobr></div>
</span></font>

<div style="position: absolute; top: 14431px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="14"><b>Page 14</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 14620px; left: 201px;"><nobr>each finger which produced a tingling or numbing sensation. The main difference be-</nobr></div>
<div style="position: absolute; top: 14646px; left: 201px;"><nobr>tween the DataGlove and the Z-Glove was the position and orientation mechanisms</nobr></div>
<div style="position: absolute; top: 14673px; left: 201px;"><nobr>used with each. A traditional magnetic tracking system was used with the DataGlove,</nobr></div>
<div style="position: absolute; top: 14700px; left: 201px;"><nobr>while the Z-Glove had an embedded ultrasonic tracking system that placed two ultra-</nobr></div>
<div style="position: absolute; top: 14727px; left: 201px;"><nobr>sonic transducers on opposite sides of the metacarpals to measure the roll and yaw of</nobr></div>
<div style="position: absolute; top: 14754px; left: 201px;"><nobr>the hand. Generally the Z-Glove was much more limited in application and as a result</nobr></div>
<div style="position: absolute; top: 14781px; left: 201px;"><nobr>was less costly.</nobr></div>
<div style="position: absolute; top: 14808px; left: 223px;"><nobr>The DataGlove and Z-Glove were designed as general-purpose interface devices.</nobr></div>
<div style="position: absolute; top: 14834px; left: 201px;"><nobr>However, their lack of accuracy limited their utility: formal testing revealed the ac-</nobr></div>
<div style="position: absolute; top: 14861px; left: 201px;"><nobr>curacy of the sensors as no better than five to ten degrees of joint rotation[119]. The</nobr></div>
<div style="position: absolute; top: 14888px; left: 201px;"><nobr>gloves could have been used for simple posture recognition and object manipulation,</nobr></div>
<div style="position: absolute; top: 14915px; left: 201px;"><nobr>but they were generally not accurate enough for complex gesture recognition.</nobr></div>
<div style="position: absolute; top: 14942px; left: 223px;"><nobr>The Dexterous HandMaster</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 14941px; left: 388px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 14942px; left: 409px;"><nobr>(DHM), first developed in 1987 was an exoskeleton</nobr></div>
<div style="position: absolute; top: 14969px; left: 201px;"><nobr>that fit over the hand. Initially it was used as a master controller for the Utah/MIT</nobr></div>
<div style="position: absolute; top: 14996px; left: 201px;"><nobr>Dexterous Hand, a four-digit robot hand[67]. A second version of the device later</nobr></div>
<div style="position: absolute; top: 15023px; left: 201px;"><nobr>developed and marketed by Marcus[101] used a total of 20 Hall-Effect sensors as</nobr></div>
<div style="position: absolute; top: 15050px; left: 201px;"><nobr>potentiometers that measured the flexion of all three joints in each finger, abduc-</nobr></div>
<div style="position: absolute; top: 15077px; left: 201px;"><nobr>tion/adduction between each finger, and four degrees of freedom for the thumb. These</nobr></div>
<div style="position: absolute; top: 15103px; left: 201px;"><nobr>sensors were sampled at 200 Hz witheight bitaccuracy. It was very accurate</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 15102px; left: 649px;"><nobr>2</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 15103px; left: 656px;"><nobr>, witha 92</nobr></div>
<div style="position: absolute; top: 15130px; left: 201px;"><nobr>to 98 percent correlation between finger position and DHM readout[64], thus it could</nobr></div>
<div style="position: absolute; top: 15157px; left: 201px;"><nobr>have been used for complex posture and gesture recognition, but it took some time to</nobr></div>
<div style="position: absolute; top: 15184px; left: 201px;"><nobr>take on and off and was not suited for rapid movements because of its instability when</nobr></div>
<div style="position: absolute; top: 15211px; left: 201px;"><nobr>worn.</nobr></div>
<div style="position: absolute; top: 15238px; left: 223px;"><nobr>The Power Glove was developed in 1989 by Mattel as an input device for Nintendo</nobr></div>
<div style="position: absolute; top: 15265px; left: 201px;"><nobr>games and, when suitably reverse-engineered for a computer’s serial port[30], became</nobr></div>
<div style="position: absolute; top: 15292px; left: 201px;"><nobr>a low-cost alternative for researchers in virtual reality and hand posture and gesture</nobr></div>
<div style="position: absolute; top: 15319px; left: 201px;"><nobr>recognition[48][85]. The glove used resistive ink sensors that measured the overall</nobr></div>
<div style="position: absolute; top: 15346px; left: 201px;"><nobr>flexion of the thumb, index, middle, and ring fingers for a total of four DOF. It also</nobr></div>
<div style="position: absolute; top: 15373px; left: 201px;"><nobr>used ultrasonic tracking to track the hand’s <i>x</i>, <i>y</i>, and <i>z </i>position and roll orientation of</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 15401px; left: 217px;"><nobr>2</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 15402px; left: 222px;"><nobr>The device was designed mainly for clinical analysis of hand impairment and robot control.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 15471px; left: 451px;"><nobr>10</nobr></div>
</span></font>

<div style="position: absolute; top: 15619px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="15"><b>Page 15</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 15808px; left: 201px;"><nobr>the wrist relative to a companion unit attached to the display. Because the finger sensors</nobr></div>
<div style="position: absolute; top: 15834px; left: 201px;"><nobr>used two bits of precision, the Power Glove was not very accurate and useful only for</nobr></div>
<div style="position: absolute; top: 15861px; left: 201px;"><nobr>a small set of simple hand postures and gestures; its big advantage was its extremely</nobr></div>
<div style="position: absolute; top: 15888px; left: 201px;"><nobr>low cost.</nobr></div>
<div style="position: absolute; top: 15915px; left: 223px;"><nobr>Finally, the Space Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 15914px; left: 369px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 15915px; left: 392px;"><nobr>developed by W Industries in 1991, was unique in that</nobr></div>
<div style="position: absolute; top: 15942px; left: 201px;"><nobr>the user placed his fingers and thumb through plastic rings that sat between the proxi-</nobr></div>
<div style="position: absolute; top: 15969px; left: 201px;"><nobr>mal interphalangeals and the metacarpophalangeal joints. The glove used sensors with</nobr></div>
<div style="position: absolute; top: 15996px; left: 201px;"><nobr>twelve bit analog-to-digital converters that measured the flexion of the metacarpopha-</nobr></div>
<div style="position: absolute; top: 16022px; left: 201px;"><nobr>langeal joints and the interphalangeal joint of the thumb for a total of six DOF[99].</nobr></div>
<div style="position: absolute; top: 16049px; left: 201px;"><nobr>According to Sturman’s personal experience[101], the Space Glove was fairly respon-</nobr></div>
<div style="position: absolute; top: 16076px; left: 201px;"><nobr>sive but uncomfortable to wear due to the inflexibility of the plastic rings around the</nobr></div>
<div style="position: absolute; top: 16103px; left: 201px;"><nobr>fingers. The glove worked only with W Industries products and, as a result, little if any</nobr></div>
<div style="position: absolute; top: 16130px; left: 201px;"><nobr>work has been done with it.</nobr></div>
<div style="position: absolute; top: 16178px; left: 201px;"><nobr><b>Current Glove-Based Input Devices </b>One of the least expensive gloves on the mar-</nobr></div>
<div style="position: absolute; top: 16206px; left: 201px;"><nobr>ket today is the 5DT Data Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 16204px; left: 404px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 16206px; left: 426px;"><nobr>(see Figure 2.1) developed by Fifth Dimension</nobr></div>
<div style="position: absolute; top: 16233px; left: 201px;"><nobr>Technologies. This glove uses five fiber optic sensors to measure the overall flexion</nobr></div>
<div style="position: absolute; top: 16260px; left: 201px;"><nobr>of the four fingers and the thumb; according to the specifications[36], these sensors</nobr></div>
<div style="position: absolute; top: 16286px; left: 201px;"><nobr>can be sampled at 200 Hz with eight bits of precision. In addition, the glove uses two</nobr></div>
<div style="position: absolute; top: 16313px; left: 201px;"><nobr>tilt sensors to measure the pitch and roll of the hand. The device is currently priced at</nobr></div>
<div style="position: absolute; top: 16340px; left: 201px;"><nobr>$495 for a right-handed glove and $535 for a left-handed glove. Since this glove senses</nobr></div>
<div style="position: absolute; top: 16367px; left: 201px;"><nobr>only the average flexion of the four fingers and the thumb, it is not suited for complex</nobr></div>
<div style="position: absolute; top: 16394px; left: 201px;"><nobr>gesture or posture recognition. However, it does perform well enough for simple pos-</nobr></div>
<div style="position: absolute; top: 16421px; left: 201px;"><nobr>tures, such as pointing or making a fist, and is the supported device for General Reality</nobr></div>
<div style="position: absolute; top: 16448px; left: 201px;"><nobr>Company’s GloveGRASP</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 16447px; left: 356px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 16448px; left: 377px;"><nobr>software toolkit for hand posture recognition[39].</nobr></div>
<div style="position: absolute; top: 16475px; left: 223px;"><nobr>The SuperGlove (see Figure 2.2), developed by Nissho Electronics, has a minimum</nobr></div>
<div style="position: absolute; top: 16502px; left: 201px;"><nobr>of 10 and a maximum of 16 bend sensors that use a special resistive ink applied to flex-</nobr></div>
<div style="position: absolute; top: 16529px; left: 201px;"><nobr>ible boards sewn into the glove[82]. With its minimal and standard configuration, the</nobr></div>
<div style="position: absolute; top: 16556px; left: 201px;"><nobr>SuperGlove measures flexion of both the metacarpophalangeal and proximal interpha-</nobr></div>
<div style="position: absolute; top: 16582px; left: 201px;"><nobr>langeal joints for all four fingers and the thumb. The glove comes in two different sizes</nobr></div>
<div style="position: absolute; top: 16609px; left: 201px;"><nobr>and is available for both the left and right hand. A unique feature of this device is</nobr></div>
<div style="position: absolute; top: 16659px; left: 451px;"><nobr>11</nobr></div>
</span></font>

<div style="position: absolute; top: 16807px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="16"><b>Page 16</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 17337px; left: 201px;"><nobr>Figure 2.1: The 5DT Data Glove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 17336px; left: 398px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 17337px; left: 418px;"><nobr>developed by Fifth Dimension Technologies. The</nobr></div>
<div style="position: absolute; top: 17355px; left: 201px;"><nobr>glove measures seven DOF (from Fifth Dimension Technologies[36]).</nobr></div>
<div style="position: absolute; top: 17403px; left: 201px;"><nobr>its embedded calibration procedure: three buttons on the control unit can collect data</nobr></div>
<div style="position: absolute; top: 17430px; left: 201px;"><nobr>for three distinct postures to allow hardware calibration. The standard version of the</nobr></div>
<div style="position: absolute; top: 17457px; left: 201px;"><nobr>SuperGlove is currently priced at around $5000. A wireless option is also available</nobr></div>
<div style="position: absolute; top: 17484px; left: 201px;"><nobr>which increases the price to over $20,000; there are currently no distributors that sell</nobr></div>
<div style="position: absolute; top: 17511px; left: 201px;"><nobr>the device in the United States.</nobr></div>
<div style="position: absolute; top: 17538px; left: 223px;"><nobr>From the author’s personal experience, the SuperGlove is adequate for simple pos-</nobr></div>
<div style="position: absolute; top: 17565px; left: 201px;"><nobr>ture recognition; here a simple posture is a posture having a combination of the fingers</nobr></div>
<div style="position: absolute; top: 17592px; left: 201px;"><nobr>either flexed or extended. The glove’s hardware-based calibration mechanism is im-</nobr></div>
<div style="position: absolute; top: 17618px; left: 201px;"><nobr>portant and does not have to be used often, but it does not remove the need for software</nobr></div>
<div style="position: absolute; top: 17645px; left: 201px;"><nobr>calibration. The glove is fairly accurate but not suited for complex gesture recogni-</nobr></div>
<div style="position: absolute; top: 17672px; left: 201px;"><nobr>tion. Unfortunately, no formal studies have been performed on the accuracy of the</nobr></div>
<div style="position: absolute; top: 17699px; left: 201px;"><nobr>SuperGlove and it is not commonly discussed in the literature.</nobr></div>
<div style="position: absolute; top: 17726px; left: 223px;"><nobr>Pinch</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 17725px; left: 257px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 17726px; left: 276px;"><nobr>Gloves (see Figure 2.3) take a differentapproach to posture recognition[33].</nobr></div>
<div style="position: absolute; top: 17753px; left: 201px;"><nobr>These gloves, originally called Chord Gloves, were prototyped by Mapes at the Uni-</nobr></div>
<div style="position: absolute; top: 17780px; left: 201px;"><nobr>versity of Central Florida[66]; the technology was bought by Fakespace Inc. and now</nobr></div>
<div style="position: absolute; top: 17847px; left: 451px;"><nobr>12</nobr></div>
</span></font>

<div style="position: absolute; top: 17995px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="17"><b>Page 17</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 18514px; left: 201px;"><nobr>Figure 2.2: Nissho Electronics SuperGlove input device. This glove has a minimum of</nobr></div>
<div style="position: absolute; top: 18532px; left: 201px;"><nobr>10 bend sensors and a maximum of 16 (from Nissho Electronics Corporation[82]).</nobr></div>
<div style="position: absolute; top: 18580px; left: 201px;"><nobr>the gloves are sold commercially under the Pinch Glove name. Instead of using bend</nobr></div>
<div style="position: absolute; top: 18607px; left: 201px;"><nobr>sensor technology to record joint angles, Pinch Gloves have electrical contacts on the</nobr></div>
<div style="position: absolute; top: 18634px; left: 201px;"><nobr>inside of the tips of the four fingers and the thumb. Users can make a variety of postures</nobr></div>
<div style="position: absolute; top: 18661px; left: 201px;"><nobr>by completing a conductive path when two or more of the electrical contacts meet. Ac-</nobr></div>
<div style="position: absolute; top: 18688px; left: 201px;"><nobr>cording to Encarnaç˜ao[31], over 1000 postures are theoretically possible. Usually two</nobr></div>
<div style="position: absolute; top: 18714px; left: 201px;"><nobr>gloves are worn to maximize the number of postures available; they are sold in pairs</nobr></div>
<div style="position: absolute; top: 18741px; left: 201px;"><nobr>and are priced at $2000/pair.</nobr></div>
<div style="position: absolute; top: 18768px; left: 223px;"><nobr>The Pinch Glove system is excellent for restricted posture recognition because no</nobr></div>
<div style="position: absolute; top: 18795px; left: 201px;"><nobr>posture recognition techniques are required (see section 4). The electrical contacts on</nobr></div>
<div style="position: absolute; top: 18822px; left: 201px;"><nobr>the gloves make it easy to map postures to a variety of tasks. Since the gloves have</nobr></div>
<div style="position: absolute; top: 18849px; left: 201px;"><nobr>a mount for a spatial tracking device such as a Polhemus, simple gestures can also be</nobr></div>
<div style="position: absolute; top: 18876px; left: 201px;"><nobr>recognized. The drawbacks of these gloves arise from the fact that they do not use</nobr></div>
<div style="position: absolute; top: 18902px; left: 201px;"><nobr>bend sensor technology. It is very difficult to provide a virtual representation of the</nobr></div>
<div style="position: absolute; top: 18929px; left: 201px;"><nobr>user’s hands, and such a representation is important in virtual environments, although</nobr></div>
<div style="position: absolute; top: 18956px; left: 201px;"><nobr>Mutigen’s SmartScene has gotten around this by using simple 3D cursors like spheres</nobr></div>
<div style="position: absolute; top: 18983px; left: 201px;"><nobr>instead of a virtual hand[76]. Another drawback of Pinch Gloves is that the types of</nobr></div>
<div style="position: absolute; top: 19035px; left: 451px;"><nobr>13</nobr></div>
</span></font>

<div style="position: absolute; top: 19183px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="18"><b>Page 18</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 19755px; left: 201px;"><nobr>Figure 2.3: FakeSpace’s Pinch</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 19754px; left: 383px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 19755px; left: 403px;"><nobr>Glove input devices. The gloves have electrical con-</nobr></div>
<div style="position: absolute; top: 19773px; left: 201px;"><nobr>tact points that allow users to make “pinch” postures that can be then mapped to a</nobr></div>
<div style="position: absolute; top: 19791px; left: 201px;"><nobr>variety of tasks.</nobr></div>
<div style="position: absolute; top: 19839px; left: 201px;"><nobr>postures are limited since electrical contacts must be touching before a posture can be</nobr></div>
<div style="position: absolute; top: 19866px; left: 201px;"><nobr>recognized. If the user makes a posture in which none of the electrical contacts create</nobr></div>
<div style="position: absolute; top: 19892px; left: 201px;"><nobr>a conductive path, the posture goes unrecognized. This type of problem does not occur</nobr></div>
<div style="position: absolute; top: 19919px; left: 201px;"><nobr>with a bend-sensor-based glove.</nobr></div>
<div style="position: absolute; top: 19946px; left: 223px;"><nobr>The final glove-based inputdevice discussed here is VirtualTechnologies’ CyberGlove</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 19945px; left: 734px;"><nobr>TM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 19973px; left: 201px;"><nobr>(see Figure 2.4), originallydeveloped by Kramer in his workon The “Talking Glove”[53].</nobr></div>
<div style="position: absolute; top: 20000px; left: 201px;"><nobr>Using his patented strain gauge bend sensor technology[52], he started Virtual Tech-</nobr></div>
<div style="position: absolute; top: 20027px; left: 201px;"><nobr>nologies and now sells the glove commercially. The CyberGlove can be equipped with</nobr></div>
<div style="position: absolute; top: 20054px; left: 201px;"><nobr>either 18 or 22 bend sensors. With 18 sensors, the CyberGlove measures the flexion</nobr></div>
<div style="position: absolute; top: 20081px; left: 201px;"><nobr>of the proximal interphalangeal and the metacarpophalangeal joints of the four fingers</nobr></div>
<div style="position: absolute; top: 20108px; left: 201px;"><nobr>and the thumb, the abduction/adduction angles between the fingers, radial and palmer</nobr></div>
<div style="position: absolute; top: 20135px; left: 201px;"><nobr>abduction, wrist roll, and wrist pitch[111] (Figure A.2 illustrates the various motions</nobr></div>
<div style="position: absolute; top: 20162px; left: 201px;"><nobr>the hand can make). The additional four sensors in the 22 sensor model measure the</nobr></div>
<div style="position: absolute; top: 20223px; left: 451px;"><nobr>14</nobr></div>
</span></font>

<div style="position: absolute; top: 20371px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="19"><b>Page 19</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 20560px; left: 201px;"><nobr>flexion of the distal interphalangeal joints in the four fingers. With a six DOF tracker</nobr></div>
<div style="position: absolute; top: 20586px; left: 201px;"><nobr>and the 22 sensor model, 28 degrees of freedom of the hand can be realized.</nobr></div>
<div style="position: absolute; top: 20613px; left: 223px;"><nobr>An interesting feature of the CyberGlove’s interface unit is that it digitizes the volt-</nobr></div>
<div style="position: absolute; top: 20640px; left: 201px;"><nobr>age output of each sensor and then modifies the value using a linear calibration func-</nobr></div>
<div style="position: absolute; top: 20667px; left: 201px;"><nobr>tion. This function uses gain and offset values to represent the slope and <i>y</i>-intercept</nobr></div>
<div style="position: absolute; top: 20694px; left: 201px;"><nobr>of the linear equation. This equation allows software calibration of the glove and thus</nobr></div>
<div style="position: absolute; top: 20721px; left: 201px;"><nobr>makes it more robust for a variety of hand sizes.</nobr></div>
<div style="position: absolute; top: 21069px; left: 201px;"><nobr>Figure 2.4: Virtual Technologies’ CyberGlove and control box. The glove can be</nobr></div>
<div style="position: absolute; top: 21087px; left: 201px;"><nobr>equipped with 18 or 22 bend sensors (from Virtual Technologies[110]).</nobr></div>
<div style="position: absolute; top: 21132px; left: 223px;"><nobr>The author’s personal experience and an evaluation by Kessler et al. [50] suggest</nobr></div>
<div style="position: absolute; top: 21158px; left: 201px;"><nobr>the CyberGlove is accurate to within one degree of flexion. It works well for both sim-</nobr></div>
<div style="position: absolute; top: 21185px; left: 201px;"><nobr>ple and complex posture and gesture recognition (Wexelblat[116] and Fels[35] verify</nobr></div>
<div style="position: absolute; top: 21212px; left: 201px;"><nobr>this claim). The only negative in regard to the CyberGlove is its price; the 18-sensor</nobr></div>
<div style="position: absolute; top: 21239px; left: 201px;"><nobr>model is available for $9800 and the 22-sensor model for $14,500. But even though</nobr></div>
<div style="position: absolute; top: 21266px; left: 201px;"><nobr>the glove is expensive, it is the best available glove-based technology for accurate and</nobr></div>
<div style="position: absolute; top: 21293px; left: 201px;"><nobr>robust hand posture and gesture recognition.</nobr></div>
<div style="position: absolute; top: 21411px; left: 451px;"><nobr>15</nobr></div>
</span></font>

<div style="position: absolute; top: 21559px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="20"><b>Page 20</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 21741px; left: 201px;"><nobr><b>2.3 Vision-Based Technology</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 21790px; left: 201px;"><nobr>One of the main difficulties in using glove-based input devices to collect raw posture</nobr></div>
<div style="position: absolute; top: 21817px; left: 201px;"><nobr>and gesture recognition data is the fact the gloves must be worn by the user and at-</nobr></div>
<div style="position: absolute; top: 21844px; left: 201px;"><nobr>tached to the computer. In many cases, users do not want to wear tracking devices and</nobr></div>
<div style="position: absolute; top: 21870px; left: 201px;"><nobr>computer-bound gloves since they can restrict freedom of movement and take consid-</nobr></div>
<div style="position: absolute; top: 21897px; left: 201px;"><nobr>erably longer to set up than traditional interaction methods. As a result, there has been</nobr></div>
<div style="position: absolute; top: 21924px; left: 201px;"><nobr>quite a bit of research into using computer vision to track human movement and extract</nobr></div>
<div style="position: absolute; top: 21951px; left: 201px;"><nobr>raw data for posture and gesture recognition.</nobr></div>
<div style="position: absolute; top: 21978px; left: 223px;"><nobr>A vision-based solution to collecting data for hand posture and gesture recognition</nobr></div>
<div style="position: absolute; top: 22005px; left: 201px;"><nobr>requires four equally important components. The first is the placement and number of</nobr></div>
<div style="position: absolute; top: 22032px; left: 201px;"><nobr>cameras used. Placing the camera(s) is critical because the visibility of the hand or</nobr></div>
<div style="position: absolute; top: 22059px; left: 201px;"><nobr>hands being tracked must be maximized for robust recognition. Visibility is important</nobr></div>
<div style="position: absolute; top: 22086px; left: 201px;"><nobr>because of the many occlusion problems present in vision-based tracking (see section</nobr></div>
<div style="position: absolute; top: 22113px; left: 201px;"><nobr>2.4). The number of cameras used for tracking is another important issue. In general,</nobr></div>
<div style="position: absolute; top: 22140px; left: 201px;"><nobr>one camera is used to collect recognitiondata, and it has been shown by Starner[96] and</nobr></div>
<div style="position: absolute; top: 22166px; left: 201px;"><nobr>Martin[68] that one is effective and accurate in recognizing hand posture and gestures.</nobr></div>
<div style="position: absolute; top: 22193px; left: 201px;"><nobr>When depth or stereo information is required for tracking hand movement, two or more</nobr></div>
<div style="position: absolute; top: 22220px; left: 201px;"><nobr>cameras are needed. Although using more than one camera adds complications due to</nobr></div>
<div style="position: absolute; top: 22247px; left: 201px;"><nobr>the algorithmic complexity of dealing with more than one image stream, they do pro-</nobr></div>
<div style="position: absolute; top: 22274px; left: 201px;"><nobr>vide more visibility and are critical in virtual environment applications which usually</nobr></div>
<div style="position: absolute; top: 22301px; left: 201px;"><nobr>require depth information. Kumo[56] and Utsumi[108] use multiple cameras effec-</nobr></div>
<div style="position: absolute; top: 22328px; left: 201px;"><nobr>tively in the context of 3D object manipulation and 3D scene creation, respectively.</nobr></div>
<div style="position: absolute; top: 22354px; left: 201px;"><nobr>Also, Rehag and Kanade[91] have shown that 27 DOF of the hand can be recovered by</nobr></div>
<div style="position: absolute; top: 22381px; left: 201px;"><nobr>using two cameras.</nobr></div>
<div style="position: absolute; top: 22408px; left: 223px;"><nobr>The second component in a vision-based solution for hand posture and gesture</nobr></div>
<div style="position: absolute; top: 22435px; left: 201px;"><nobr>recognition is to make the hands more visible to the camera for simpler extraction of</nobr></div>
<div style="position: absolute; top: 22462px; left: 201px;"><nobr>hand data. One of the first ways of doing this was to place LEDs (light emitting diodes)</nobr></div>
<div style="position: absolute; top: 22489px; left: 201px;"><nobr>on various points on the hand[99]. These LEDs let the camera quickly pick up feature</nobr></div>
<div style="position: absolute; top: 22516px; left: 201px;"><nobr>points on the hand to aid recognition. A more common method in the literature is</nobr></div>
<div style="position: absolute; top: 22543px; left: 201px;"><nobr>to simply use colored gloves. Starner[97], Davis[25], and Kumo[56] have all shown</nobr></div>
<div style="position: absolute; top: 22599px; left: 451px;"><nobr>16</nobr></div>
</span></font>

<div style="position: absolute; top: 22747px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="21"><b>Page 21</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 22936px; left: 201px;"><nobr>that using solid colored gloves allows faster hand silhouette extraction than simply</nobr></div>
<div style="position: absolute; top: 22962px; left: 201px;"><nobr>wearing no gloves at all, but using such gloves makes it difficult to recognize finger</nobr></div>
<div style="position: absolute; top: 22989px; left: 201px;"><nobr>movement and bending. In order to achieve fast silhouette extraction and track finger</nobr></div>
<div style="position: absolute; top: 23016px; left: 201px;"><nobr>joint movement, Dorner developed a complex encoding scheme using sets of colored</nobr></div>
<div style="position: absolute; top: 23043px; left: 201px;"><nobr>rings around the finger joints instead of solid colored gloves[28].</nobr></div>
<div style="position: absolute; top: 23070px; left: 223px;"><nobr>Even though colored gloves are wireless and simple to wear, the ideal situation</nobr></div>
<div style="position: absolute; top: 23097px; left: 201px;"><nobr>for vision-based hand tracking is to track the hand with no gloves at all. Tracking</nobr></div>
<div style="position: absolute; top: 23124px; left: 201px;"><nobr>a gloveless hand presents some interesting difficulties, among them skin color and</nobr></div>
<div style="position: absolute; top: 23150px; left: 201px;"><nobr>background environment issues. In many cases a solid colored screen is placed behind</nobr></div>
<div style="position: absolute; top: 23177px; left: 201px;"><nobr>the user so that the natural color of the hands can be found and features extracted. One</nobr></div>
<div style="position: absolute; top: 23204px; left: 201px;"><nobr>of the best vision systems for tracking the naked hand was Krueger’s VIDEODESK</nobr></div>
<div style="position: absolute; top: 23231px; left: 201px;"><nobr>system[55], although it required complex image-processing hardware. He was able to</nobr></div>
<div style="position: absolute; top: 23258px; left: 201px;"><nobr>track hand silhouettes in order to create simple 2D and 3D shapes. Utsumi also tracked</nobr></div>
<div style="position: absolute; top: 23285px; left: 201px;"><nobr>the naked hand in his 3D scene creation system[108].</nobr></div>
<div style="position: absolute; top: 23312px; left: 223px;"><nobr>The third component of a vision-based solution for hand gesture and posture recog-</nobr></div>
<div style="position: absolute; top: 23339px; left: 201px;"><nobr>nition is the extraction of features from the stream or streams of raw image data; the</nobr></div>
<div style="position: absolute; top: 23366px; left: 201px;"><nobr>fourth component is to apply recognition algorithms to these extracted features. Both</nobr></div>
<div style="position: absolute; top: 23393px; left: 201px;"><nobr>these components are discussed in section 3.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 23450px; left: 201px;"><nobr><b>2.4 Advantages and Disadvantages of Glove-and Vision-</b></nobr></div>
<div style="position: absolute; top: 23490px; left: 249px;"><nobr><b>Based Data Collection Systems</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 23539px; left: 201px;"><nobr>We can now examine the advantages and disadvantages of glove-based and vision-</nobr></div>
<div style="position: absolute; top: 23566px; left: 201px;"><nobr>based technologyfor hand posture and gesture recognition. Kadous[48] and Sturman[101]</nobr></div>
<div style="position: absolute; top: 23593px; left: 201px;"><nobr>have also discussed these issues to varying extents.</nobr></div>
<div style="position: absolute; top: 23641px; left: 201px;"><nobr><b>Cost </b>Even though glove-based technology has come down in price (under $500 for</nobr></div>
<div style="position: absolute; top: 23669px; left: 201px;"><nobr>the 5DT Glove), the cost of robust and complex posture and gesture recognition is</nobr></div>
<div style="position: absolute; top: 23695px; left: 201px;"><nobr>going to be high if a glove-based solution is used. The cost of a tracking device and a</nobr></div>
<div style="position: absolute; top: 23722px; left: 201px;"><nobr>robust glove is in the thousands of dollars. On the other hand, a vision-based solution</nobr></div>
<div style="position: absolute; top: 23787px; left: 451px;"><nobr>17</nobr></div>
</span></font>

<div style="position: absolute; top: 23935px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="22"><b>Page 22</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 24124px; left: 201px;"><nobr>is relatively inexpensive, especially since modern-day workstations are equipped with</nobr></div>
<div style="position: absolute; top: 24150px; left: 201px;"><nobr>cameras.</nobr></div>
<div style="position: absolute; top: 24198px; left: 201px;"><nobr><b>User Comfort </b>With a glove-based solution, the user must wear a tracking device and</nobr></div>
<div style="position: absolute; top: 24225px; left: 201px;"><nobr>glove that are connected to a computer. Putting these devices on takes time, can be</nobr></div>
<div style="position: absolute; top: 24252px; left: 201px;"><nobr>quite cumbersome, and can limit one’s range of motion. With a vision-based solution,</nobr></div>
<div style="position: absolute; top: 24279px; left: 201px;"><nobr>the user may have to wear a glove, but the glove will be extremely lightweight, easy to</nobr></div>
<div style="position: absolute; top: 24306px; left: 201px;"><nobr>put on, and not connected to the computer. Applications in which no gloves are used,</nobr></div>
<div style="position: absolute; top: 24333px; left: 201px;"><nobr>give the user complete freedom of motion and provides a cleaner way to interact and</nobr></div>
<div style="position: absolute; top: 24360px; left: 201px;"><nobr>perform posture and gesture recognition.</nobr></div>
<div style="position: absolute; top: 24408px; left: 201px;"><nobr><b>Computing Power </b>Depending on the algorithms used, both glove-based and vision-</nobr></div>
<div style="position: absolute; top: 24435px; left: 201px;"><nobr>based solutions can require significant computing power. However, in general, the</nobr></div>
<div style="position: absolute; top: 24462px; left: 201px;"><nobr>vision-based approach takes more computing power due to the image processing nec-</nobr></div>
<div style="position: absolute; top: 24489px; left: 201px;"><nobr>essary. Glove-based solutions have a slight advantage over vision-based solutions in</nobr></div>
<div style="position: absolute; top: 24516px; left: 201px;"><nobr>that the data the gloves send to the computer can easily be transformed into records</nobr></div>
<div style="position: absolute; top: 24543px; left: 201px;"><nobr>that are suitable for recognition. However, with faster computers, computational power</nobr></div>
<div style="position: absolute; top: 24570px; left: 201px;"><nobr>should not be an issue.</nobr></div>
<div style="position: absolute; top: 24618px; left: 201px;"><nobr><b>Hand Size </b>Human hands vary in shape and size. This is a significant problem with</nobr></div>
<div style="position: absolute; top: 24645px; left: 201px;"><nobr>glove-based solutions: some users cannot wear these input devices because their hands</nobr></div>
<div style="position: absolute; top: 24672px; left: 201px;"><nobr>are too big or too small. This problem is not an issue with vision-based solutions.</nobr></div>
<div style="position: absolute; top: 24720px; left: 201px;"><nobr><b>Hand Anatomy </b>Glove-based input devices may not always fit well enough to pre-</nobr></div>
<div style="position: absolute; top: 24747px; left: 201px;"><nobr>vent their position sensors from moving relative to the joints the sensors are trying to</nobr></div>
<div style="position: absolute; top: 24774px; left: 201px;"><nobr>measure. This problem reduces recognition accuracy after extended periods of use and</nobr></div>
<div style="position: absolute; top: 24801px; left: 201px;"><nobr>forces users to recalibrate the devices which can be a nuisance. This problem also is</nobr></div>
<div style="position: absolute; top: 24828px; left: 201px;"><nobr>not an issue with vision-based solutions.</nobr></div>
<div style="position: absolute; top: 24876px; left: 201px;"><nobr><b>Calibration </b>Calibration is important in both vision- and glove-based solutions but,</nobr></div>
<div style="position: absolute; top: 24903px; left: 201px;"><nobr>due to the anatomy of the hand, it is more critical with glove-based solutions. In gen-</nobr></div>
<div style="position: absolute; top: 24930px; left: 201px;"><nobr>eral, a calibration procedure or step is required for every user and, in some cases, every</nobr></div>
<div style="position: absolute; top: 24975px; left: 451px;"><nobr>18</nobr></div>
</span></font>

<div style="position: absolute; top: 25123px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="23"><b>Page 23</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 25312px; left: 201px;"><nobr>time a user wants to run the system. In some vision-based solutions, however, a general</nobr></div>
<div style="position: absolute; top: 25338px; left: 201px;"><nobr>calibration step can be used for a wide variety of users.</nobr></div>
<div style="position: absolute; top: 25387px; left: 201px;"><nobr><b>Portability </b>In many applications, especially gesture to speech systems, freedom from</nobr></div>
<div style="position: absolute; top: 25414px; left: 201px;"><nobr>being tied down to a workstationis important. With glove-based solutions, this freedom</nobr></div>
<div style="position: absolute; top: 25441px; left: 201px;"><nobr>is generally available as long as hand tracking is not involved, since these input devices</nobr></div>
<div style="position: absolute; top: 25468px; left: 201px;"><nobr>can be plugged right into a laptop computer. Vision-based solutions were originally</nobr></div>
<div style="position: absolute; top: 25495px; left: 201px;"><nobr>quite difficult to use in a mobile environment due to camera placement issues and com-</nobr></div>
<div style="position: absolute; top: 25521px; left: 201px;"><nobr>puting power requirements. However, with the advent of wearable computing[65] and</nobr></div>
<div style="position: absolute; top: 25548px; left: 201px;"><nobr>powerful laptops with built-in cameras, mobile vision-based solutions are becoming</nobr></div>
<div style="position: absolute; top: 25575px; left: 201px;"><nobr>more practical.</nobr></div>
<div style="position: absolute; top: 25623px; left: 201px;"><nobr><b>Noise </b>In glove-based solutions where hand tracking is required, some type of noise is</nobr></div>
<div style="position: absolute; top: 25651px; left: 201px;"><nobr>bound to be associated with the data (it can come from a variety of sources depending</nobr></div>
<div style="position: absolute; top: 25678px; left: 201px;"><nobr>on the tracking technology used). Filteringalgorithms are therefore necessary to reduce</nobr></div>
<div style="position: absolute; top: 25705px; left: 201px;"><nobr>noise and jitter. In some cases this can get computationally expensive when predictive</nobr></div>
<div style="position: absolute; top: 25732px; left: 201px;"><nobr>techniques such as Kalman filtering[114] are used. With a vision-based solution, noise</nobr></div>
<div style="position: absolute; top: 25759px; left: 201px;"><nobr>from input devices is minimal (although occlusion could be considered a form of noise,</nobr></div>
<div style="position: absolute; top: 25785px; left: 201px;"><nobr>since it contributes to the difficulty of vision-based solutions).</nobr></div>
<div style="position: absolute; top: 25834px; left: 201px;"><nobr><b>Accuracy </b>In both vision- and glove-based solutions for hand posture and gesture</nobr></div>
<div style="position: absolute; top: 25861px; left: 201px;"><nobr>recognition, accuracy is one of the most critical components to providing robust recog-</nobr></div>
<div style="position: absolute; top: 25888px; left: 201px;"><nobr>nition. Both these solutions provide the potential for high levels of accuracy depending</nobr></div>
<div style="position: absolute; top: 25915px; left: 201px;"><nobr>on the technology and recognition algorithms used. Accuracy also depends on the</nobr></div>
<div style="position: absolute; top: 25942px; left: 201px;"><nobr>complexity and quantity of the postures and gestures to be recognized. Obviously, the</nobr></div>
<div style="position: absolute; top: 25969px; left: 201px;"><nobr>quantity of possible postures and gestures and their complexity greatly affect accuracy</nobr></div>
<div style="position: absolute; top: 25996px; left: 201px;"><nobr>no matter what raw data collection system is used.</nobr></div>
<div style="position: absolute; top: 26163px; left: 451px;"><nobr>19</nobr></div>
</span></font>

<div style="position: absolute; top: 26311px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="24"><b>Page 24</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 26614px; left: 201px;"><nobr><b>3</b></nobr></div>
</span></font>
<font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 26619px; left: 239px;"><nobr><b>Algorithmic Techniques for Recognizing</b></nobr></div>
<div style="position: absolute; top: 26669px; left: 201px;"><nobr><b>Hand Postures and Gestures</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 26766px; left: 223px;"><nobr>Once the raw data has been collected from a vision- or glove-based data collection</nobr></div>
<div style="position: absolute; top: 26793px; left: 201px;"><nobr>system, it must be analyzed to determine if any postures or gestures have been recog-</nobr></div>
<div style="position: absolute; top: 26820px; left: 201px;"><nobr>nized. In this section, various algorithmic techniques for recognizing hand postures and</nobr></div>
<div style="position: absolute; top: 26847px; left: 201px;"><nobr>gestures are discussed. Although some good surveys have discussed hand gesture and</nobr></div>
<div style="position: absolute; top: 26874px; left: 201px;"><nobr>posture recognition techniques[48][97][112], this present survey is considerably more</nobr></div>
<div style="position: absolute; top: 26901px; left: 201px;"><nobr>thorough and is up to date with the current literature. The techniques in this survey fall</nobr></div>
<div style="position: absolute; top: 26928px; left: 201px;"><nobr>into three rough categories:</nobr></div>
<div style="position: absolute; top: 26955px; left: 231px;"><nobr>Feature extraction, statistics and models</nobr></div>
<div style="position: absolute; top: 26992px; left: 231px;"><nobr>Learning algorithms</nobr></div>
<div style="position: absolute; top: 27029px; left: 231px;"><nobr>Miscellaneous techniques</nobr></div>
<div style="position: absolute; top: 27067px; left: 223px;"><nobr>Each category contains a number of recognition techniques. These techniques will</nobr></div>
<div style="position: absolute; top: 27093px; left: 201px;"><nobr>be discussed through a general introduction to the technique, a look at the current</nobr></div>
<div style="position: absolute; top: 27120px; left: 201px;"><nobr>literature, and an analysis of advantages and disadvantages.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 27176px; left: 201px;"><nobr><b>3.1 Feature Extraction, Statistics, and Models</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 27226px; left: 201px;"><nobr>This category contains six of the most common techniques for hand posture and gesture</nobr></div>
<div style="position: absolute; top: 27253px; left: 201px;"><nobr>recognition that extract some mathematical quantity from the raw data. In these cases,</nobr></div>
<div style="position: absolute; top: 27279px; left: 201px;"><nobr>the mathematical quantity is represented as a feature, a statistic, or a model (see Table</nobr></div>
<div style="position: absolute; top: 27306px; left: 201px;"><nobr>3.1 for a summary of the techniques).</nobr></div>
<div style="position: absolute; top: 27351px; left: 451px;"><nobr>20</nobr></div>
</span></font>

<div style="position: absolute; top: 27499px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="25"><b>Page 25</b></a></font></td></tr></tbody></table></div><font size="2" face="Times"><span style="font-size: 5px; font-family: Times;">
<div style="position: absolute; top: 27699px; left: 301px;"><nobr>Vison</nobr></div>
<div style="position: absolute; top: 27699px; left: 336px;"><nobr>Glove</nobr></div>
<div style="position: absolute; top: 27699px; left: 372px;"><nobr>Postures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 27699px; left: 461px;"><nobr>Gestures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 27699px; left: 552px;"><nobr>Training</nobr></div>
<div style="position: absolute; top: 27699px; left: 598px;"><nobr>Previous Work</nobr></div>
<div style="position: absolute; top: 27699px; left: 660px;"><nobr>Adv. Knowledge</nobr></div>
<div style="position: absolute; top: 27726px; left: 210px;"><nobr>Template Matching</nobr></div>
<div style="position: absolute; top: 27726px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27726px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27726px; left: 372px;"><nobr>Complex-Small-98%</nobr></div>
<div style="position: absolute; top: 27726px; left: 461px;"><nobr>Simple-Small-96%</nobr></div>
<div style="position: absolute; top: 27726px; left: 552px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27726px; left: 598px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 27726px; left: 660px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27754px; left: 210px;"><nobr>Feature Extraction</nobr></div>
<div style="position: absolute; top: 27754px; left: 301px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27754px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27754px; left: 372px;"><nobr>Complex-N/A-N/A</nobr></div>
<div style="position: absolute; top: 27754px; left: 461px;"><nobr>Complex-N/A-N/A</nobr></div>
<div style="position: absolute; top: 27754px; left: 552px;"><nobr>None</nobr></div>
<div style="position: absolute; top: 27754px; left: 598px;"><nobr>Moderate</nobr></div>
<div style="position: absolute; top: 27754px; left: 660px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27781px; left: 210px;"><nobr>Active Shape Models</nobr></div>
<div style="position: absolute; top: 27781px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27781px; left: 336px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27781px; left: 372px;"><nobr>Simple-Small-N/A</nobr></div>
<div style="position: absolute; top: 27781px; left: 461px;"><nobr>Simple-Small-N/A</nobr></div>
<div style="position: absolute; top: 27781px; left: 552px;"><nobr>None</nobr></div>
<div style="position: absolute; top: 27781px; left: 598px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27781px; left: 660px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27809px; left: 210px;"><nobr>Principal Components</nobr></div>
<div style="position: absolute; top: 27809px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27809px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27809px; left: 372px;"><nobr>Complex-Large-99%</nobr></div>
<div style="position: absolute; top: 27809px; left: 461px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 27809px; left: 552px;"><nobr>Moderate</nobr></div>
<div style="position: absolute; top: 27809px; left: 598px;"><nobr>Moderate</nobr></div>
<div style="position: absolute; top: 27809px; left: 660px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27836px; left: 210px;"><nobr>Linear Fingertip Models</nobr></div>
<div style="position: absolute; top: 27836px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27836px; left: 336px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27836px; left: 372px;"><nobr>Complex-Small-90%</nobr></div>
<div style="position: absolute; top: 27836px; left: 461px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 27836px; left: 552px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27836px; left: 598px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27836px; left: 660px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27864px; left: 210px;"><nobr>Causal Analysis</nobr></div>
<div style="position: absolute; top: 27864px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 27864px; left: 336px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 27864px; left: 372px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 27864px; left: 461px;"><nobr>Simple-Small-N/A</nobr></div>
<div style="position: absolute; top: 27864px; left: 552px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27864px; left: 598px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 27864px; left: 660px;"><nobr>No</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 27900px; left: 201px;"><nobr>Table 3.1: A summary of the hand posture and gesture recognition techniques found</nobr></div>
<div style="position: absolute; top: 27918px; left: 201px;"><nobr>in Section 3.1. The table shows information about whether a technique has been used</nobr></div>
<div style="position: absolute; top: 27936px; left: 201px;"><nobr>in a glove- or vision-based solution, the posture and gesture complexity, set size, and</nobr></div>
<div style="position: absolute; top: 27954px; left: 201px;"><nobr>reported accuracy, the extent of the training required, how much work has been done</nobr></div>
<div style="position: absolute; top: 27972px; left: 201px;"><nobr>using the technique, and if it is important to have advance knowledge of the posture or</nobr></div>
<div style="position: absolute; top: 27990px; left: 201px;"><nobr>gesture set during implementation.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 28034px; left: 201px;"><nobr><b>3.1.1 Template Matching</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 28075px; left: 201px;"><nobr>Template matching is one of the simplest methods for recognizing hand postures and</nobr></div>
<div style="position: absolute; top: 28102px; left: 201px;"><nobr>has been discussed frequently, with thorough contributions by Sturman[101] and Wat-</nobr></div>
<div style="position: absolute; top: 28129px; left: 201px;"><nobr>son[112]. Templates can be used in both glove-based and vision-based solutions; the</nobr></div>
<div style="position: absolute; top: 28155px; left: 201px;"><nobr>templates are sequences of sensor values (gloves) and a static or small set of images</nobr></div>
<div style="position: absolute; top: 28182px; left: 201px;"><nobr>(computer vision). Here we discuss only gloved-based template matching although it</nobr></div>
<div style="position: absolute; top: 28209px; left: 201px;"><nobr>is used in vision-based solutions as well.</nobr></div>
<div style="position: absolute; top: 28236px; left: 223px;"><nobr>In general, template matching determines whether a given data record can be clas-</nobr></div>
<div style="position: absolute; top: 28263px; left: 201px;"><nobr>sified as a member of a set of stored data records. Recognizing hand postures using</nobr></div>
<div style="position: absolute; top: 28290px; left: 201px;"><nobr>template matching has two parts. The first is to create the templates by collecting data</nobr></div>
<div style="position: absolute; top: 28317px; left: 201px;"><nobr>values for each posture in the posture set. Generally, each posture is performed a num-</nobr></div>
<div style="position: absolute; top: 28343px; left: 201px;"><nobr>ber of times and the average of the raw data for each sensor is taken and stored as the</nobr></div>
<div style="position: absolute; top: 28370px; left: 201px;"><nobr>template. The second part is to compare the current sensor readings with the given set</nobr></div>
<div style="position: absolute; top: 28397px; left: 201px;"><nobr>of templates to find the posture template most closely matching the current data record.</nobr></div>
<div style="position: absolute; top: 28424px; left: 223px;"><nobr>An example of the template matching comparison is the use of a Boolean func-</nobr></div>
<div style="position: absolute; top: 28451px; left: 201px;"><nobr>tion on the results of the explicit comparison between each sensor value in the current</nobr></div>
<div style="position: absolute; top: 28478px; left: 201px;"><nobr>data record and the corresponding value in the posture templates. The comparisons</nobr></div>
<div style="position: absolute; top: 28539px; left: 451px;"><nobr>21</nobr></div>
</span></font>

<div style="position: absolute; top: 28687px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="26"><b>Page 26</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 28876px; left: 201px;"><nobr>are often made within a range of values, which helps to improve recognition accuracy</nobr></div>
<div style="position: absolute; top: 28902px; left: 201px;"><nobr>with noisy glove devices. A problem with comparisons of angle measurements within</nobr></div>
<div style="position: absolute; top: 28929px; left: 201px;"><nobr>a range of values, however, is that while in theory, these ranges usually go from zero</nobr></div>
<div style="position: absolute; top: 28956px; left: 201px;"><nobr>to a power of two based on the bits of precision in each bend sensor, the actual angle</nobr></div>
<div style="position: absolute; top: 28983px; left: 201px;"><nobr>measurements from the bend sensors do not follow their theoretical ranges and each</nobr></div>
<div style="position: absolute; top: 29010px; left: 201px;"><nobr>bend sensor often has a different range. A way to remedy this problem is to normal-</nobr></div>
<div style="position: absolute; top: 29037px; left: 201px;"><nobr>ize the bend sensor’s measurements using the maximum and minimum value for each</nobr></div>
<div style="position: absolute; top: 29064px; left: 201px;"><nobr>bend sensor. Normalization makes all the angle measurements zero for full extension</nobr></div>
<div style="position: absolute; top: 29090px; left: 201px;"><nobr>and one for full flexion, thus making comparisons with ranges easier to implement.</nobr></div>
<div style="position: absolute; top: 29117px; left: 201px;"><nobr>The drawback of normalizing bend angles is that maximum and minimum values can</nobr></div>
<div style="position: absolute; top: 29144px; left: 201px;"><nobr>change during glove use; however dynamic calculation and updating of maximum and</nobr></div>
<div style="position: absolute; top: 29171px; left: 201px;"><nobr>minimum values has been shown to combat this problem[101].</nobr></div>
<div style="position: absolute; top: 29198px; left: 223px;"><nobr>Another example of template matching comparisons is the use of distance mea-</nobr></div>
<div style="position: absolute; top: 29225px; left: 201px;"><nobr>surements between the current data record and each of the data records in the posture</nobr></div>
<div style="position: absolute; top: 29252px; left: 201px;"><nobr>set recognizing the posture with the lowest distance measurement. The distance mea-</nobr></div>
<div style="position: absolute; top: 29279px; left: 201px;"><nobr>surement must be below some threshold value to avoid false positive recognition. Two</nobr></div>
<div style="position: absolute; top: 29306px; left: 201px;"><nobr>distance measurements used for hand posture template matching are the sum of the</nobr></div>
<div style="position: absolute; top: 29333px; left: 201px;"><nobr>absolute differences[121] and the sum of the squares[81]. The main advantage of com-</nobr></div>
<div style="position: absolute; top: 29359px; left: 201px;"><nobr>puting a distance measurement is that comparison ranges need not be used. The main</nobr></div>
<div style="position: absolute; top: 29386px; left: 201px;"><nobr>disadvantage is that a measurement must be made for each posture template.</nobr></div>
<div style="position: absolute; top: 29413px; left: 223px;"><nobr>Template matching is the simplest of the hand posture and gesture recognition tech-</nobr></div>
<div style="position: absolute; top: 29440px; left: 201px;"><nobr>niques, and for a small set of postures, it is appropriate and can be quite accurate. But</nobr></div>
<div style="position: absolute; top: 29467px; left: 201px;"><nobr>the technique does have limitations. First, template matching is much more difficult</nobr></div>
<div style="position: absolute; top: 29494px; left: 201px;"><nobr>for hand gestures. However, Darrell and Pentland have recognized hand gestures in</nobr></div>
<div style="position: absolute; top: 29521px; left: 201px;"><nobr>a vision-based solution using sets of view models or templates that are matched with</nobr></div>
<div style="position: absolute; top: 29548px; left: 201px;"><nobr>gesture patterns using dynamic time warping[27]</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 29546px; left: 496px;"><nobr>1</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 29548px; left: 502px;"><nobr>. The second limitation is the small</nobr></div>
<div style="position: absolute; top: 29575px; left: 201px;"><nobr>number of possible postures that can be recognized. If the application requires a large</nobr></div>
<div style="position: absolute; top: 29602px; left: 201px;"><nobr>posture set, then template matching will not work since the posture templates will</nobr></div>
<div style="position: absolute; top: 29629px; left: 201px;"><nobr>overlap[112].</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 29657px; left: 217px;"><nobr>1</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 29658px; left: 222px;"><nobr>In this case, two gestures were recognized[27].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 29727px; left: 451px;"><nobr>22</nobr></div>
</span></font>

<div style="position: absolute; top: 29875px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="27"><b>Page 27</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 30063px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 30101px; left: 254px;"><nobr><b>I. Simplest technique to implement</b></nobr></div>
<div style="position: absolute; top: 30135px; left: 248px;"><nobr><b>II. Accurate (for small set of postures)</b></nobr></div>
<div style="position: absolute; top: 30167px; left: 242px;"><nobr><b>III. Requires only a small amount of calibration</b></nobr></div>
<div style="position: absolute; top: 30206px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 30245px; left: 254px;"><nobr><b>I. Not suited for hand gestures</b></nobr></div>
<div style="position: absolute; top: 30278px; left: 248px;"><nobr><b>II. Does not work well for large posture sets due to overlappingtemplates[112]</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 30328px; left: 201px;"><nobr><b>3.1.2 Feature Extraction and Analysis</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 30369px; left: 201px;"><nobr>In feature extraction and analysis, low-level information from the raw data is analyzed</nobr></div>
<div style="position: absolute; top: 30396px; left: 201px;"><nobr>to produce higher-level semantic information and then used to recognize postures and</nobr></div>
<div style="position: absolute; top: 30422px; left: 201px;"><nobr>gestures. One of the first gestural interfaces to use a feature-based system was Rubine’s</nobr></div>
<div style="position: absolute; top: 30449px; left: 201px;"><nobr>2D single-stroke gesture recognizer[92]. Rubine extracted such features as the cosine</nobr></div>
<div style="position: absolute; top: 30476px; left: 201px;"><nobr>and sine of the initial angle of the gesture, the distance between the first and last point,</nobr></div>
<div style="position: absolute; top: 30503px; left: 201px;"><nobr>the maximum speed of the gesture, and so on. From these features, the system recog-</nobr></div>
<div style="position: absolute; top: 30530px; left: 201px;"><nobr>nized gestures that represented numbers and letters of the alphabet, among others. The</nobr></div>
<div style="position: absolute; top: 30557px; left: 201px;"><nobr>system recognized these gestures with over 97% accuracy.</nobr></div>
<div style="position: absolute; top: 30584px; left: 223px;"><nobr>This type of feature-based approach has also been applied to recognizing hand</nobr></div>
<div style="position: absolute; top: 30611px; left: 201px;"><nobr>postures and gestures. However, it is slightly more complex due to the increase in di-</nobr></div>
<div style="position: absolute; top: 30638px; left: 201px;"><nobr>mensions from two to three and the increase in the amount of raw data produced by the</nobr></div>
<div style="position: absolute; top: 30665px; left: 201px;"><nobr>input devices. Sturman[101] was the first person to extend the ideas behind Rubine’s</nobr></div>
<div style="position: absolute; top: 30692px; left: 201px;"><nobr>work into three dimensions and to make possible continual analysis and recognition</nobr></div>
<div style="position: absolute; top: 30718px; left: 201px;"><nobr>(instead of requiring a starting and ending point in the gesture). Sturman used explicit</nobr></div>
<div style="position: absolute; top: 30745px; left: 201px;"><nobr>formulations for each gesture that do not require training by individual users; however,</nobr></div>
<div style="position: absolute; top: 30772px; left: 201px;"><nobr>manual programmer intervention is necessary to add new gestures. Both position data</nobr></div>
<div style="position: absolute; top: 30799px; left: 201px;"><nobr>for a tracker and flex-sensor data were kept for feature extraction. The features used</nobr></div>
<div style="position: absolute; top: 30826px; left: 201px;"><nobr>were similar to Rubine’s but also included such inherently 3D features as cross product</nobr></div>
<div style="position: absolute; top: 30853px; left: 201px;"><nobr>and bounding volume.</nobr></div>
<div style="position: absolute; top: 30915px; left: 451px;"><nobr>23</nobr></div>
</span></font>

<div style="position: absolute; top: 31063px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="28"><b>Page 28</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 31252px; left: 223px;"><nobr>Using the work by Rubine and Sturman, Wexelblat developed a robust hand ges-</nobr></div>
<div style="position: absolute; top: 31278px; left: 201px;"><nobr>ture and posture analyzer useful in a variety of applications[116]. Wexelblat’s system</nobr></div>
<div style="position: absolute; top: 31305px; left: 201px;"><nobr>uses a hierarchical structure that moves raw data from two CyberGloves and a set of</nobr></div>
<div style="position: absolute; top: 31332px; left: 201px;"><nobr>trackers through a series of layers, each layer analyzing the data to provide higher level</nobr></div>
<div style="position: absolute; top: 31359px; left: 201px;"><nobr>semantic meaning[117]. The first layer above the input devices uses entities called seg-</nobr></div>
<div style="position: absolute; top: 31386px; left: 201px;"><nobr>menters to watch the raw data. The segmenters look for significant changes over time</nobr></div>
<div style="position: absolute; top: 31413px; left: 201px;"><nobr>in the raw data values. Once a significant change is found, the segmenter sends a data</nobr></div>
<div style="position: absolute; top: 31440px; left: 201px;"><nobr>record consisting of information that represents the data change to the next layer in the</nobr></div>
<div style="position: absolute; top: 31466px; left: 201px;"><nobr>analyzer.</nobr></div>
<div style="position: absolute; top: 31493px; left: 223px;"><nobr>The next layer in Wexelblat’s analyzer has a set of proto-feature detectors that ex-</nobr></div>
<div style="position: absolute; top: 31520px; left: 201px;"><nobr>tract (from the data records of one or more segmenters) information such as the exten-</nobr></div>
<div style="position: absolute; top: 31547px; left: 201px;"><nobr>sion of a finger or curvature in the palm. This information is sent to the higher-level</nobr></div>
<div style="position: absolute; top: 31574px; left: 201px;"><nobr>feature detectors, which use the information from one or more proto-feature detectors</nobr></div>
<div style="position: absolute; top: 31601px; left: 201px;"><nobr>to derive more global features like a fist, flat hand posture or a waving gesture. The</nobr></div>
<div style="position: absolute; top: 31628px; left: 201px;"><nobr>path analysis module then takes data from all the feature detectors and puts the informa-</nobr></div>
<div style="position: absolute; top: 31655px; left: 201px;"><nobr>tion into a frame to hold the completed description of the gesture. Finally, the frames</nobr></div>
<div style="position: absolute; top: 31682px; left: 201px;"><nobr>are sent to an integration module that handles the interaction and performs temporal</nobr></div>
<div style="position: absolute; top: 31709px; left: 201px;"><nobr>integration over them.</nobr></div>
<div style="position: absolute; top: 31735px; left: 223px;"><nobr>Feature extraction and analysis is a robust way to recognize hand postures and</nobr></div>
<div style="position: absolute; top: 31762px; left: 201px;"><nobr>gestures. It can be used not only to recognize simple hand postures and gestures but</nobr></div>
<div style="position: absolute; top: 31789px; left: 201px;"><nobr>complex ones as well. Its main drawback is that it can become computationally expen-</nobr></div>
<div style="position: absolute; top: 31816px; left: 201px;"><nobr>sive when a large number of features are being collected, which slows down system</nobr></div>
<div style="position: absolute; top: 31843px; left: 201px;"><nobr>response time. This slowdown is extremely significant in virtual environment appli-</nobr></div>
<div style="position: absolute; top: 31870px; left: 201px;"><nobr>cations, but should diminish with the advent of faster machines. Finally, note that</nobr></div>
<div style="position: absolute; top: 31897px; left: 201px;"><nobr>Wexelblat’s feature extraction and analysis method could be used in vision-based solu-</nobr></div>
<div style="position: absolute; top: 31924px; left: 201px;"><nobr>tions by altering how the features are extracted. With a glove-based solution, features</nobr></div>
<div style="position: absolute; top: 31951px; left: 201px;"><nobr>are extracted from bend sensor and tracker data, while a vision-based solution would</nobr></div>
<div style="position: absolute; top: 31978px; left: 201px;"><nobr>require features to be extracted from images.</nobr></div>
<div style="position: absolute; top: 32019px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 32058px; left: 254px;"><nobr><b>I. Handles postures and gestures equally well</b></nobr></div>
<div style="position: absolute; top: 32103px; left: 451px;"><nobr>24</nobr></div>
</span></font>

<div style="position: absolute; top: 32251px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="29"><b>Page 29</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 32439px; left: 248px;"><nobr><b>II. Uses layered architecture to analyze postures and gestures</b></nobr></div>
<div style="position: absolute; top: 32476px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 32514px; left: 254px;"><nobr><b>I. Can be computationally expensive depending on how many features</b></nobr></div>
<div style="position: absolute; top: 32541px; left: 271px;"><nobr><b>are extracted</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 32591px; left: 201px;"><nobr><b>3.1.3 Active Shape Models</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 32631px; left: 201px;"><nobr>Active shape models, or “smart snakes” as they are sometimes called, are a technique</nobr></div>
<div style="position: absolute; top: 32658px; left: 201px;"><nobr>for locating a feature within a still image[23]. The technique places a contour on the</nobr></div>
<div style="position: absolute; top: 32685px; left: 201px;"><nobr>image that is roughly the shape of the feature to be tracked. The contour is then manip-</nobr></div>
<div style="position: absolute; top: 32712px; left: 201px;"><nobr>ulated by moving it iteratively toward nearby edges that deform the contour to fit the</nobr></div>
<div style="position: absolute; top: 32739px; left: 201px;"><nobr>feature. Heap and Samaria extend this technique to recognize hand postures and ges-</nobr></div>
<div style="position: absolute; top: 32766px; left: 201px;"><nobr>tures using computer vision[44]. In their system, they apply an active shape model to</nobr></div>
<div style="position: absolute; top: 32793px; left: 201px;"><nobr>each frame and use the position of the feature in that frame as an initial approximation</nobr></div>
<div style="position: absolute; top: 32820px; left: 201px;"><nobr>for the next frame. They also use a point distributionmodel[22] to find a suitable model</nobr></div>
<div style="position: absolute; top: 32847px; left: 201px;"><nobr>for the tracked object and the inherent application specificity of tracking a human hand.</nobr></div>
<div style="position: absolute; top: 32874px; left: 223px;"><nobr>Heap and Samaria’s system runs in real time (25 frames per second) and is appli-</nobr></div>
<div style="position: absolute; top: 32901px; left: 201px;"><nobr>cable only to vision-based solutions. The main disadvantage of this technique is that</nobr></div>
<div style="position: absolute; top: 32927px; left: 201px;"><nobr>currently it can track only the open hand, which severely limits the number of hand</nobr></div>
<div style="position: absolute; top: 32954px; left: 201px;"><nobr>postures and gestures that can be recognized. Also, there is very little empirical evi-</nobr></div>
<div style="position: absolute; top: 32981px; left: 201px;"><nobr>dence in the literature to support its validity. Open areas of research using active shape</nobr></div>
<div style="position: absolute; top: 33008px; left: 201px;"><nobr>models include extending them to the 3D domain (i.e. using multiple cameras) so that</nobr></div>
<div style="position: absolute; top: 33035px; left: 201px;"><nobr>the number of possible postures and gestures can be increased, and determining the</nobr></div>
<div style="position: absolute; top: 33062px; left: 201px;"><nobr>accuracy of the technique.</nobr></div>
<div style="position: absolute; top: 33101px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 33138px; left: 254px;"><nobr><b>I. Allows real time recognition</b></nobr></div>
<div style="position: absolute; top: 33170px; left: 248px;"><nobr><b>II. Handles both hand postures and gestures</b></nobr></div>
<div style="position: absolute; top: 33208px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 33246px; left: 254px;"><nobr><b>I. Tracks only the open hand</b></nobr></div>
<div style="position: absolute; top: 33291px; left: 451px;"><nobr>25</nobr></div>
</span></font>

<div style="position: absolute; top: 33439px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="30"><b>Page 30</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 33627px; left: 248px;"><nobr><b>II. Has not been applied to stereo data from multiple cameras</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 33677px; left: 201px;"><nobr><b>3.1.4 Principal Component Analysis</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 33718px; left: 201px;"><nobr>Principal component analysis (PCA) is a statistical technique for reducing the dimen-</nobr></div>
<div style="position: absolute; top: 33745px; left: 201px;"><nobr>sionality of a data set in which there are many interrelated variables, while retaining as</nobr></div>
<div style="position: absolute; top: 33772px; left: 201px;"><nobr>much of the variation in the dataset as possible[47]. The data set is reduced by trans-</nobr></div>
<div style="position: absolute; top: 33798px; left: 201px;"><nobr>forming the old data to a new set of variables (principal components) that are ordered so</nobr></div>
<div style="position: absolute; top: 33825px; left: 201px;"><nobr>that the first few variables contain most of the variation present in the originalvariables.</nobr></div>
<div style="position: absolute; top: 33852px; left: 201px;"><nobr>The original data set is transformed by computing the eigenvectors and eigenvalues of</nobr></div>
<div style="position: absolute; top: 33879px; left: 201px;"><nobr>the data set’s covariance matrix. The eigenvector with the highest eigenvalue holds the</nobr></div>
<div style="position: absolute; top: 33906px; left: 201px;"><nobr>highest variance, the eigenvector with the second highest eigenvalue holds the second</nobr></div>
<div style="position: absolute; top: 33933px; left: 201px;"><nobr>highest variance, and so on.</nobr></div>
<div style="position: absolute; top: 33960px; left: 223px;"><nobr>PCA was first applied in the computer vision community to face recognition by</nobr></div>
<div style="position: absolute; top: 33986px; left: 201px;"><nobr>Sirovich and Kirby[95] and later extended by Turk and Pentland[106]. Birk et al. and</nobr></div>
<div style="position: absolute; top: 34013px; left: 201px;"><nobr>Martin independently developed the first two systems using PCA to recognize hand</nobr></div>
<div style="position: absolute; top: 34040px; left: 201px;"><nobr>postures and gestures in a vision-based system[11][68]. Birk’s system was able to</nobr></div>
<div style="position: absolute; top: 34067px; left: 201px;"><nobr>recognize 25 postures from the International Hand Alphabet, while Martin’s system</nobr></div>
<div style="position: absolute; top: 34094px; left: 201px;"><nobr>was used to interact in a virtual workspace.</nobr></div>
<div style="position: absolute; top: 34121px; left: 223px;"><nobr>Birk’s system first performs PCA on sets of training images to generate a posture</nobr></div>
<div style="position: absolute; top: 34148px; left: 201px;"><nobr>classifier that is then used to classify postures in real time. Each set of training images</nobr></div>
<div style="position: absolute; top: 34175px; left: 201px;"><nobr>can be considered a multivariate data set: each image consists of <i>N </i>pixels and repre-</nobr></div>
<div style="position: absolute; top: 34202px; left: 201px;"><nobr>sents a point in <i>N</i>-dimensional space. In order for PCA to work successfully, there</nobr></div>
<div style="position: absolute; top: 34229px; left: 201px;"><nobr>must be little variance in at least one direction and whatever variance exists should not</nobr></div>
<div style="position: absolute; top: 34256px; left: 201px;"><nobr>be meaningful. Birk’s recognition system works well but there is little indication that</nobr></div>
<div style="position: absolute; top: 34282px; left: 201px;"><nobr>PCA compresses the data set significantly beyond a naive approach.</nobr></div>
<div style="position: absolute; top: 34309px; left: 223px;"><nobr>Another important issue when dealing with image data is that it is highly sensitive</nobr></div>
<div style="position: absolute; top: 34336px; left: 201px;"><nobr>to position, orientation, and scaling of the hand in the image. PCA cannot transform</nobr></div>
<div style="position: absolute; top: 34363px; left: 201px;"><nobr>two identical postures with different hand sizes and positions to the same point. Birk</nobr></div>
<div style="position: absolute; top: 34390px; left: 201px;"><nobr>thus normalizes each image to center the hand, rotate it so that its major axis is vertical,</nobr></div>
<div style="position: absolute; top: 34417px; left: 201px;"><nobr>and scale it to fit the gesture image.</nobr></div>
<div style="position: absolute; top: 34479px; left: 451px;"><nobr>26</nobr></div>
</span></font>

<div style="position: absolute; top: 34627px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="31"><b>Page 31</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 34816px; left: 223px;"><nobr>The posture classifier, which is created off line, is a transformation matrix contain-</nobr></div>
<div style="position: absolute; top: 34842px; left: 201px;"><nobr>ing the results of the PCA performed on all the images in the training set. After the</nobr></div>
<div style="position: absolute; top: 34869px; left: 201px;"><nobr>transformation matrix has been calculated, the number of principal components is re-</nobr></div>
<div style="position: absolute; top: 34896px; left: 201px;"><nobr>duced by discarding the least important ones. The common approach is simply to keep</nobr></div>
<div style="position: absolute; top: 34923px; left: 201px;"><nobr>the principal components with the <i>n </i>highest eigenvalues. However, Birk has shown</nobr></div>
<div style="position: absolute; top: 34950px; left: 201px;"><nobr>that this is not effective when only a small number of principal components are to be</nobr></div>
<div style="position: absolute; top: 34977px; left: 201px;"><nobr>kept[12]. Other information such as the number of posture classes, their mean, and</nobr></div>
<div style="position: absolute; top: 35004px; left: 201px;"><nobr>their variance in the reduced data set can be used to choose the principal components.</nobr></div>
<div style="position: absolute; top: 35030px; left: 201px;"><nobr>A Bayes classifier is then used to recognize postures from the reduced set of principal</nobr></div>
<div style="position: absolute; top: 35057px; left: 201px;"><nobr>components. Note that such parameters as image resolution and number of training</nobr></div>
<div style="position: absolute; top: 35084px; left: 201px;"><nobr>images are also important components of PCA in a vision-based solution and can be</nobr></div>
<div style="position: absolute; top: 35111px; left: 201px;"><nobr>modified to improve recognition results[11].</nobr></div>
<div style="position: absolute; top: 35138px; left: 223px;"><nobr>Althoughprincipal component analysis can be used in a glove-based solution[105],</nobr></div>
<div style="position: absolute; top: 35165px; left: 201px;"><nobr>it has been used primarily in the computer vision community. It is accurate for specific</nobr></div>
<div style="position: absolute; top: 35192px; left: 201px;"><nobr>posture sets such as the International Hand Alphabet[11] but requires training by more</nobr></div>
<div style="position: absolute; top: 35219px; left: 201px;"><nobr>than one person to provide robust results. More research still needs to be done to</nobr></div>
<div style="position: absolute; top: 35246px; left: 201px;"><nobr>measure the validity of this technique and to determine whether more complex hand</nobr></div>
<div style="position: absolute; top: 35273px; left: 201px;"><nobr>gestures can be recognized accurately.</nobr></div>
<div style="position: absolute; top: 35314px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 35353px; left: 254px;"><nobr><b>I. Can recognize on the order of 25 to 35 postures[11][105]</b></nobr></div>
<div style="position: absolute; top: 35392px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 35431px; left: 254px;"><nobr><b>I. Requires training by more than one person for accurate results and</b></nobr></div>
<div style="position: absolute; top: 35457px; left: 271px;"><nobr><b>user independence[11]</b></nobr></div>
<div style="position: absolute; top: 35490px; left: 248px;"><nobr><b>II. Requires normalization to keep images consistent</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 35541px; left: 201px;"><nobr><b>3.1.5 Linear Fingertip Models</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 35581px; left: 201px;"><nobr>The linear fingertip model assumes that most finger movements are linear and comprise</nobr></div>
<div style="position: absolute; top: 35608px; left: 201px;"><nobr>very little rotational movement. This assumption allows for a simplified hand model</nobr></div>
<div style="position: absolute; top: 35667px; left: 451px;"><nobr>27</nobr></div>
</span></font>

<div style="position: absolute; top: 35815px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="32"><b>Page 32</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 36004px; left: 201px;"><nobr>that uses only the fingertips as input data and permits a model that represents each fin-</nobr></div>
<div style="position: absolute; top: 36030px; left: 201px;"><nobr>gertip trajectory through space as a simple vector. Davis and Shah use this approach</nobr></div>
<div style="position: absolute; top: 36057px; left: 201px;"><nobr>in a vision-based solution that puts a glove with brightly colored fingertips on the</nobr></div>
<div style="position: absolute; top: 36084px; left: 201px;"><nobr>user’s hand[25] and extracts the fingertip positions using histogram segmentation[40].</nobr></div>
<div style="position: absolute; top: 36111px; left: 201px;"><nobr>Once the fingertips are detected, their trajectories are calculated using motion corres-</nobr></div>
<div style="position: absolute; top: 36138px; left: 201px;"><nobr>pondence[90]. The postures themselves are modeled from a small training set by stor-</nobr></div>
<div style="position: absolute; top: 36165px; left: 201px;"><nobr>ing a motion code, the gesture name, and direction and magnitude vectors for each</nobr></div>
<div style="position: absolute; top: 36192px; left: 201px;"><nobr>of the fingertips. The postures are recognized if all the direction and magnitude vec-</nobr></div>
<div style="position: absolute; top: 36218px; left: 201px;"><nobr>tors match (within some threshold) a gesture record in the training set. System testing</nobr></div>
<div style="position: absolute; top: 36245px; left: 201px;"><nobr>showed good recognition accuracy (greater than 90%), but the system did not run in</nobr></div>
<div style="position: absolute; top: 36272px; left: 201px;"><nobr>real time and the posture and gesture set should be expanded to determine if the tech-</nobr></div>
<div style="position: absolute; top: 36299px; left: 201px;"><nobr>nique is robust.</nobr></div>
<div style="position: absolute; top: 36340px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 36378px; left: 254px;"><nobr><b>I. Simple approach</b></nobr></div>
<div style="position: absolute; top: 36411px; left: 248px;"><nobr><b>II. Concerned only with starting and ending points of fingertips</b></nobr></div>
<div style="position: absolute; top: 36443px; left: 242px;"><nobr><b>III. Has good recognition accuracy</b></nobr></div>
<div style="position: absolute; top: 36482px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 36520px; left: 254px;"><nobr><b>I. System did not run in real time</b></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 36520px; left: 469px;"><nobr>2</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 36553px; left: 248px;"><nobr><b>II. Recognizes only a small set of postures</b></nobr></div>
<div style="position: absolute; top: 36585px; left: 242px;"><nobr><b>III. Does not take curvilinear fingertip motion into account</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 36636px; left: 201px;"><nobr><b>3.1.6 Causal Analysis</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 36676px; left: 201px;"><nobr>Causal analysis is a vision-based recognition technique that stems from work in scene</nobr></div>
<div style="position: absolute; top: 36703px; left: 201px;"><nobr>analysis[16]. The technique attempts to extract information from a video stream by us-</nobr></div>
<div style="position: absolute; top: 36730px; left: 201px;"><nobr>ing high-level knowledge about actions in the scene and how they relate to one another</nobr></div>
<div style="position: absolute; top: 36757px; left: 201px;"><nobr>and the physical environment. Examples of causal data (e.g. the underlying physics</nobr></div>
<div style="position: absolute; top: 36784px; left: 201px;"><nobr>of the scene) include rigidity, mass, friction, balance, and work against gravity. Brand</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 36811px; left: 217px;"><nobr>2</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 36813px; left: 222px;"><nobr>With today’s computer performance, this system should run in real time.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 36855px; left: 451px;"><nobr>28</nobr></div>
</span></font>

<div style="position: absolute; top: 37003px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="33"><b>Page 33</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 37192px; left: 201px;"><nobr>uses this information in his system, BUSTER, that understands and outputs informa-</nobr></div>
<div style="position: absolute; top: 37218px; left: 201px;"><nobr>tion about the structure and stability of block towers[16]; In [15], Brand provides a</nobr></div>
<div style="position: absolute; top: 37245px; left: 201px;"><nobr>more detailed description on BUSTER and other scene analysis systems.</nobr></div>
<div style="position: absolute; top: 37272px; left: 223px;"><nobr>Brand and Essa have applied causal analysis to vision-based gesture recognition[14].</nobr></div>
<div style="position: absolute; top: 37299px; left: 201px;"><nobr>By using knowledge about body kinematics and dynamics, features recovered from the</nobr></div>
<div style="position: absolute; top: 37326px; left: 201px;"><nobr>video stream can be used to identify gestures based on human motor plans. The system</nobr></div>
<div style="position: absolute; top: 37353px; left: 201px;"><nobr>captures information on shoulder, elbow and wrist joint positions in the image plane.</nobr></div>
<div style="position: absolute; top: 37380px; left: 201px;"><nobr>From these positions, the system extracts a feature set that includes wrist acceleration</nobr></div>
<div style="position: absolute; top: 37406px; left: 201px;"><nobr>and deceleration, work done against gravity, size of gesture, area between arms, an-</nobr></div>
<div style="position: absolute; top: 37433px; left: 201px;"><nobr>gle between forearms, nearness to body, and verticality. Gesture filters normalize and</nobr></div>
<div style="position: absolute; top: 37460px; left: 201px;"><nobr>combine the features and use causal knowledge of how humans interact with objects</nobr></div>
<div style="position: absolute; top: 37487px; left: 201px;"><nobr>in the physical world to recognize gestures such as opening, lifting, patting, pushing,</nobr></div>
<div style="position: absolute; top: 37514px; left: 201px;"><nobr>stopping, and clutching.</nobr></div>
<div style="position: absolute; top: 37541px; left: 223px;"><nobr>Causal analysis in gesture recognition is an interesting concept, but Brand and</nobr></div>
<div style="position: absolute; top: 37568px; left: 201px;"><nobr>Essa’s discussion of their implementation is cursory[14] and, as a result, it is unclear</nobr></div>
<div style="position: absolute; top: 37595px; left: 201px;"><nobr>how accurate their system is. This system also has the disadvantage of not using data</nobr></div>
<div style="position: absolute; top: 37622px; left: 201px;"><nobr>from the fingers. More research needs to be conducted in order to determine if this</nobr></div>
<div style="position: absolute; top: 37649px; left: 201px;"><nobr>technique is robust enough to be used in any nontrivial applications.</nobr></div>
<div style="position: absolute; top: 37690px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 37729px; left: 254px;"><nobr><b>I. Uses information about how humans interact with the physical world</b></nobr></div>
<div style="position: absolute; top: 37756px; left: 271px;"><nobr><b>to help identify gestures</b></nobr></div>
<div style="position: absolute; top: 37795px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 37833px; left: 254px;"><nobr><b>I. Uses only a limited set of gestures</b></nobr></div>
<div style="position: absolute; top: 37866px; left: 248px;"><nobr><b>II. Does not use hand orientation and position data or finger data</b></nobr></div>
<div style="position: absolute; top: 37899px; left: 242px;"><nobr><b>III. Does not run in real time</b></nobr></div>
<div style="position: absolute; top: 37932px; left: 245px;"><nobr><b>IV. Implementation is unclear[14].</b></nobr></div>
<div style="position: absolute; top: 38043px; left: 451px;"><nobr>29</nobr></div>
</span></font>

<div style="position: absolute; top: 38191px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="34"><b>Page 34</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 38373px; left: 201px;"><nobr><b>3.2 Learning Algorithms</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 38422px; left: 201px;"><nobr>Here we describe the use of three of the most common learning algorithms used to</nobr></div>
<div style="position: absolute; top: 38449px; left: 201px;"><nobr>recognize hand postures and gestures. These algorithms all stem from the artificial</nobr></div>
<div style="position: absolute; top: 38476px; left: 201px;"><nobr>intelligence community, and their common trait is that recognition accuracy can be</nobr></div>
<div style="position: absolute; top: 38502px; left: 201px;"><nobr>increased through training (see Table 3.2 for a summary of these techniques).</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 5px; font-family: Times;">
<div style="position: absolute; top: 38549px; left: 301px;"><nobr>Vison</nobr></div>
<div style="position: absolute; top: 38549px; left: 336px;"><nobr>Glove</nobr></div>
<div style="position: absolute; top: 38549px; left: 372px;"><nobr>Postures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 38549px; left: 462px;"><nobr>Gestures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 38549px; left: 552px;"><nobr>Training</nobr></div>
<div style="position: absolute; top: 38549px; left: 599px;"><nobr>Previous Work</nobr></div>
<div style="position: absolute; top: 38549px; left: 661px;"><nobr>Adv. Knowledge</nobr></div>
<div style="position: absolute; top: 38576px; left: 210px;"><nobr>Neural Networks</nobr></div>
<div style="position: absolute; top: 38576px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38576px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38576px; left: 372px;"><nobr>Complex-Large-98%</nobr></div>
<div style="position: absolute; top: 38576px; left: 462px;"><nobr>Complex-Small-96%</nobr></div>
<div style="position: absolute; top: 38576px; left: 552px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 38576px; left: 599px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 38576px; left: 661px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38604px; left: 210px;"><nobr>Hidden Markov Models</nobr></div>
<div style="position: absolute; top: 38604px; left: 301px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38604px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38604px; left: 372px;"><nobr>Complex-Large-90%</nobr></div>
<div style="position: absolute; top: 38604px; left: 462px;"><nobr>Complex-Small-96%</nobr></div>
<div style="position: absolute; top: 38604px; left: 552px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 38604px; left: 599px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 38604px; left: 661px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38631px; left: 210px;"><nobr>Instance-based Learning</nobr></div>
<div style="position: absolute; top: 38631px; left: 301px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 38631px; left: 336px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 38631px; left: 372px;"><nobr>Complex-Large-80%</nobr></div>
<div style="position: absolute; top: 38631px; left: 462px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 38631px; left: 552px;"><nobr>Extensive</nobr></div>
<div style="position: absolute; top: 38631px; left: 599px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 38631px; left: 661px;"><nobr>No</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 38668px; left: 201px;"><nobr>Table 3.2: A summary of the hand posture and gesture recognition techniques found</nobr></div>
<div style="position: absolute; top: 38685px; left: 201px;"><nobr>in Section 3.2. The table shows information about whether a technique has been used</nobr></div>
<div style="position: absolute; top: 38703px; left: 201px;"><nobr>in a glove- or vision-based solution, the posture and gesture complexity, set size, and</nobr></div>
<div style="position: absolute; top: 38721px; left: 201px;"><nobr>reported accuracy, the extent of the training required, how much work has been done</nobr></div>
<div style="position: absolute; top: 38739px; left: 201px;"><nobr>using the technique, and if it is important to have advance knowledge of the posture or</nobr></div>
<div style="position: absolute; top: 38757px; left: 201px;"><nobr>gesture set during implementation.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 38823px; left: 201px;"><nobr><b>3.2.1 Neural Networks</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 38863px; left: 201px;"><nobr>This section presents a brief introductioninto the concepts involved in neural networks.</nobr></div>
<div style="position: absolute; top: 38890px; left: 201px;"><nobr>For a more comprehensive description, see Russell and Norvig[93], Krose and van der</nobr></div>
<div style="position: absolute; top: 38917px; left: 201px;"><nobr>Smagt[54] or Anderson[5].</nobr></div>
<div style="position: absolute; top: 38944px; left: 223px;"><nobr>A neural network is an information processing system loosely based on the opera-</nobr></div>
<div style="position: absolute; top: 38971px; left: 201px;"><nobr>tion of neurons in the brain. While the neuron acts as the fundamental functional unit</nobr></div>
<div style="position: absolute; top: 38998px; left: 201px;"><nobr>of the brain, the neural network uses the node as its fundamental unit; the nodes are</nobr></div>
<div style="position: absolute; top: 39025px; left: 201px;"><nobr>connected by links, and the links have an associated weight that can act as a storage</nobr></div>
<div style="position: absolute; top: 39052px; left: 201px;"><nobr>mechanism[93]. Each node is considered a single computational unit containing two</nobr></div>
<div style="position: absolute; top: 39079px; left: 201px;"><nobr>components. The first component is the input function which computes the weighted</nobr></div>
<div style="position: absolute; top: 39106px; left: 201px;"><nobr>sum of its input values; the second is the activation function, which transforms the</nobr></div>
<div style="position: absolute; top: 39133px; left: 201px;"><nobr>weighted sum into a final output value. Many different activation functions can be</nobr></div>
<div style="position: absolute; top: 39159px; left: 201px;"><nobr>used; the step, sign, and sigmoid functions being quite common[93] since they are all</nobr></div>
<div style="position: absolute; top: 39186px; left: 201px;"><nobr>relatively simple to use. For example, using the step function, if the weighted sum is</nobr></div>
<div style="position: absolute; top: 39231px; left: 451px;"><nobr>30</nobr></div>
</span></font>

<div style="position: absolute; top: 39379px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="35"><b>Page 35</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 39568px; left: 201px;"><nobr>above a certain threshold, the function outputs a one indicating the node has “fired”</nobr></div>
<div style="position: absolute; top: 39594px; left: 201px;"><nobr>otherwise it outputs a zero indicating the node has not fired. The other two activation</nobr></div>
<div style="position: absolute; top: 39621px; left: 201px;"><nobr>functions act in a similar manner.</nobr></div>
<div style="position: absolute; top: 39648px; left: 223px;"><nobr>Neural networks generally have two basic structures or topologies, a feed-forward</nobr></div>
<div style="position: absolute; top: 39675px; left: 201px;"><nobr>structure and a recurrent structure. A feed-forward network can be considered a di-</nobr></div>
<div style="position: absolute; top: 39702px; left: 201px;"><nobr>rected acyclic graph, while a recurrent network has an arbitrary topology. The recurrent</nobr></div>
<div style="position: absolute; top: 39729px; left: 201px;"><nobr>network has the advantage over a feed-forward network in that it can model systems</nobr></div>
<div style="position: absolute; top: 39756px; left: 201px;"><nobr>with state transitions. However, recurrent networks require more complex mathemat-</nobr></div>
<div style="position: absolute; top: 39782px; left: 201px;"><nobr>ical descriptions and can exhibit chaotic behavior. In both network topologies, there</nobr></div>
<div style="position: absolute; top: 39809px; left: 201px;"><nobr>is no restriction on the number of layers in the network. These multilayered networks</nobr></div>
<div style="position: absolute; top: 39836px; left: 201px;"><nobr>provide more representation power at the cost of more complex training. The nodes in</nobr></div>
<div style="position: absolute; top: 39863px; left: 201px;"><nobr>between the input and output nodes of the multilayered network have no communica-</nobr></div>
<div style="position: absolute; top: 39890px; left: 201px;"><nobr>tion with the outside world and cannot be directly observed from the input or output</nobr></div>
<div style="position: absolute; top: 39917px; left: 201px;"><nobr>behavior of the network. If the number of hidden nodes is large, it is possible to repre-</nobr></div>
<div style="position: absolute; top: 39944px; left: 201px;"><nobr>sent any continuous function of the inputs[93].</nobr></div>
<div style="position: absolute; top: 39971px; left: 223px;"><nobr>Training is an important issue in neural networks and can be classified in two dif-</nobr></div>
<div style="position: absolute; top: 39998px; left: 201px;"><nobr>ferent ways. First, supervised learning trains the network by providing matching input</nobr></div>
<div style="position: absolute; top: 40025px; left: 201px;"><nobr>and output patterns; this trains the network in advance and as a result the network does</nobr></div>
<div style="position: absolute; top: 40051px; left: 201px;"><nobr>not learn while it is running. The second learning mechanism is unsupervised learning</nobr></div>
<div style="position: absolute; top: 40078px; left: 201px;"><nobr>or self-organization which trains the network to respond to clusters of patterns within</nobr></div>
<div style="position: absolute; top: 40105px; left: 201px;"><nobr>the input. There is no training in advance and the system must develop its own repre-</nobr></div>
<div style="position: absolute; top: 40132px; left: 201px;"><nobr>sentation of the input, since no matching output is provided[54]. Note that supervised</nobr></div>
<div style="position: absolute; top: 40159px; left: 201px;"><nobr>and unsupervised learning do not have to be mutually exclusive: depending on the net-</nobr></div>
<div style="position: absolute; top: 40186px; left: 201px;"><nobr>work, a combination of the two learning strategies can be employed. Neural network</nobr></div>
<div style="position: absolute; top: 40213px; left: 201px;"><nobr>training is one of most important areas of research in neural network technology, but</nobr></div>
<div style="position: absolute; top: 40240px; left: 201px;"><nobr>the many different algorithms for supervised and unsupervised learning strategies are</nobr></div>
<div style="position: absolute; top: 40267px; left: 201px;"><nobr>beyond the scope of this survey; for a thorough discussion see Mehrotra, Mohan, and</nobr></div>
<div style="position: absolute; top: 40294px; left: 201px;"><nobr>Ranka[71].</nobr></div>
<div style="position: absolute; top: 40321px; left: 223px;"><nobr>Neural networks have been used principally in the artificial intelligence commu-</nobr></div>
<div style="position: absolute; top: 40347px; left: 201px;"><nobr>nity to build certain types of autonomous agents and recognize patterns. One of the</nobr></div>
<div style="position: absolute; top: 40374px; left: 201px;"><nobr>first systems to use neural networks in hand posture and gesture recognition was de-</nobr></div>
<div style="position: absolute; top: 40419px; left: 451px;"><nobr>31</nobr></div>
</span></font>

<div style="position: absolute; top: 40567px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="36"><b>Page 36</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 40756px; left: 201px;"><nobr>veloped by Murakami[77]. Hand postures were recognized with a three-layered neural</nobr></div>
<div style="position: absolute; top: 40782px; left: 201px;"><nobr>network that contained 13 input nodes, 100 hidden nodes, and 42 output nodes, one</nobr></div>
<div style="position: absolute; top: 40809px; left: 201px;"><nobr>for each posture to be recognized. The network used back propagation, a learning</nobr></div>
<div style="position: absolute; top: 40836px; left: 201px;"><nobr>mechanism that minimizes the error between target output and the output produced</nobr></div>
<div style="position: absolute; top: 40863px; left: 201px;"><nobr>by the network[93], and achieved 77% accuracy with an initial training set. Accuracy</nobr></div>
<div style="position: absolute; top: 40890px; left: 201px;"><nobr>increased to 98% for participants in the original training set when the number of train-</nobr></div>
<div style="position: absolute; top: 40917px; left: 201px;"><nobr>ing patterns was increased from 42 to 206. Hand gesture recognition was done with a</nobr></div>
<div style="position: absolute; top: 40944px; left: 201px;"><nobr>recurrent three-layered network with 16 input units, 150 hidden units, and 10 output</nobr></div>
<div style="position: absolute; top: 40970px; left: 201px;"><nobr>units, one for each of the 10 possible gestures recognized. Recurrency in the network</nobr></div>
<div style="position: absolute; top: 40997px; left: 201px;"><nobr>allowed for processing of time variant data. Recognition accuracy was initially 80%,</nobr></div>
<div style="position: absolute; top: 41024px; left: 201px;"><nobr>but 96% recognition rates were achieved with a filtering mechanism for the raw data</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 41023px; left: 705px;"><nobr>3</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 41024px; left: 711px;"><nobr>.</nobr></div>
<div style="position: absolute; top: 41051px; left: 223px;"><nobr>Fels did extensive work with neural networks with his Glove-TalkII system[34] in</nobr></div>
<div style="position: absolute; top: 41078px; left: 201px;"><nobr>which three back propagation neural networks are used to translate hand gestures into</nobr></div>
<div style="position: absolute; top: 41105px; left: 201px;"><nobr>speech. The hand gesture recognition process was broken up into three networks in or-</nobr></div>
<div style="position: absolute; top: 41132px; left: 201px;"><nobr>der to increase speed and reduce training time. The first network, the vowel/consonant</nobr></div>
<div style="position: absolute; top: 41159px; left: 201px;"><nobr>network, determined if the user wanted to produce a vowel or a consonant. This net-</nobr></div>
<div style="position: absolute; top: 41186px; left: 201px;"><nobr>work employed a feed-forward topology with 12 input nodes, 10 hidden nodes and</nobr></div>
<div style="position: absolute; top: 41213px; left: 201px;"><nobr>one output node. The second network was used to generate consonant sounds. It also</nobr></div>
<div style="position: absolute; top: 41239px; left: 201px;"><nobr>employed a feed-forward topology but used 12 input nodes, 15 hidden nodes, and nine</nobr></div>
<div style="position: absolute; top: 41266px; left: 201px;"><nobr>output nodes. The third network used to generate vowel sounds, had a feed-forward</nobr></div>
<div style="position: absolute; top: 41293px; left: 201px;"><nobr>structure and employed two input nodes, 11 hidden nodes, and eight output nodes. The</nobr></div>
<div style="position: absolute; top: 41320px; left: 201px;"><nobr>consonant and vowel networks used normalized radial basis activation functions[17]</nobr></div>
<div style="position: absolute; top: 41347px; left: 201px;"><nobr>for the hidden inputs that solved problems arising from similar-sounding consonants</nobr></div>
<div style="position: absolute; top: 41374px; left: 201px;"><nobr>and vowels (see Fels[35]). With these networks, a well-trained user (100 hours of train-</nobr></div>
<div style="position: absolute; top: 41401px; left: 201px;"><nobr>ing, including training the networks) was able to make intelligible speech that sounded</nobr></div>
<div style="position: absolute; top: 41428px; left: 201px;"><nobr>somewhat natural.</nobr></div>
<div style="position: absolute; top: 41455px; left: 223px;"><nobr>Another system using neural networks developed by Banarse[8] was vision-based</nobr></div>
<div style="position: absolute; top: 41482px; left: 201px;"><nobr>and recognized hand postures using a neocognitron network, a neural network based</nobr></div>
<div style="position: absolute; top: 41509px; left: 201px;"><nobr>on the spatial recognition system of the visual cortex of the brain (for a detailed de-</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 41537px; left: 217px;"><nobr>3</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 41538px; left: 222px;"><nobr>See Murakami for details on this mechanism[77].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 41607px; left: 451px;"><nobr>32</nobr></div>
</span></font>

<div style="position: absolute; top: 41755px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="37"><b>Page 37</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 41944px; left: 201px;"><nobr>scription of the neocognitron see Fukushima[38]). However, the system was limited</nobr></div>
<div style="position: absolute; top: 41970px; left: 201px;"><nobr>and recognized only a handful of postures. More extensive research is needed to deter-</nobr></div>
<div style="position: absolute; top: 41997px; left: 201px;"><nobr>mine the validityof the neocognitronnetwork as a hand posture and gesture recognition</nobr></div>
<div style="position: absolute; top: 42024px; left: 201px;"><nobr>technique.</nobr></div>
<div style="position: absolute; top: 42051px; left: 223px;"><nobr>Neural networks are a useful method for recognizing hand postures and gestures,</nobr></div>
<div style="position: absolute; top: 42078px; left: 201px;"><nobr>yield increased accuracy conditioned upon network training, and work for both glove-</nobr></div>
<div style="position: absolute; top: 42105px; left: 201px;"><nobr>based and vision-based solutions. However, they have distinct disadvantages. First,</nobr></div>
<div style="position: absolute; top: 42132px; left: 201px;"><nobr>different configurations of a given network can give very different results, and it is dif-</nobr></div>
<div style="position: absolute; top: 42158px; left: 201px;"><nobr>ficult to determine which configuration is best without implementing them: Fels[35]</nobr></div>
<div style="position: absolute; top: 42185px; left: 201px;"><nobr>reports that he implemented many different network configurations before finding ones</nobr></div>
<div style="position: absolute; top: 42212px; left: 201px;"><nobr>that provided good results. Another disadvantage is the considerable time involved in</nobr></div>
<div style="position: absolute; top: 42239px; left: 201px;"><nobr>training the network. Finally, the whole network must be retrained in order to incorpo-</nobr></div>
<div style="position: absolute; top: 42266px; left: 201px;"><nobr>rate a new posture or gesture. If the posture or gesture set is known beforehand this is</nobr></div>
<div style="position: absolute; top: 42293px; left: 201px;"><nobr>not an issue, but if postures and gestures are likely to change dynamically as the system</nobr></div>
<div style="position: absolute; top: 42320px; left: 201px;"><nobr>develops, a neural network is probably not appropriate.</nobr></div>
<div style="position: absolute; top: 42354px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 42390px; left: 254px;"><nobr><b>I. Can be used in either a vision- or glove-based solution</b></nobr></div>
<div style="position: absolute; top: 42421px; left: 248px;"><nobr><b>II. Can recognize large posture or gesture sets</b></nobr></div>
<div style="position: absolute; top: 42451px; left: 242px;"><nobr><b>III. With adequate training, high accuracy can be achieved</b></nobr></div>
<div style="position: absolute; top: 42487px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 42523px; left: 254px;"><nobr><b>I. Network training can be very time consuming and does not guarantee</b></nobr></div>
<div style="position: absolute; top: 42550px; left: 271px;"><nobr><b>good results</b></nobr></div>
<div style="position: absolute; top: 42580px; left: 248px;"><nobr><b>II. Requires retraining of the entire network if hand postures or gestures</b></nobr></div>
<div style="position: absolute; top: 42607px; left: 271px;"><nobr><b>are added or removed</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 42656px; left: 201px;"><nobr><b>3.2.2 Hidden Markov Models</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 42697px; left: 201px;"><nobr>We first give a brief introduction of the concepts involved in hidden Markov models.</nobr></div>
<div style="position: absolute; top: 42723px; left: 201px;"><nobr>For a more detailed description see Rabiner and Juang[88], Rabiner[89], Huang et al.</nobr></div>
<div style="position: absolute; top: 42750px; left: 201px;"><nobr>[45], or Charniak[21].</nobr></div>
<div style="position: absolute; top: 42795px; left: 451px;"><nobr>33</nobr></div>
</span></font>

<div style="position: absolute; top: 42943px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="38"><b>Page 38</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 43132px; left: 223px;"><nobr>In describing hidden Markov models it is convenient first to consider Markov</nobr></div>
<div style="position: absolute; top: 43158px; left: 201px;"><nobr>chains. Markov chains are simply finite-state automata in which each state transi-</nobr></div>
<div style="position: absolute; top: 43185px; left: 201px;"><nobr>tion arc has an associated probability value; the probability values of the arcs leaving</nobr></div>
<div style="position: absolute; top: 43212px; left: 201px;"><nobr>a single state sum to one. Markov chains impose the restriction on the finite-state au-</nobr></div>
<div style="position: absolute; top: 43239px; left: 201px;"><nobr>tomaton that a state can have only one transition arc with a given output; a restriction</nobr></div>
<div style="position: absolute; top: 43266px; left: 201px;"><nobr>that makes Markov chains deterministic. A hidden Markov model (HMM) can be con-</nobr></div>
<div style="position: absolute; top: 43293px; left: 201px;"><nobr>sidered a generalization of a Markov chain without this Markov-chain restriction[21].</nobr></div>
<div style="position: absolute; top: 43320px; left: 201px;"><nobr>Since HMMs can have more than one arc with the same output symbol, they are non-</nobr></div>
<div style="position: absolute; top: 43346px; left: 201px;"><nobr>deterministic, and it is impossible to directly determine the state sequence for a set of</nobr></div>
<div style="position: absolute; top: 43373px; left: 201px;"><nobr>inputs simply by lookingat the output (hence the “hidden” in “hidden Markov model”).</nobr></div>
<div style="position: absolute; top: 43400px; left: 223px;"><nobr>More formally, a HMM is defined as a set of states of which one state is the initial</nobr></div>
<div style="position: absolute; top: 43427px; left: 201px;"><nobr>state, a set of output symbols, and a set of state transitions. Each state transition is</nobr></div>
<div style="position: absolute; top: 43454px; left: 201px;"><nobr>represented by the state from which the transition starts, the state to which transition</nobr></div>
<div style="position: absolute; top: 43481px; left: 201px;"><nobr>moves, the output symbol generated, and the probabilitythat the transitionis taken[21].</nobr></div>
<div style="position: absolute; top: 43508px; left: 201px;"><nobr>In the context of hand gesture recognition, each state could represent a set of possible</nobr></div>
<div style="position: absolute; top: 43535px; left: 201px;"><nobr>hand positions. The state transitions represent the probability that a certain hand po-</nobr></div>
<div style="position: absolute; top: 43562px; left: 201px;"><nobr>sition transitions into another; the corresponding output symbol represents a specific</nobr></div>
<div style="position: absolute; top: 43589px; left: 201px;"><nobr>posture and a sequence of output symbols represent a hand gesture. One then uses a</nobr></div>
<div style="position: absolute; top: 43615px; left: 201px;"><nobr>group of HMMs, one for each gesture, and runs a sequence of input data through each</nobr></div>
<div style="position: absolute; top: 43642px; left: 201px;"><nobr>HMM. The input data, derived from pixels in a vision-based solution or from bend</nobr></div>
<div style="position: absolute; top: 43669px; left: 201px;"><nobr>sensor values in a glove-based solution, can be represented in many different ways, the</nobr></div>
<div style="position: absolute; top: 43696px; left: 201px;"><nobr>most common by feature vectors[97]. The HMM with the highest forward probability</nobr></div>
<div style="position: absolute; top: 43723px; left: 201px;"><nobr>(described later in this section) determines the users’ most likely gesture. An HMM</nobr></div>
<div style="position: absolute; top: 43750px; left: 201px;"><nobr>can also be used for hand posture recognition; see Liang and Ouhyoung[61] for details.</nobr></div>
<div style="position: absolute; top: 43777px; left: 223px;"><nobr>A number of important issues arise in dealing with HMMs. As with neural net-</nobr></div>
<div style="position: absolute; top: 43804px; left: 201px;"><nobr>works, training HMMs is very important for increasing recognition accuracy of the</nobr></div>
<div style="position: absolute; top: 43831px; left: 201px;"><nobr>model. A common approach is to adjust the HMM’s transition probabilities in order to</nobr></div>
<div style="position: absolute; top: 43858px; left: 201px;"><nobr>optimize it for a training data set. If the training data is an accurate representation of</nobr></div>
<div style="position: absolute; top: 43885px; left: 201px;"><nobr>a particular gesture, for example, then the HMM should be able to recognize that ges-</nobr></div>
<div style="position: absolute; top: 43911px; left: 201px;"><nobr>ture given new data. The Baum-Welch algorithm[21][88][89] uses the given training</nobr></div>
<div style="position: absolute; top: 43938px; left: 201px;"><nobr>sequence to reestimate the probabilities of the state transitions in the HMM.</nobr></div>
<div style="position: absolute; top: 43983px; left: 451px;"><nobr>34</nobr></div>
</span></font>

<div style="position: absolute; top: 44131px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="39"><b>Page 39</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 44320px; left: 223px;"><nobr>One of the components of the Baum-Welch algorithm is forward probability. For-</nobr></div>
<div style="position: absolute; top: 44346px; left: 201px;"><nobr>ward probability, or alpha probability as it is sometimes called, is the probability of an</nobr></div>
<div style="position: absolute; top: 44373px; left: 201px;"><nobr>HMM given an outputsequence and is calculated by incrementally computing the prob-</nobr></div>
<div style="position: absolute; top: 44400px; left: 201px;"><nobr>abilities for the output on a symbol-by-symbol basis[88]. The algorithm goes through</nobr></div>
<div style="position: absolute; top: 44427px; left: 201px;"><nobr>each timestep <i>t </i>and examines each state <i>j </i>in the HMM. For each state <i>j</i>, it computes a</nobr></div>
<div style="position: absolute; top: 44454px; left: 201px;"><nobr>summation of the probability of producing the current output symbol at <i>t </i>and moving</nobr></div>
<div style="position: absolute; top: 44481px; left: 201px;"><nobr>to <i>j </i>given that the algorithm was in state <i>k</i>, multiplied by the probability of producing</nobr></div>
<div style="position: absolute; top: 44508px; left: 201px;"><nobr>the output up to <i>t </i>and ending up in <i>k</i>. Note that the summation is over all <i>k</i>. Performing</nobr></div>
<div style="position: absolute; top: 44534px; left: 201px;"><nobr>this calculation iteratively for each output symbol yields the probability that the HMM</nobr></div>
<div style="position: absolute; top: 44561px; left: 201px;"><nobr>would generate the whole output sequence. The forward probability is used to find a</nobr></div>
<div style="position: absolute; top: 44588px; left: 201px;"><nobr>HMM with the highest probability of matching an output sequence. For example, if a</nobr></div>
<div style="position: absolute; top: 44615px; left: 201px;"><nobr>hand gesture set contains 10 gestures, each one having its own HMM, the HMM with</nobr></div>
<div style="position: absolute; top: 44642px; left: 201px;"><nobr>the highest forward probability score would determine which gesture to recognize. See</nobr></div>
<div style="position: absolute; top: 44669px; left: 201px;"><nobr>Charniak[21] for a more detailed description of forward probability.</nobr></div>
<div style="position: absolute; top: 44696px; left: 223px;"><nobr>As stated previously, one of the drawbacks of HMMs is that one cannot directly</nobr></div>
<div style="position: absolute; top: 44723px; left: 201px;"><nobr>determine the state sequence for a set of output symbols since a state can have more</nobr></div>
<div style="position: absolute; top: 44750px; left: 201px;"><nobr>than one arc with the same output. Nevertheless, this information can be approximated</nobr></div>
<div style="position: absolute; top: 44777px; left: 201px;"><nobr>by finding the most likely states in the HMM. A common technique for finding the best</nobr></div>
<div style="position: absolute; top: 44803px; left: 201px;"><nobr>state sequence for a given output, the Viterbi algorithm[88], uses calculations similar to</nobr></div>
<div style="position: absolute; top: 44830px; left: 201px;"><nobr>forward probability except the maximum is taken instead of taking a summation over</nobr></div>
<div style="position: absolute; top: 44857px; left: 201px;"><nobr>all <i>k</i>. The Viterbi algorithm is useful because it is a fast method for evaluating HMMs.</nobr></div>
<div style="position: absolute; top: 44884px; left: 201px;"><nobr>For more detail on the Viterbi algorithm see Charniak[21].</nobr></div>
<div style="position: absolute; top: 44911px; left: 223px;"><nobr>Hidden Markov models were first used in the recognition community for recog-</nobr></div>
<div style="position: absolute; top: 44938px; left: 201px;"><nobr>nizing handwriting[109] and speech[45], and more recently a significant amount of</nobr></div>
<div style="position: absolute; top: 44965px; left: 201px;"><nobr>research has been done on applying HMMs to hand posture and gesture recognition.</nobr></div>
<div style="position: absolute; top: 44992px; left: 201px;"><nobr>Starner used HMMs in a vision-based solution to recognize a forty word subset of</nobr></div>
<div style="position: absolute; top: 45019px; left: 201px;"><nobr>American Sign Language[97]. Instead of using a different model for each sign in the</nobr></div>
<div style="position: absolute; top: 45046px; left: 201px;"><nobr>recognition set, Starner found the minimum and maximum number of states required</nobr></div>
<div style="position: absolute; top: 45073px; left: 201px;"><nobr>for an individual HMM and then, using skip transitions (which give low weights to</nobr></div>
<div style="position: absolute; top: 45099px; left: 201px;"><nobr>state transitions that are not needed) developed a general HMM topology for all mod-</nobr></div>
<div style="position: absolute; top: 45126px; left: 201px;"><nobr>els used in the system. With ample training of the HMMs (between 20 and 80 training</nobr></div>
<div style="position: absolute; top: 45171px; left: 451px;"><nobr>35</nobr></div>
</span></font>

<div style="position: absolute; top: 45319px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="40"><b>Page 40</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 45508px; left: 201px;"><nobr>samples for each sign) the system was able to achieve an accuracy of over 90 percent.</nobr></div>
<div style="position: absolute; top: 45534px; left: 223px;"><nobr>Schlenzig also used a single HMM to recognize hand gestures in a vision-based</nobr></div>
<div style="position: absolute; top: 45561px; left: 201px;"><nobr>solution[94]. The state of the HMM represents the gestures and the observation sym-</nobr></div>
<div style="position: absolute; top: 45588px; left: 201px;"><nobr>bols represent the current static hand posture. This HMM had three possible states and</nobr></div>
<div style="position: absolute; top: 45615px; left: 201px;"><nobr>nine possible observation symbols so the number of recognizable gestures was lim-</nobr></div>
<div style="position: absolute; top: 45642px; left: 201px;"><nobr>ited. The system employs a recursive filter for updating estimates of the gesture being</nobr></div>
<div style="position: absolute; top: 45669px; left: 201px;"><nobr>recognized based on the current posture information. This recursive estimator allows</nobr></div>
<div style="position: absolute; top: 45696px; left: 201px;"><nobr>recognition of combined gestures and requires only one HMM.</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 7px; font-family: Times;">
<div style="position: absolute; top: 45868px; left: 301px;"><nobr>1</nobr></div>
<div style="position: absolute; top: 45868px; left: 396px;"><nobr>2</nobr></div>
<div style="position: absolute; top: 45868px; left: 495px;"><nobr>3</nobr></div>
<div style="position: absolute; top: 45868px; left: 605px;"><nobr>4</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 11px; font-family: Times;">
<div style="position: absolute; top: 45750px; left: 354px;"><nobr>A 4 State Bakis Hidden Markov Model</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 46026px; left: 347px;"><nobr>Figure 3.1: A four state Bakis HMM.</nobr></div>
<div style="position: absolute; top: 46071px; left: 223px;"><nobr>Lee and Xu were able to recognize a set of 14 gestures in the context of robot</nobr></div>
<div style="position: absolute; top: 46098px; left: 201px;"><nobr>teleoperation[60]. The system uses a CyberGlove and implements a Bakis hidden</nobr></div>
<div style="position: absolute; top: 46125px; left: 201px;"><nobr>Markov model that restricts state transitions so that a given state can have a transi-</nobr></div>
<div style="position: absolute; top: 46152px; left: 201px;"><nobr>tion only to itself or one of the next two states; for example, if the HMM has four</nobr></div>
<div style="position: absolute; top: 46179px; left: 201px;"><nobr>states, state two could have a transition arc only to itself, state three, or state four (see</nobr></div>
<div style="position: absolute; top: 46206px; left: 201px;"><nobr>Figure 3.1). This type of model assumes that the recognizable gestures have no cycli-</nobr></div>
<div style="position: absolute; top: 46232px; left: 201px;"><nobr>cal properties. The system also performed iterative model updates as each gesture is</nobr></div>
<div style="position: absolute; top: 46259px; left: 201px;"><nobr>recognized. Recognition accuracy of over 90 percent was achieved.</nobr></div>
<div style="position: absolute; top: 46286px; left: 223px;"><nobr>Nam and Wohn[79] and Liang and Ouhyoung[61] have also explored the use of</nobr></div>
<div style="position: absolute; top: 46313px; left: 201px;"><nobr>HMMs in hand posture and gesture recognition. Nam and Wohn use a glove-based so-</nobr></div>
<div style="position: absolute; top: 46359px; left: 451px;"><nobr>36</nobr></div>
</span></font>

<div style="position: absolute; top: 46507px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="41"><b>Page 41</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 46696px; left: 201px;"><nobr>lution with a VPL DataGlove and Polhemus tracker and reduce the dimensional com-</nobr></div>
<div style="position: absolute; top: 46722px; left: 201px;"><nobr>plexity of the tracking device from 6D (three positionaxes and three orientationaxes) to</nobr></div>
<div style="position: absolute; top: 46749px; left: 201px;"><nobr>2D by using plane fitting. The 2D information is then sent to the HMM. After training</nobr></div>
<div style="position: absolute; top: 46776px; left: 201px;"><nobr>each of the 10 HMMs, one for each gesture, with approximately 200 training sam-</nobr></div>
<div style="position: absolute; top: 46803px; left: 201px;"><nobr>ples per gesture, accuracy of over 96 percent was attained. Liang and Ouhyoung use</nobr></div>
<div style="position: absolute; top: 46830px; left: 201px;"><nobr>HMMs to recognize 50 signs in Taiwanese Sign Language. The system uses a <i>n</i>-best</nobr></div>
<div style="position: absolute; top: 46857px; left: 201px;"><nobr>approach to outputting recognition results. Unfortunately, no recognition accuracies</nobr></div>
<div style="position: absolute; top: 46884px; left: 201px;"><nobr>were reported.</nobr></div>
<div style="position: absolute; top: 46910px; left: 223px;"><nobr>Hidden Markov models provide a good way to perform hand posture and gesture</nobr></div>
<div style="position: absolute; top: 46937px; left: 201px;"><nobr>recognition, and can be used in both vision-based and glove-based solutions. The lit-</nobr></div>
<div style="position: absolute; top: 46964px; left: 201px;"><nobr>erature has shown that high accuracy can be achieved and the number of possible hand</nobr></div>
<div style="position: absolute; top: 46991px; left: 201px;"><nobr>gestures or postures in a posture or gesture set can be quite large. Like neural net-</nobr></div>
<div style="position: absolute; top: 47018px; left: 201px;"><nobr>works, HMMs must be trained and the correct number of states for each posture or</nobr></div>
<div style="position: absolute; top: 47045px; left: 201px;"><nobr>gesture must be determined to maximize performance. If the number and types of</nobr></div>
<div style="position: absolute; top: 47072px; left: 201px;"><nobr>hand posture and gestures are known beforehand, HMMs are a good choice for recog-</nobr></div>
<div style="position: absolute; top: 47099px; left: 201px;"><nobr>nition. If the hand postures and gestures are determined as the system is developed, the</nobr></div>
<div style="position: absolute; top: 47126px; left: 201px;"><nobr>development process can be more time-consuming due to retraining. If one HMM is</nobr></div>
<div style="position: absolute; top: 47153px; left: 201px;"><nobr>used for all gestures, as in Starner’s work, then the single HMM must be retrained. If</nobr></div>
<div style="position: absolute; top: 47179px; left: 201px;"><nobr>each gesture has an associated HMM, then only the new gesture’s HMM will have to</nobr></div>
<div style="position: absolute; top: 47206px; left: 201px;"><nobr>be trained. Although HMMs require extensive training, and their hidden nature makes</nobr></div>
<div style="position: absolute; top: 47233px; left: 201px;"><nobr>it difficult to understand what is occurring within them, they still may be the technique</nobr></div>
<div style="position: absolute; top: 47260px; left: 201px;"><nobr>of choice since they are well covered in the literature and the accuracies reported are</nobr></div>
<div style="position: absolute; top: 47287px; left: 201px;"><nobr>usually above 90 percent.</nobr></div>
<div style="position: absolute; top: 47329px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 47367px; left: 254px;"><nobr><b>I. Can be used in either a vision- or glove-based solution</b></nobr></div>
<div style="position: absolute; top: 47400px; left: 248px;"><nobr><b>II. Can recognize large posture or gesture sets</b></nobr></div>
<div style="position: absolute; top: 47433px; left: 242px;"><nobr><b>III. With adequate training, high accuracy can be achieved</b></nobr></div>
<div style="position: absolute; top: 47466px; left: 245px;"><nobr><b>IV. Well discussed in the literature</b></nobr></div>
<div style="position: absolute; top: 47547px; left: 451px;"><nobr>37</nobr></div>
</span></font>

<div style="position: absolute; top: 47695px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="42"><b>Page 42</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 47883px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 47921px; left: 254px;"><nobr><b>I. Training can be time consuming and does not guarantee good results</b></nobr></div>
<div style="position: absolute; top: 47954px; left: 248px;"><nobr><b>II. As with multi-levelneural networks, the hidden nature of HMMs makes</b></nobr></div>
<div style="position: absolute; top: 47981px; left: 271px;"><nobr><b>it difficult to observe their internal behavior</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 48031px; left: 201px;"><nobr><b>3.2.3 Instance-Based Learning</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 48072px; left: 201px;"><nobr>Instance-based learning is another recognition technique that stems from work done</nobr></div>
<div style="position: absolute; top: 48098px; left: 201px;"><nobr>in machine learning. The main difference between instance-based learning and other</nobr></div>
<div style="position: absolute; top: 48125px; left: 201px;"><nobr>learning algorithms such as neural networks and hidden Markov models is the way</nobr></div>
<div style="position: absolute; top: 48152px; left: 201px;"><nobr>in which the training data is used. With supervised neural networks, for example,</nobr></div>
<div style="position: absolute; top: 48179px; left: 201px;"><nobr>the training data is passed through the network and the weights at various nodes are</nobr></div>
<div style="position: absolute; top: 48206px; left: 201px;"><nobr>updated to fit the training set. With instance-based learning, the training data is simply</nobr></div>
<div style="position: absolute; top: 48233px; left: 201px;"><nobr>used as a database in which to classify other “instances”. An instance, in general, is</nobr></div>
<div style="position: absolute; top: 48260px; left: 201px;"><nobr>a vector of features of the entity to be classified. For instance, in posture and gesture</nobr></div>
<div style="position: absolute; top: 48287px; left: 201px;"><nobr>recognition, a feature vector might be the position and orientation of the hand and the</nobr></div>
<div style="position: absolute; top: 48314px; left: 201px;"><nobr>bend values for each of the fingers.</nobr></div>
<div style="position: absolute; top: 48341px; left: 223px;"><nobr>Instance-based learning methods include techniques that represent instances as</nobr></div>
<div style="position: absolute; top: 48367px; left: 201px;"><nobr>points in Euclidean space, such as the <i>K</i>-Nearest Neighbor algorithm, and techniques in</nobr></div>
<div style="position: absolute; top: 48394px; left: 201px;"><nobr>which instances have a more symbolic representation, such as case-based reasoning[74].</nobr></div>
<div style="position: absolute; top: 48421px; left: 201px;"><nobr>In the <i>K</i>-Nearest Neighbor algorithm, an instance is a feature vector of size <i>n </i>with</nobr></div>
<div style="position: absolute; top: 48448px; left: 201px;"><nobr>points in <i>n</i>-dimensional space. The training phase of the algorithm involves storing a</nobr></div>
<div style="position: absolute; top: 48475px; left: 201px;"><nobr>set of representative instances in a list of training examples. For each new record, the</nobr></div>
<div style="position: absolute; top: 48502px; left: 201px;"><nobr>Euclidean distance is computed from each instance in the training example list, and the</nobr></div>
<div style="position: absolute; top: 48529px; left: 201px;"><nobr><i>K </i>closest instances to the new instance are returned. The new instance is then classified</nobr></div>
<div style="position: absolute; top: 48556px; left: 201px;"><nobr>and added to the training example list so that training can be continuous. In the case of</nobr></div>
<div style="position: absolute; top: 48583px; left: 201px;"><nobr>hand posture recognition, the training set would be divided into a number of categories</nobr></div>
<div style="position: absolute; top: 48610px; left: 201px;"><nobr>based on the types of recognizable postures. As a new posture instance is entered, its <i>K</i></nobr></div>
<div style="position: absolute; top: 48637px; left: 201px;"><nobr>nearest neighbors are found and used to determine the category in which the instance</nobr></div>
<div style="position: absolute; top: 48663px; left: 201px;"><nobr>should be placed (thus recognizing the instance as a particular posture).</nobr></div>
<div style="position: absolute; top: 48690px; left: 223px;"><nobr>Another type of instance-based learning technique is case-based reasoning, in which</nobr></div>
<div style="position: absolute; top: 48735px; left: 451px;"><nobr>38</nobr></div>
</span></font>

<div style="position: absolute; top: 48883px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="43"><b>Page 43</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 49072px; left: 201px;"><nobr>instances have more elaborate descriptions. A typical instance in a case-based reason-</nobr></div>
<div style="position: absolute; top: 49098px; left: 201px;"><nobr>ing system might be a description of a mechanical part. Since the instance descriptions</nobr></div>
<div style="position: absolute; top: 49125px; left: 201px;"><nobr>are more elaborate, simple approaches to determining instance similarity, such as the</nobr></div>
<div style="position: absolute; top: 49152px; left: 201px;"><nobr>Euclidean distance measure, are not applicable. Other methods for determining sim-</nobr></div>
<div style="position: absolute; top: 49179px; left: 201px;"><nobr>ilarity between instances must be used, such as those found in knowledge base tech-</nobr></div>
<div style="position: absolute; top: 49206px; left: 201px;"><nobr>niques. See Mitchell for more detail on case-based reasoning, the <i>K</i>-Nearest Neighbor</nobr></div>
<div style="position: absolute; top: 49233px; left: 201px;"><nobr>algorithm, and other instance-based learning algorithms[74].</nobr></div>
<div style="position: absolute; top: 49260px; left: 223px;"><nobr>Instance-based learning techniques have the advantage of simplicity, but they have a</nobr></div>
<div style="position: absolute; top: 49286px; left: 201px;"><nobr>number of disadvantages as well. One major disadvantage is the cost of classifying new</nobr></div>
<div style="position: absolute; top: 49313px; left: 201px;"><nobr>instances. All of the computation must be done whenever a new instance is classified,</nobr></div>
<div style="position: absolute; top: 49340px; left: 201px;"><nobr>which means there will be response time issues when dealing with a large amount of</nobr></div>
<div style="position: absolute; top: 49367px; left: 201px;"><nobr>training examples. Another disadvantage of these methods is that not all of the training</nobr></div>
<div style="position: absolute; top: 49394px; left: 201px;"><nobr>examples may fit in main memory, and thus will also increase response time. Note</nobr></div>
<div style="position: absolute; top: 49421px; left: 201px;"><nobr>that Aha has developed two instance-based learning algorithms that alleviate some of</nobr></div>
<div style="position: absolute; top: 49448px; left: 201px;"><nobr>these problems[2]. The first one saves space by discarding instances that have already</nobr></div>
<div style="position: absolute; top: 49475px; left: 201px;"><nobr>been correctly classified, and the second makes assumptions about the data to weed out</nobr></div>
<div style="position: absolute; top: 49502px; left: 201px;"><nobr>irrelevant instances.</nobr></div>
<div style="position: absolute; top: 49529px; left: 223px;"><nobr>Unfortunately, very little work has been done on instance-based learning in recog-</nobr></div>
<div style="position: absolute; top: 49555px; left: 201px;"><nobr>nizing hand postures and gestures. One of the few systems reported in the literature was</nobr></div>
<div style="position: absolute; top: 49582px; left: 201px;"><nobr>developed by Kadous[48], which recognized 95 discrete hand postures for performing</nobr></div>
<div style="position: absolute; top: 49609px; left: 201px;"><nobr>sign language recognition using the three instance-based learning algorithms described</nobr></div>
<div style="position: absolute; top: 49636px; left: 201px;"><nobr>by Aha[2]. An interesting feature of this system was its capability of achieving ap-</nobr></div>
<div style="position: absolute; top: 49663px; left: 201px;"><nobr>proximately 80% accuracy with the use of a Power Glove as the raw data collection</nobr></div>
<div style="position: absolute; top: 49690px; left: 201px;"><nobr>unit.</nobr></div>
<div style="position: absolute; top: 49717px; left: 223px;"><nobr>Instance based learning shows promise as a way to recognize hand postures. How-</nobr></div>
<div style="position: absolute; top: 49744px; left: 201px;"><nobr>ever, response time may be an important factor when issuing posture commands due to</nobr></div>
<div style="position: absolute; top: 49771px; left: 201px;"><nobr>the amount of computation required when each instance is generated, especially when</nobr></div>
<div style="position: absolute; top: 49798px; left: 201px;"><nobr>instances are generated at 30 or more per second (based on the speed of the input de-</nobr></div>
<div style="position: absolute; top: 49825px; left: 201px;"><nobr>vices used). More research is needed to determine whether the technique can be applied</nobr></div>
<div style="position: absolute; top: 49851px; left: 201px;"><nobr>to hand gestures and if the accuracy can be improved.</nobr></div>
<div style="position: absolute; top: 49923px; left: 451px;"><nobr>39</nobr></div>
</span></font>

<div style="position: absolute; top: 50071px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="44"><b>Page 44</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 50259px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 50297px; left: 254px;"><nobr><b>I. Except for case-based reasoning, instance-based learning techniques</b></nobr></div>
<div style="position: absolute; top: 50324px; left: 271px;"><nobr><b>are relatively simple to implement</b></nobr></div>
<div style="position: absolute; top: 50357px; left: 248px;"><nobr><b>II. Can recognize a large set of hand postures with moderately high accu-</b></nobr></div>
<div style="position: absolute; top: 50384px; left: 271px;"><nobr><b>racy</b></nobr></div>
<div style="position: absolute; top: 50417px; left: 242px;"><nobr><b>III. Provides continuous training</b></nobr></div>
<div style="position: absolute; top: 50456px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 50495px; left: 254px;"><nobr><b>I. Requires a large amount of primary memory as training set increases</b></nobr></div>
<div style="position: absolute; top: 50528px; left: 248px;"><nobr><b>II. Response time issues may arise due to a large amount of computation</b></nobr></div>
<div style="position: absolute; top: 50555px; left: 271px;"><nobr><b>at instance classification time</b></nobr></div>
<div style="position: absolute; top: 50588px; left: 242px;"><nobr><b>III. Only a little reported in the literature on using instance-based learning</b></nobr></div>
<div style="position: absolute; top: 50614px; left: 271px;"><nobr><b>with hand postures and gestures</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 50672px; left: 201px;"><nobr><b>3.3 Miscellaneous Techniques</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 50721px; left: 201px;"><nobr>This section presents three other techniques, for recognizing hand gestures and pos-</nobr></div>
<div style="position: absolute; top: 50748px; left: 201px;"><nobr>tures: the linguistic approach, appearance-based motion analysis and spatio-temporal</nobr></div>
<div style="position: absolute; top: 50775px; left: 201px;"><nobr>vector analysis (see Table 3.3 for a summary of these techniques).</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 5px; font-family: Times;">
<div style="position: absolute; top: 50823px; left: 307px;"><nobr>Vison</nobr></div>
<div style="position: absolute; top: 50823px; left: 342px;"><nobr>Glove</nobr></div>
<div style="position: absolute; top: 50823px; left: 378px;"><nobr>Postures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 50823px; left: 468px;"><nobr>Gestures-Size-Accuracy</nobr></div>
<div style="position: absolute; top: 50823px; left: 558px;"><nobr>Training</nobr></div>
<div style="position: absolute; top: 50823px; left: 601px;"><nobr>Previous Work</nobr></div>
<div style="position: absolute; top: 50823px; left: 664px;"><nobr>Adv. Knowledge</nobr></div>
<div style="position: absolute; top: 50850px; left: 210px;"><nobr>Linguistic Approach</nobr></div>
<div style="position: absolute; top: 50850px; left: 307px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 50850px; left: 342px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 50850px; left: 378px;"><nobr>Simple-Small-50%</nobr></div>
<div style="position: absolute; top: 50850px; left: 467px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 50850px; left: 558px;"><nobr>N/A</nobr></div>
<div style="position: absolute; top: 50850px; left: 601px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 50850px; left: 664px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 50878px; left: 210px;"><nobr>Appearance-based Motion</nobr></div>
<div style="position: absolute; top: 50878px; left: 307px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 50878px; left: 342px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 50878px; left: 378px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 50878px; left: 467px;"><nobr>N/A-N/A-N/A</nobr></div>
<div style="position: absolute; top: 50878px; left: 558px;"><nobr>N/A</nobr></div>
<div style="position: absolute; top: 50878px; left: 601px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 50878px; left: 664px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 50905px; left: 210px;"><nobr>Spatio Temporal Vectors</nobr></div>
<div style="position: absolute; top: 50905px; left: 307px;"><nobr>Yes</nobr></div>
<div style="position: absolute; top: 50905px; left: 342px;"><nobr>No</nobr></div>
<div style="position: absolute; top: 50905px; left: 378px;"><nobr>Complex-Medium-N/A</nobr></div>
<div style="position: absolute; top: 50905px; left: 467px;"><nobr>Complex-Medium-N/A</nobr></div>
<div style="position: absolute; top: 50905px; left: 558px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 50905px; left: 601px;"><nobr>Minimal</nobr></div>
<div style="position: absolute; top: 50905px; left: 664px;"><nobr>No</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 50941px; left: 201px;"><nobr>Table 3.3: A summary of the hand posture and gesture recognition techniques found</nobr></div>
<div style="position: absolute; top: 50959px; left: 201px;"><nobr>in Section 3.3. The table shows information about whether a technique has been used</nobr></div>
<div style="position: absolute; top: 50977px; left: 201px;"><nobr>in a glove- or vision-based solution, the posture and gesture complexity, set size, and</nobr></div>
<div style="position: absolute; top: 50995px; left: 201px;"><nobr>reported accuracy, the extent of the training required, how much work has been done</nobr></div>
<div style="position: absolute; top: 51013px; left: 201px;"><nobr>using the technique, and if it is important to have advance knowledge of the posture or</nobr></div>
<div style="position: absolute; top: 51031px; left: 201px;"><nobr>gesture set during implementation.</nobr></div>
<div style="position: absolute; top: 51111px; left: 451px;"><nobr>40</nobr></div>
</span></font>

<div style="position: absolute; top: 51259px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="45"><b>Page 45</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 51444px; left: 201px;"><nobr><b>3.3.1 The Linguistic Approach</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 51484px; left: 201px;"><nobr>The linguistic approach uses a formal grammar to represent the hand posture and ges-</nobr></div>
<div style="position: absolute; top: 51511px; left: 201px;"><nobr>ture set. Hand[43] used this approach to recognize a small set of postures under the</nobr></div>
<div style="position: absolute; top: 51538px; left: 201px;"><nobr>constraint that postures have fingers either fully flexed or fully extended in a number of</nobr></div>
<div style="position: absolute; top: 51565px; left: 201px;"><nobr>configurations. Hand postures were mapped to the grammar, which was specified by</nobr></div>
<div style="position: absolute; top: 51592px; left: 201px;"><nobr>a series of tokens and production rules. The system used a Power Glove and an ultra-</nobr></div>
<div style="position: absolute; top: 51619px; left: 201px;"><nobr>sonic tracker to record raw data. Recognition accuracy was poor, ranging from about</nobr></div>
<div style="position: absolute; top: 51646px; left: 201px;"><nobr>15 to 75 percent depending on the posture to be recognized. Hand[43] claims the poor</nobr></div>
<div style="position: absolute; top: 51673px; left: 201px;"><nobr>recognition rate may be due to the inaccuracy of the Power Glove; however, Kadous’s</nobr></div>
<div style="position: absolute; top: 51700px; left: 201px;"><nobr>work[48] with a Power Glove for instance-based learning (see section 3.2.3) achieved</nobr></div>
<div style="position: absolute; top: 51727px; left: 201px;"><nobr>a much higher recognition rate using a larger posture set.</nobr></div>
<div style="position: absolute; top: 51766px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 51804px; left: 254px;"><nobr><b>I. Simple approach</b></nobr></div>
<div style="position: absolute; top: 51836px; left: 248px;"><nobr><b>II. Can be used in either a vision- or glove-based solution</b></nobr></div>
<div style="position: absolute; top: 51874px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 51912px; left: 254px;"><nobr><b>I. Poor recognition results</b></nobr></div>
<div style="position: absolute; top: 51944px; left: 248px;"><nobr><b>II. Limited to only simple hand postures</b></nobr></div>
<div style="position: absolute; top: 51976px; left: 242px;"><nobr><b>III. Little work reported in the literature using this technique to recognize</b></nobr></div>
<div style="position: absolute; top: 52002px; left: 271px;"><nobr><b>hand postures and gestures</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 52053px; left: 201px;"><nobr><b>3.3.2 Appearance-Based Motion Analysis</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 52093px; left: 201px;"><nobr>Appearance-based motion analysis exploits the observation that humans can recognize</nobr></div>
<div style="position: absolute; top: 52120px; left: 201px;"><nobr>actions from extremely low resolution images and with little or no information about</nobr></div>
<div style="position: absolute; top: 52147px; left: 201px;"><nobr>the three-dimensional structure of the scene. Using this observation, Davis[25] devel-</nobr></div>
<div style="position: absolute; top: 52174px; left: 201px;"><nobr>oped an appearance-based recognition strategy for human motion using a hypothesize</nobr></div>
<div style="position: absolute; top: 52201px; left: 201px;"><nobr>and test paradigm[42]. This strategy recognizes motion patterns from a video stream</nobr></div>
<div style="position: absolute; top: 52227px; left: 201px;"><nobr>by first creating a filter called a binary motion region (BMR), which describes the spa-</nobr></div>
<div style="position: absolute; top: 52254px; left: 201px;"><nobr>tial distributionof motion energy for a given action. In other words, the filter highlights</nobr></div>
<div style="position: absolute; top: 52299px; left: 451px;"><nobr>41</nobr></div>
</span></font>

<div style="position: absolute; top: 52447px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="46"><b>Page 46</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 52636px; left: 201px;"><nobr>regions in an image in which any form of motion has been present since the beginning</nobr></div>
<div style="position: absolute; top: 52662px; left: 201px;"><nobr>of the action. The BMR acts as an index into a database of binary motion regions col-</nobr></div>
<div style="position: absolute; top: 52689px; left: 201px;"><nobr>lected from training sessions. If the current BMR is close to any of the stored BMRs,</nobr></div>
<div style="position: absolute; top: 52716px; left: 201px;"><nobr>then the current BMR is tested against a motion model of an action.</nobr></div>
<div style="position: absolute; top: 52743px; left: 223px;"><nobr>Once a set of possible actions has been found by the current BMR, the unknown</nobr></div>
<div style="position: absolute; top: 52770px; left: 201px;"><nobr>movement is classified as one of the predefined actions. Davis developed two ap-</nobr></div>
<div style="position: absolute; top: 52797px; left: 201px;"><nobr>proaches to represent actions for classification. The first approach creates specific</nobr></div>
<div style="position: absolute; top: 52824px; left: 201px;"><nobr>region-based motion parameterizations by reducing motion time traces of particular</nobr></div>
<div style="position: absolute; top: 52850px; left: 201px;"><nobr>regions of the image to a single coefficient vector where statistics are used for classi-</nobr></div>
<div style="position: absolute; top: 52877px; left: 201px;"><nobr>fication. The second approach breaks down the motion information of an action se-</nobr></div>
<div style="position: absolute; top: 52904px; left: 201px;"><nobr>quence into a single image based on a motion history image that is calculated using the</nobr></div>
<div style="position: absolute; top: 52931px; left: 201px;"><nobr>pixel intensity as a function of the motion history at a given location. Features from the</nobr></div>
<div style="position: absolute; top: 52958px; left: 201px;"><nobr>single image are extracted and matched (using the Mahalanobis distance</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 52957px; left: 638px;"><nobr>4</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 52958px; left: 648px;"><nobr>metric[32])</nobr></div>
<div style="position: absolute; top: 52985px; left: 201px;"><nobr>against known movement</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 52984px; left: 353px;"><nobr>5</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 52985px; left: 359px;"><nobr>.</nobr></div>
<div style="position: absolute; top: 53012px; left: 223px;"><nobr>The system was tested for a small set of motions (sitting, arm waving, and crouch-</nobr></div>
<div style="position: absolute; top: 53039px; left: 201px;"><nobr>ing) with good results (90%) for those subjects included in the training phase. Sub-</nobr></div>
<div style="position: absolute; top: 53066px; left: 201px;"><nobr>jects who were not included in the training phase performed significantly lower (76%)</nobr></div>
<div style="position: absolute; top: 53093px; left: 201px;"><nobr>which Davis claims is representative of not enough training data. Although it did not</nobr></div>
<div style="position: absolute; top: 53119px; left: 201px;"><nobr>specifically recognize hand gestures, appearance-based motion analysis could be used</nobr></div>
<div style="position: absolute; top: 53146px; left: 201px;"><nobr>to recognize very simple ones, but the technique will not work with gestures and pos-</nobr></div>
<div style="position: absolute; top: 53173px; left: 201px;"><nobr>tures that have complicated finger movement. More research is needed to determine</nobr></div>
<div style="position: absolute; top: 53200px; left: 201px;"><nobr>if a more complex set of motions can be recognized with the existing technique, since</nobr></div>
<div style="position: absolute; top: 53227px; left: 201px;"><nobr>most hand gestures are more complicated than the set of test motions used in Davis’s</nobr></div>
<div style="position: absolute; top: 53254px; left: 201px;"><nobr>evaluation.</nobr></div>
<div style="position: absolute; top: 53295px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 53334px; left: 254px;"><nobr><b>I. Provides unobtrusive recognition</b></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 53363px; left: 217px;"><nobr>4</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 53364px; left: 222px;"><nobr>The Mahalanobis distance contructs an ellipsoid in multidimensional space where variations in the di-</nobr></div>
<div style="position: absolute; top: 53386px; left: 201px;"><nobr>rections of the shorter axes have more weight.</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 53401px; left: 217px;"><nobr>5</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 53402px; left: 222px;"><nobr>Note that a detailed descriptionof the recognition techniquedescribed in this and the previous paragraph</nobr></div>
<div style="position: absolute; top: 53424px; left: 201px;"><nobr>can be found in Davis[25].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 53487px; left: 451px;"><nobr>42</nobr></div>
</span></font>

<div style="position: absolute; top: 53635px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="47"><b>Page 47</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 53823px; left: 248px;"><nobr><b>II. Accurate recognition with a small set of motions for the trained user</b></nobr></div>
<div style="position: absolute; top: 53861px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 53900px; left: 254px;"><nobr><b>I. Has not been used for recognizing hand posture and gestures</b></nobr></div>
<div style="position: absolute; top: 53933px; left: 248px;"><nobr><b>II. Very difficult to detect small details in finger movement</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 15px; font-family: Times;">
<div style="position: absolute; top: 53984px; left: 201px;"><nobr><b>3.3.3 Spatio-Temporal Vector Analysis</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 54024px; left: 201px;"><nobr>Spatio-temporal vector analysis is used to track the boundaries of the moving hand</nobr></div>
<div style="position: absolute; top: 54051px; left: 201px;"><nobr>in a vision-based system[87]. This technique makes it possible to extract the image</nobr></div>
<div style="position: absolute; top: 54078px; left: 201px;"><nobr>flow from a hand gesture which then can be used for hand gesture interpretation and</nobr></div>
<div style="position: absolute; top: 54105px; left: 201px;"><nobr>recognition. Quek[87] computes the spatio-temporal vectors in three steps. First, the</nobr></div>
<div style="position: absolute; top: 54132px; left: 201px;"><nobr>images are processed to find the location of moving edges; second, an initial flow field</nobr></div>
<div style="position: absolute; top: 54159px; left: 201px;"><nobr>describing the velocities at moving edge points is computed; finally, the flow field is</nobr></div>
<div style="position: absolute; top: 54186px; left: 201px;"><nobr>refined and smoothed using a variance constraint.</nobr></div>
<div style="position: absolute; top: 54212px; left: 223px;"><nobr>The location of moving edges is calculated from the assumption that the moving</nobr></div>
<div style="position: absolute; top: 54239px; left: 201px;"><nobr>hand is the fastest moving object in the static scene. The video image is modeled using</nobr></div>
<div style="position: absolute; top: 54266px; left: 201px;"><nobr>a three-dimensional signal specifying an <i>xy </i>point and time. Partial derivatives with a</nobr></div>
<div style="position: absolute; top: 54293px; left: 201px;"><nobr>varying <i>x </i>and <i>y </i>are computed from this signal using Sobel operators (a commonly used</nobr></div>
<div style="position: absolute; top: 54320px; left: 201px;"><nobr>technique in image processing). The Sobel operators yield gradient images in which the</nobr></div>
<div style="position: absolute; top: 54347px; left: 201px;"><nobr>image value is a measure of pixel intensity. These images are then combined with their</nobr></div>
<div style="position: absolute; top: 54374px; left: 201px;"><nobr>corresponding time-derivative images to yield new images in which the pixel intensity</nobr></div>
<div style="position: absolute; top: 54401px; left: 201px;"><nobr>is a measure of moving-edge intensity. The edges are extracted from these derived</nobr></div>
<div style="position: absolute; top: 54428px; left: 201px;"><nobr>images by eliminating edge points whose neighbors parallel to the edge direction do</nobr></div>
<div style="position: absolute; top: 54455px; left: 201px;"><nobr>not have a maximum magnitude.</nobr></div>
<div style="position: absolute; top: 54482px; left: 223px;"><nobr>After the moving edges have been found, the vector flow field of moving edges is</nobr></div>
<div style="position: absolute; top: 54508px; left: 201px;"><nobr>computed. Edge velocities are calculated by first choosing dominant edge points at</nobr></div>
<div style="position: absolute; top: 54535px; left: 201px;"><nobr>which to calculate velocities. Then edge point correspondences from different edge</nobr></div>
<div style="position: absolute; top: 54562px; left: 201px;"><nobr>images are found by a correlation process called absolute difference correlation[1].</nobr></div>
<div style="position: absolute; top: 54589px; left: 201px;"><nobr>After the correlation process is complete, the best set of vectors from the endpoint</nobr></div>
<div style="position: absolute; top: 54616px; left: 201px;"><nobr>candidates is found. This set is calculated with a variance constraint that represents the</nobr></div>
<div style="position: absolute; top: 54675px; left: 451px;"><nobr>43</nobr></div>
</span></font>

<div style="position: absolute; top: 54823px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="48"><b>Page 48</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 55012px; left: 201px;"><nobr>normalized velocity change across the image’s vector field. The best set of vectors is</nobr></div>
<div style="position: absolute; top: 55038px; left: 201px;"><nobr>then used to recognize the predefined hand gestures</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 8px; font-family: Times;">
<div style="position: absolute; top: 55037px; left: 509px;"><nobr>6</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 55038px; left: 515px;"><nobr>.</nobr></div>
<div style="position: absolute; top: 55065px; left: 223px;"><nobr>More research needs to be conducted with concrete accuracy testing to determine if</nobr></div>
<div style="position: absolute; top: 55092px; left: 201px;"><nobr>spatio-temporal vector analysis can recognize hand postures and gestures adequately.</nobr></div>
<div style="position: absolute; top: 55133px; left: 208px;"><nobr><b>Strengths</b></nobr></div>
<div style="position: absolute; top: 55172px; left: 254px;"><nobr><b>I. Provides unobtrusive recognition</b></nobr></div>
<div style="position: absolute; top: 55205px; left: 248px;"><nobr><b>II. Can recognize a medium-sized set of hand postures and gestures</b></nobr></div>
<div style="position: absolute; top: 55244px; left: 208px;"><nobr><b>Weaknesses</b></nobr></div>
<div style="position: absolute; top: 55283px; left: 254px;"><nobr><b>I. No recognition accuracy results reported</b></nobr></div>
<div style="position: absolute; top: 55316px; left: 248px;"><nobr><b>II. Requires significant computation to track the hands</b></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 6px; font-family: Times;">
<div style="position: absolute; top: 55819px; left: 217px;"><nobr>6</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 9px; font-family: Times;">
<div style="position: absolute; top: 55821px; left: 222px;"><nobr>For details on the mathematics behind this technique see Quek[87].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 55863px; left: 451px;"><nobr>44</nobr></div>
</span></font>

<div style="position: absolute; top: 56011px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="49"><b>Page 49</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 56314px; left: 201px;"><nobr><b>4</b></nobr></div>
</span></font>
<font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 56319px; left: 239px;"><nobr><b>Applications Areas That Use Hand</b></nobr></div>
<div style="position: absolute; top: 56369px; left: 201px;"><nobr><b>Postures and Gestures</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 56466px; left: 223px;"><nobr>This section discusses the various applications that have used hand postures and</nobr></div>
<div style="position: absolute; top: 56493px; left: 201px;"><nobr>gestures as their interaction metaphor. Note that Sturman and Zeltzer[99] and Stur-</nobr></div>
<div style="position: absolute; top: 56520px; left: 201px;"><nobr>man[101] have also reported on application areas that use hand postures and gestures.</nobr></div>
<div style="position: absolute; top: 56547px; left: 201px;"><nobr>Unfortunately, in many of these application areas, usability and recognition accuracy</nobr></div>
<div style="position: absolute; top: 56574px; left: 201px;"><nobr>measures have not been reported.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 56607px; left: 201px;"><nobr><b>4.1 Sign Language</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 56657px; left: 201px;"><nobr>One of the more intuitive applications for using hand posture and gesture recogni-</nobr></div>
<div style="position: absolute; top: 56683px; left: 201px;"><nobr>tion is sign language. A number of systems have been implemented that recognize</nobr></div>
<div style="position: absolute; top: 56710px; left: 201px;"><nobr>various types of sign languages. For example, Starner was able to recognize a dis-</nobr></div>
<div style="position: absolute; top: 56737px; left: 201px;"><nobr>tinct forty word lexicon consisting of pronouns, nouns, verbs, and adjectives taken</nobr></div>
<div style="position: absolute; top: 56764px; left: 201px;"><nobr>from American Sign Language with accuracies of over 90% [96]. A system devel-</nobr></div>
<div style="position: absolute; top: 56791px; left: 201px;"><nobr>oped by Kadous[48] recognized a 95 word lexicon from Australian Sign Language</nobr></div>
<div style="position: absolute; top: 56818px; left: 201px;"><nobr>and achieved accuracies of approximately 80%. Murakami and Taguchi were able to</nobr></div>
<div style="position: absolute; top: 56845px; left: 201px;"><nobr>adequately recognize 42 finger-alphabet symbols and 10 sign-language words taken</nobr></div>
<div style="position: absolute; top: 56872px; left: 201px;"><nobr>from Japanese Sign Language[77]. Lu et al. [62] and Matuso[69] have also developed</nobr></div>
<div style="position: absolute; top: 56899px; left: 201px;"><nobr>systems for recognizing a subset of Japanese Sign Language. Finally, Takahashi and</nobr></div>
<div style="position: absolute; top: 56926px; left: 201px;"><nobr>Kishino were able to accurately recognize 34 hand gestures used in the Japanese Kana</nobr></div>
<div style="position: absolute; top: 56952px; left: 201px;"><nobr>Manual Alphabet[105].</nobr></div>
<div style="position: absolute; top: 57051px; left: 451px;"><nobr>45</nobr></div>
</span></font>

<div style="position: absolute; top: 57199px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="50"><b>Page 50</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 57381px; left: 201px;"><nobr><b>4.2 Gesture-to-Speech</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 57430px; left: 201px;"><nobr>Gesture-to-speech applications translate the user’s hand postures and gestures into</nobr></div>
<div style="position: absolute; top: 57457px; left: 201px;"><nobr>speech. Such systems could give hearing-impaired people the ability to communi-</nobr></div>
<div style="position: absolute; top: 57484px; left: 201px;"><nobr>cate through a computer. A gesture-to-speech interface could be especially valuable</nobr></div>
<div style="position: absolute; top: 57510px; left: 201px;"><nobr>to hearing-impaired people who wish to communicate with people who do not know</nobr></div>
<div style="position: absolute; top: 57537px; left: 201px;"><nobr>sign language. Fels[35]and Kramer[53] have developed systems for converting hand</nobr></div>
<div style="position: absolute; top: 57564px; left: 201px;"><nobr>gestures to speech that use gloves to collect the hand gestures and speech synthesizers</nobr></div>
<div style="position: absolute; top: 57591px; left: 201px;"><nobr>for speech output.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 57648px; left: 201px;"><nobr><b>4.3 Presentations</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 57697px; left: 201px;"><nobr>Baudel and Beaudouin-Lafon have developed an application that uses hand postures</nobr></div>
<div style="position: absolute; top: 57724px; left: 201px;"><nobr>and gestures for giving presentations[9]. Their system, called Charade, gives the user</nobr></div>
<div style="position: absolute; top: 57751px; left: 201px;"><nobr>the ability to control a computer-aided presentation. Charade uses the concept of a sta-</nobr></div>
<div style="position: absolute; top: 57778px; left: 201px;"><nobr>tionary “active zone”, the zone in which gestures can be recognized; any gestures made</nobr></div>
<div style="position: absolute; top: 57805px; left: 201px;"><nobr>outside the “active zone” are ignored. This lets the presenter make other gesture-based</nobr></div>
<div style="position: absolute; top: 57832px; left: 201px;"><nobr>communications without having to worry that the system will recognize these gestures</nobr></div>
<div style="position: absolute; top: 57859px; left: 201px;"><nobr>and create inappropriate commands. Charade gives the presenter gestural control of</nobr></div>
<div style="position: absolute; top: 57886px; left: 201px;"><nobr>moving to the next or previous slide, marking a particular page, highlighting an area</nobr></div>
<div style="position: absolute; top: 57912px; left: 201px;"><nobr>on the screen, and so on. Usability testing showed novice users achieved recognition</nobr></div>
<div style="position: absolute; top: 57939px; left: 201px;"><nobr>accuracies of 72- to 84% while trained users obtained 90- to 98%.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 57997px; left: 201px;"><nobr><b>4.4 Virtual Environments</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 58046px; left: 201px;"><nobr>Virtual environments (VE) are a common testbed and application domain for hand pos-</nobr></div>
<div style="position: absolute; top: 58073px; left: 201px;"><nobr>tures and gestures since the hands are the primary communication medium in VEs. One</nobr></div>
<div style="position: absolute; top: 58100px; left: 201px;"><nobr>of the most important tasks a user must perform in many VE applications (e.g. architec-</nobr></div>
<div style="position: absolute; top: 58126px; left: 201px;"><nobr>tural walkthrough, information visualization) is navigating through the VE. Among the</nobr></div>
<div style="position: absolute; top: 58153px; left: 201px;"><nobr>many techniques for VE navigation (most of which are beyond the scope of this sur-</nobr></div>
<div style="position: absolute; top: 58180px; left: 201px;"><nobr>vey) is to use hand gestures for flying through the VE[73]. Typically, the user points in</nobr></div>
<div style="position: absolute; top: 58239px; left: 451px;"><nobr>46</nobr></div>
</span></font>

<div style="position: absolute; top: 58387px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="51"><b>Page 51</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 58576px; left: 201px;"><nobr>the direction he or she wants to go, and perhaps uses the other hand to control velocity.</nobr></div>
<div style="position: absolute; top: 58602px; left: 201px;"><nobr>Object interaction is also necessary in a VE, and using hand gestures to interact with</nobr></div>
<div style="position: absolute; top: 58629px; left: 201px;"><nobr>objects has attracted extensive research. The most traditional methods for interacting</nobr></div>
<div style="position: absolute; top: 58656px; left: 201px;"><nobr>with objects is pointing, reaching and grabbing. Sturman, Zeltzer and Pieper[102],</nobr></div>
<div style="position: absolute; top: 58683px; left: 201px;"><nobr>Davis[25], and Bryson[18] have all used hand gestures for object interaction in VEs.</nobr></div>
<div style="position: absolute; top: 58710px; left: 201px;"><nobr>Rehag and Kanade have also developed hand posture and gesture recognition methods</nobr></div>
<div style="position: absolute; top: 58737px; left: 201px;"><nobr>to create a 3D mouse for use in a virtual environment[91].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 58794px; left: 201px;"><nobr><b>4.5 3D Modeling</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 58843px; left: 201px;"><nobr>3D modeling requires the user to create, manipulate and view 3D objects, and creation</nobr></div>
<div style="position: absolute; top: 58870px; left: 201px;"><nobr>of these objects requires the user to specify a particular shape with the hands. For exam-</nobr></div>
<div style="position: absolute; top: 58897px; left: 201px;"><nobr>ple, Krueger’s VIDEODESK system allows the user to create 2D and 3D objects using</nobr></div>
<div style="position: absolute; top: 58924px; left: 201px;"><nobr>the silhouette of the hand and pointing[55]. Weimer and Ganapathy use hand gestures</nobr></div>
<div style="position: absolute; top: 58951px; left: 201px;"><nobr>to create B-spline-based 3D models[115], and Utsumi also uses static hand postures to</nobr></div>
<div style="position: absolute; top: 58978px; left: 201px;"><nobr>create simple 3D primitives[108]. Manipulating already created models by translation,</nobr></div>
<div style="position: absolute; top: 59004px; left: 201px;"><nobr>rotation and scale operations is also an important part of the modeling process, and</nobr></div>
<div style="position: absolute; top: 59031px; left: 201px;"><nobr>Mapes presents a series of techniques for doing this type of object manipulation[66].</nobr></div>
<div style="position: absolute; top: 59058px; left: 201px;"><nobr>Using 3D hand gestures for modeling is relatively new and more research is required</nobr></div>
<div style="position: absolute; top: 59085px; left: 201px;"><nobr>so that users can take advantage of their natural everyday movements for creating 3D</nobr></div>
<div style="position: absolute; top: 59112px; left: 201px;"><nobr>models.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 59169px; left: 201px;"><nobr><b>4.6 Multimodal Interaction</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 59218px; left: 201px;"><nobr>Another area in which hand postures and gestures are playing a significant role is mul-</nobr></div>
<div style="position: absolute; top: 59245px; left: 201px;"><nobr>timodal interfaces. In many applications, hand posture and gesture recognition is in-</nobr></div>
<div style="position: absolute; top: 59272px; left: 201px;"><nobr>corporated with speech to create a more natural interface. In this type of multimodal</nobr></div>
<div style="position: absolute; top: 59299px; left: 201px;"><nobr>integration the two modes can complement each other and make up for each other’s</nobr></div>
<div style="position: absolute; top: 59326px; left: 201px;"><nobr>recognition mistakes[83]. Although multimodal interaction has been around since the</nobr></div>
<div style="position: absolute; top: 59353px; left: 201px;"><nobr>early 1980s[13], the paradigm is still in its infancy and is currently an active area of</nobr></div>
<div style="position: absolute; top: 59380px; left: 201px;"><nobr>interface research. Lucente[63] has designed a multimodal interface that incorporates</nobr></div>
<div style="position: absolute; top: 59427px; left: 451px;"><nobr>47</nobr></div>
</span></font>

<div style="position: absolute; top: 59575px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="52"><b>Page 52</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 59764px; left: 201px;"><nobr>a vision-based hand posture and gesture recognition solution with speech input for a</nobr></div>
<div style="position: absolute; top: 59790px; left: 201px;"><nobr>number of different applications (e.g. vacation planning, real-estate purchase). Mul-</nobr></div>
<div style="position: absolute; top: 59817px; left: 201px;"><nobr>timodal interaction has also been applied to areas such as scientific visualization (see</nobr></div>
<div style="position: absolute; top: 59844px; left: 201px;"><nobr>Figure 4.1) and room layout[58].</nobr></div>
<div style="position: absolute; top: 60222px; left: 201px;"><nobr>Figure 4.1: A user interacting with a dataset for visualizing a flow field around a space</nobr></div>
<div style="position: absolute; top: 60240px; left: 201px;"><nobr>shuttle. The user manipulates the streamlines with his left hand and the shuttle with his</nobr></div>
<div style="position: absolute; top: 60258px; left: 201px;"><nobr>right hand while viewing the data in stereo[58].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 60333px; left: 201px;"><nobr><b>4.7 Human/Robot Manipulation and Instruction</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 60382px; left: 201px;"><nobr>An interesting application that can exploit hand postures and gestures is robot telema-</nobr></div>
<div style="position: absolute; top: 60409px; left: 201px;"><nobr>nipulation. An example of this type of telemanipulation was developed by Papper and</nobr></div>
<div style="position: absolute; top: 60436px; left: 201px;"><nobr>Gigante to control a robot arm[84]. Hand postures and gestures can also be used to</nobr></div>
<div style="position: absolute; top: 60463px; left: 201px;"><nobr>teach robots various commands and interactions by demonstration of the appropriate</nobr></div>
<div style="position: absolute; top: 60490px; left: 201px;"><nobr>gestures by humans. Lee and Xu developed a system for teaching robots that can inter-</nobr></div>
<div style="position: absolute; top: 60517px; left: 201px;"><nobr>actively learn new gestures after only a few training examples[60]. Tung and Kak have</nobr></div>
<div style="position: absolute; top: 60544px; left: 201px;"><nobr>also developed a system for automatic learning of robot tasks using hand gestures[107].</nobr></div>
<div style="position: absolute; top: 60615px; left: 451px;"><nobr>48</nobr></div>
</span></font>

<div style="position: absolute; top: 60763px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="53"><b>Page 53</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 60945px; left: 201px;"><nobr><b>4.8 Television Control</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 60994px; left: 201px;"><nobr>Another application for hand postures and gestures is control of audio and video de-</nobr></div>
<div style="position: absolute; top: 61021px; left: 201px;"><nobr>vices. Freeman and Weissman have developed a system to control a television set by</nobr></div>
<div style="position: absolute; top: 61048px; left: 201px;"><nobr>hand gestures[37]. Using an open hand, the user can turn the television on and off,</nobr></div>
<div style="position: absolute; top: 61074px; left: 201px;"><nobr>change the channel, increase and decrease the volume, and mute the sound. Other</nobr></div>
<div style="position: absolute; top: 61101px; left: 201px;"><nobr>applications that could use hand gestures are control of a VCR, a stereo, or a whole</nobr></div>
<div style="position: absolute; top: 61128px; left: 201px;"><nobr>room[51].</nobr></div>
<div style="position: absolute; top: 61803px; left: 451px;"><nobr>49</nobr></div>
</span></font>

<div style="position: absolute; top: 61951px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="54"><b>Page 54</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 62254px; left: 201px;"><nobr><b>5</b></nobr></div>
</span></font>
<font size="4" face="Times"><span style="font-size: 23px; font-family: Times;">
<div style="position: absolute; top: 62259px; left: 239px;"><nobr><b>Conclusions</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 62357px; left: 201px;"><nobr>Hand postures and gestures are an interesting interaction paradigm in a variety of com-</nobr></div>
<div style="position: absolute; top: 62384px; left: 201px;"><nobr>puter applications. Two principal questions must be answered when using them. The</nobr></div>
<div style="position: absolute; top: 62411px; left: 201px;"><nobr>first question is what technology to use for collecting raw data from the hand. Gener-</nobr></div>
<div style="position: absolute; top: 62438px; left: 201px;"><nobr>ally, two types of technologies are available for collecting this raw data. The first one</nobr></div>
<div style="position: absolute; top: 62465px; left: 201px;"><nobr>is a glove input device, which measures a number of joint angles in the hand (except</nobr></div>
<div style="position: absolute; top: 62492px; left: 201px;"><nobr>for the Pinch Gloves). Flexion and extension of the finger are the most common joints</nobr></div>
<div style="position: absolute; top: 62518px; left: 201px;"><nobr>measured, although abduction, adduction, radial abduction, and palmer abduction can</nobr></div>
<div style="position: absolute; top: 62545px; left: 201px;"><nobr>be also measured. Accuracy of a glove input device depends on the type of bend sensor</nobr></div>
<div style="position: absolute; top: 62572px; left: 201px;"><nobr>technology used; usually, the more accurate the glove is, the more expensive it is. In</nobr></div>
<div style="position: absolute; top: 62599px; left: 201px;"><nobr>many glove-based applications position and orientation data of the hand or hands must</nobr></div>
<div style="position: absolute; top: 62626px; left: 201px;"><nobr>be collected by some tracking system, and many different technologies are used for</nobr></div>
<div style="position: absolute; top: 62653px; left: 201px;"><nobr>this, including magnetic, inertial, and ultrasonic systems. The second way of collect-</nobr></div>
<div style="position: absolute; top: 62680px; left: 201px;"><nobr>ing raw data is to use computer vision. In a vision-based solution, one or more cameras</nobr></div>
<div style="position: absolute; top: 62707px; left: 201px;"><nobr>placed in the environment record hand movement. Both types of solutions have many</nobr></div>
<div style="position: absolute; top: 62734px; left: 201px;"><nobr>advantages and disadvantages, and the question of which solution to use is a difficult</nobr></div>
<div style="position: absolute; top: 62761px; left: 201px;"><nobr>one. However, when using a hand posture or gesture-based interface, the user does not</nobr></div>
<div style="position: absolute; top: 62787px; left: 201px;"><nobr>want to wear the device and be physically attached to the computer. If vision-based</nobr></div>
<div style="position: absolute; top: 62814px; left: 201px;"><nobr>solutions can overcome some of their difficulties and disadvantages, they appear to be</nobr></div>
<div style="position: absolute; top: 62841px; left: 201px;"><nobr>the best choice for raw data collection.</nobr></div>
<div style="position: absolute; top: 62868px; left: 223px;"><nobr>The second question to be answered when using hand posture and gestures is what</nobr></div>
<div style="position: absolute; top: 62895px; left: 201px;"><nobr>recognition technique will maximize accuracy and robustness. A number of recogni-</nobr></div>
<div style="position: absolute; top: 62922px; left: 201px;"><nobr>tion techniques are available and in some cases, the answer to the first question will</nobr></div>
<div style="position: absolute; top: 62991px; left: 451px;"><nobr>50</nobr></div>
</span></font>

<div style="position: absolute; top: 63139px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="55"><b>Page 55</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 63328px; left: 201px;"><nobr>narrow down the possibilities, since some of the recognition techniques work only for</nobr></div>
<div style="position: absolute; top: 63354px; left: 201px;"><nobr>vision-based solutions. This survey has categorized these techniques into three broad</nobr></div>
<div style="position: absolute; top: 63381px; left: 201px;"><nobr>categories:</nobr></div>
<div style="position: absolute; top: 63423px; left: 231px;"><nobr>Feature extraction, statistics, and models</nobr></div>
<div style="position: absolute; top: 63462px; left: 231px;"><nobr>Learning algorithms</nobr></div>
<div style="position: absolute; top: 63501px; left: 231px;"><nobr>Miscellaneous techniques</nobr></div>
<div style="position: absolute; top: 63542px; left: 223px;"><nobr>Many of these techniques can be considered proven, but some of them have been</nobr></div>
<div style="position: absolute; top: 63569px; left: 201px;"><nobr>reported in the literature only once, and this gives little indication that they are viable.</nobr></div>
<div style="position: absolute; top: 63596px; left: 201px;"><nobr>The researcher can choose whether to use an established technique or one that requires</nobr></div>
<div style="position: absolute; top: 63623px; left: 201px;"><nobr>more study. In addition, a recognition algorithm should be chosen on the basis of how</nobr></div>
<div style="position: absolute; top: 63650px; left: 201px;"><nobr>many postures or gestures are in the recognition set, the complexity of the set, and</nobr></div>
<div style="position: absolute; top: 63677px; left: 201px;"><nobr>whether or not the set is known beforehand.</nobr></div>
<div style="position: absolute; top: 63704px; left: 223px;"><nobr>There are a number of interesting areas for future research in hand posture and</nobr></div>
<div style="position: absolute; top: 63731px; left: 201px;"><nobr>gesture recognition. The field is by no means mature – we have a long way to go</nobr></div>
<div style="position: absolute; top: 63758px; left: 201px;"><nobr>before this type of metaphor is robust enough to be seen in commercial, mainstream</nobr></div>
<div style="position: absolute; top: 63785px; left: 201px;"><nobr>applications. Research into better hardware for data collection is important. Better</nobr></div>
<div style="position: absolute; top: 63811px; left: 201px;"><nobr>joint angle bend sensors, tracking systems, and faster processors will benefit the field</nobr></div>
<div style="position: absolute; top: 63838px; left: 201px;"><nobr>immensely. Another possible area of research is to develop a glove input device that</nobr></div>
<div style="position: absolute; top: 63865px; left: 201px;"><nobr>combines the qualities of the Pinch Glove and the CyberGlove and thus produce a</nobr></div>
<div style="position: absolute; top: 63892px; left: 201px;"><nobr>more robust interface device[59]. There is a signifigant amount of research to be done</nobr></div>
<div style="position: absolute; top: 63919px; left: 201px;"><nobr>in quantifying the validity of many of the techniques that have been reported only</nobr></div>
<div style="position: absolute; top: 63946px; left: 201px;"><nobr>minimally in the literature, since it is unclear as to whether these techniques are viable</nobr></div>
<div style="position: absolute; top: 63973px; left: 201px;"><nobr>without further analysis. Other areas of research include new techniques that allow</nobr></div>
<div style="position: absolute; top: 64000px; left: 201px;"><nobr>robust and accurate recognition and fine-tuning established techniques to support larger</nobr></div>
<div style="position: absolute; top: 64027px; left: 201px;"><nobr>and more complex posture and gesture sets.</nobr></div>
<div style="position: absolute; top: 64054px; left: 223px;"><nobr>This survey has provided a comprehensive overview of hand posture and gesture</nobr></div>
<div style="position: absolute; top: 64081px; left: 201px;"><nobr>recognition techniques and technology. It gives researchers interested in starting work</nobr></div>
<div style="position: absolute; top: 64107px; left: 201px;"><nobr>in this field an introductionto the various issues to be addressed when dealing with this</nobr></div>
<div style="position: absolute; top: 64179px; left: 451px;"><nobr>51</nobr></div>
</span></font>

<div style="position: absolute; top: 64327px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="56"><b>Page 56</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 64516px; left: 201px;"><nobr>type of interaction and allows them to have one document that references most of the</nobr></div>
<div style="position: absolute; top: 64542px; left: 201px;"><nobr>viable literature. It also is useful as reference guide to this interaction paradigm.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 64599px; left: 201px;"><nobr><b>Acknowledgments</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 64648px; left: 201px;"><nobr>Special thanks go to Timothy Miller, Robert Zeleznik, and Katrina Avery for valu-</nobr></div>
<div style="position: absolute; top: 64675px; left: 201px;"><nobr>able comments and discussions, and to Andries van Dam for support and encourage-</nobr></div>
<div style="position: absolute; top: 64702px; left: 201px;"><nobr>ment. This work is supported in part by the NSF Graphics and Visualization Center,</nobr></div>
<div style="position: absolute; top: 64729px; left: 201px;"><nobr>Advanced Networks and Services, Alias/Wavefront, International Business Machines,</nobr></div>
<div style="position: absolute; top: 64756px; left: 201px;"><nobr>Microsoft, Sun Microsystems, and TACO.</nobr></div>
<div style="position: absolute; top: 65367px; left: 451px;"><nobr>52</nobr></div>
</span></font>

<div style="position: absolute; top: 65515px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="57"><b>Page 57</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 65818px; left: 201px;"><nobr><b>Appendix A</b></nobr></div>
</span></font>
<font size="5" face="Times"><span style="font-size: 35px; font-family: Times;">
<div style="position: absolute; top: 65910px; left: 201px;"><nobr><b>Anatomy of the Human Hand</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 66018px; left: 201px;"><nobr>This section gives a brief introduction into the anatomical characteristics of the human</nobr></div>
<div style="position: absolute; top: 66045px; left: 201px;"><nobr>hand. For a more comprehensive discussion of human hand anatomy, see Napier[78],</nobr></div>
<div style="position: absolute; top: 66072px; left: 201px;"><nobr>the American Society for Surgery of the Hand[4], or the American Academy of Ortho-</nobr></div>
<div style="position: absolute; top: 66098px; left: 201px;"><nobr>pedic Surgeons[3].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 66156px; left: 201px;"><nobr><b>A.1 Hand and Finger Joints</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 66205px; left: 201px;"><nobr>The human hand is comprised of 17 joints that provide a total of 23 degrees of freedom</nobr></div>
<div style="position: absolute; top: 66232px; left: 201px;"><nobr>(see Figure A.1). The fingers, labeled 2-5 in in Figure A.1, have three joints each. In</nobr></div>
<div style="position: absolute; top: 66259px; left: 201px;"><nobr>order from fingertip to finger base, these joints are the distal interphalangeal (DIP),</nobr></div>
<div style="position: absolute; top: 66286px; left: 201px;"><nobr>the proximal interphalangeal (PIP), and the metacarpophalangeal (MCP) . The thumb</nobr></div>
<div style="position: absolute; top: 66313px; left: 201px;"><nobr>also has three joints, the thumb interphalangeal (Thumb IP), the thumb metacarpopha-</nobr></div>
<div style="position: absolute; top: 66339px; left: 201px;"><nobr>langeal (Thumb MP), and the trapeziometacarpal, in order from thumb tip to thumb</nobr></div>
<div style="position: absolute; top: 66366px; left: 201px;"><nobr>base. The last two joints that make up the hand are the metacarpocarpal joints, located</nobr></div>
<div style="position: absolute; top: 66393px; left: 201px;"><nobr>between the metacarpal and carpal bones on digits four and five.</nobr></div>
<div style="position: absolute; top: 66555px; left: 451px;"><nobr>53</nobr></div>
</span></font>

<div style="position: absolute; top: 66703px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="58"><b>Page 58</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 66885px; left: 201px;"><nobr><b>A.2 Hand Motion</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 66934px; left: 201px;"><nobr>There are 23 degrees of freedom available in the hand above the wrist; if we include</nobr></div>
<div style="position: absolute; top: 66961px; left: 201px;"><nobr>components of three dimensional movement, the degrees of freedom increase to 29.</nobr></div>
<div style="position: absolute; top: 66988px; left: 201px;"><nobr>Figure A.2 shows a classification of hand motions. The DIP, PIP, Thumb IP, and Thumb</nobr></div>
<div style="position: absolute; top: 67014px; left: 201px;"><nobr>MP joints in the four fingers are characterized by flexion and extension (a one degree</nobr></div>
<div style="position: absolute; top: 67041px; left: 201px;"><nobr>of freedom movement). The MCPs in the four fingers are characterized by flexion,</nobr></div>
<div style="position: absolute; top: 67068px; left: 201px;"><nobr>extension, hyperextension, and abduction (the separation or degree of bend between</nobr></div>
<div style="position: absolute; top: 67095px; left: 201px;"><nobr>the fingers). The most complex movements the hand can perform are in the province</nobr></div>
<div style="position: absolute; top: 67122px; left: 201px;"><nobr>of the trapeziometacarpal joint in the thumb: this joint’s motion is characterized by</nobr></div>
<div style="position: absolute; top: 67149px; left: 201px;"><nobr>radial and palmer abduction and anteposition, which is further classified by opposition</nobr></div>
<div style="position: absolute; top: 67176px; left: 201px;"><nobr>and circumduction, and retroposition (see Figure A.2).</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 67233px; left: 201px;"><nobr><b>A.3 Muscles and Tendons in the Hand</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 67282px; left: 201px;"><nobr>Figure A.3 shows the muscles and tendons of the hand. There are many complexities</nobr></div>
<div style="position: absolute; top: 67309px; left: 201px;"><nobr>and interconnections involved in the hand anatomy that contribute to the complexity</nobr></div>
<div style="position: absolute; top: 67336px; left: 201px;"><nobr>of hand motion. An important group of tendons, connected to the extensor digitorum</nobr></div>
<div style="position: absolute; top: 67363px; left: 201px;"><nobr>communis, are essentially the conduit between the extensor digitorum muscle and fin-</nobr></div>
<div style="position: absolute; top: 67390px; left: 201px;"><nobr>ger movements. This can have an adverse effect on measurement data from glove-based</nobr></div>
<div style="position: absolute; top: 67416px; left: 201px;"><nobr>input devices[101].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 67474px; left: 201px;"><nobr><b>A.4 Importance of the Hand’s Anatomy</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 67523px; left: 201px;"><nobr>UI developers need a sound understanding of the human hand’s anatomical structure</nobr></div>
<div style="position: absolute; top: 67550px; left: 201px;"><nobr>in order to help them to determine what kinds of postures and gestures are easy and</nobr></div>
<div style="position: absolute; top: 67577px; left: 201px;"><nobr>comfortable to make. For example, any posture or gesture requiring hyperextension</nobr></div>
<div style="position: absolute; top: 67604px; left: 201px;"><nobr>of any of the fingers is not suitable since it puts more strain on the joints and tendons</nobr></div>
<div style="position: absolute; top: 67630px; left: 201px;"><nobr>than the hand is accustomed to and thus can result in strain or injury. It is often useful</nobr></div>
<div style="position: absolute; top: 67657px; left: 201px;"><nobr>to experiment with any postures or gestures than are going to be a part of an interface</nobr></div>
<div style="position: absolute; top: 67684px; left: 201px;"><nobr>before they are implemented to make sure that they are comfortable, and do not put any</nobr></div>
<div style="position: absolute; top: 67743px; left: 451px;"><nobr>54</nobr></div>
</span></font>

<div style="position: absolute; top: 67891px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="59"><b>Page 59</b></a></font></td></tr></tbody></table></div><font size="2" face="Times"><span style="font-size: 7px; font-family: Times;">
<div style="position: absolute; top: 68183px; left: 519px;"><nobr>DIP – Distal Interphalangeal Joints</nobr></div>
<div style="position: absolute; top: 68192px; left: 564px;"><nobr>1 DOF each</nobr></div>
<div style="position: absolute; top: 68216px; left: 516px;"><nobr>PIP – Proximal Interphalangeal Joints</nobr></div>
<div style="position: absolute; top: 68225px; left: 568px;"><nobr>1 DOF each</nobr></div>
<div style="position: absolute; top: 68255px; left: 512px;"><nobr>MCP – Metacarpophalangeal Joints</nobr></div>
<div style="position: absolute; top: 68264px; left: 560px;"><nobr>2 DOF each</nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 4px; font-family: Times;">
<div style="position: absolute; top: 68144px; left: 301px;"><nobr><i>Phalanges</i></nobr></div>
<div style="position: absolute; top: 68160px; left: 341px;"><nobr><i>Proximal</i></nobr></div>
<div style="position: absolute; top: 68145px; left: 341px;"><nobr><i>Middle</i></nobr></div>
<div style="position: absolute; top: 68130px; left: 341px;"><nobr><i>Distal</i></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 5px; font-family: Times;">
<div style="position: absolute; top: 68132px; left: 334px;"><nobr></nobr></div>
<div style="position: absolute; top: 68138px; left: 334px;"><nobr></nobr></div>
<div style="position: absolute; top: 68156px; left: 334px;"><nobr></nobr></div>
<div style="position: absolute; top: 68144px; left: 334px;"><nobr></nobr></div>
<div style="position: absolute; top: 68150px; left: 334px;"><nobr></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 4px; font-family: Times;">
<div style="position: absolute; top: 68376px; left: 351px;"><nobr><i>Radius</i></nobr></div>
<div style="position: absolute; top: 68300px; left: 495px;"><nobr><i>Metacarpals</i></nobr></div>
<div style="position: absolute; top: 68349px; left: 483px;"><nobr><i>Carpals</i></nobr></div>
<div style="position: absolute; top: 68377px; left: 484px;"><nobr><i>Ulna</i></nobr></div>
<div style="position: absolute; top: 68180px; left: 352px;"><nobr><i>1</i></nobr></div>
<div style="position: absolute; top: 68098px; left: 400px;"><nobr><i>2</i></nobr></div>
<div style="position: absolute; top: 68087px; left: 439px;"><nobr><i>3</i></nobr></div>
<div style="position: absolute; top: 68103px; left: 472px;"><nobr><i>4</i></nobr></div>
<div style="position: absolute; top: 68146px; left: 499px;"><nobr><i>5</i></nobr></div>
</span></font>
<font size="2" face="Times"><span style="font-size: 7px; font-family: Times;">
<div style="position: absolute; top: 68319px; left: 507px;"><nobr>Metacarpocarpal Joints</nobr></div>
<div style="position: absolute; top: 68328px; left: 499px;"><nobr>1 DOF each on digits 4 &amp; 5</nobr></div>
<div style="position: absolute; top: 68225px; left: 256px;"><nobr>Thumb IP Joint</nobr></div>
<div style="position: absolute; top: 68234px; left: 274px;"><nobr>1 DOF</nobr></div>
<div style="position: absolute; top: 68268px; left: 258px;"><nobr>Thumb MP joint</nobr></div>
<div style="position: absolute; top: 68277px; left: 277px;"><nobr>1 DOF</nobr></div>
<div style="position: absolute; top: 68318px; left: 241px;"><nobr>Trapeziometacarpal Joint</nobr></div>
<div style="position: absolute; top: 68327px; left: 280px;"><nobr>3 DOF</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 68413px; left: 201px;"><nobr>Figure A.1: The 17 joints in the hand and the associated 23 degrees of freedom (from</nobr></div>
<div style="position: absolute; top: 68431px; left: 201px;"><nobr>Sturman[101]).</nobr></div>
<div style="position: absolute; top: 68478px; left: 201px;"><nobr>excessive strain on the hands.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 68535px; left: 201px;"><nobr><b>A.5 Hand Models Used in Posture and Gesture Recog-</b></nobr></div>
<div style="position: absolute; top: 68575px; left: 254px;"><nobr><b>nition</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 68625px; left: 201px;"><nobr>It is useful in recognizing hand postures and gestures, to have a model of the hand</nobr></div>
<div style="position: absolute; top: 68651px; left: 201px;"><nobr>that can be used in recognition algorithms. The most common model is the angle-</nobr></div>
<div style="position: absolute; top: 68678px; left: 201px;"><nobr>based hand model, which describes the hand using the joints angles and their associated</nobr></div>
<div style="position: absolute; top: 68705px; left: 201px;"><nobr>degrees of freedom (see Figure A.1). The full model has a total of 29 data values, 23</nobr></div>
<div style="position: absolute; top: 68732px; left: 201px;"><nobr>associated with the degrees of freedom in the hand and six associated with its position</nobr></div>
<div style="position: absolute; top: 68759px; left: 201px;"><nobr>and orientation. A major advantage of this hand model is that most glove-based input</nobr></div>
<div style="position: absolute; top: 68786px; left: 201px;"><nobr>devices measure joint angles, so that no complex mathematical conversion is needed to</nobr></div>
<div style="position: absolute; top: 68813px; left: 201px;"><nobr>move between glove data and model.</nobr></div>
<div style="position: absolute; top: 68839px; left: 223px;"><nobr>Another less common hand model is the point-based model, which has six tracked</nobr></div>
<div style="position: absolute; top: 68866px; left: 201px;"><nobr>points, the center of the hand and the tips of each of the five digits[103]. Su[104]</nobr></div>
<div style="position: absolute; top: 68931px; left: 451px;"><nobr>55</nobr></div>
</span></font>

<div style="position: absolute; top: 69079px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="60"><b>Page 60</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 69268px; left: 201px;"><nobr>gives a mathematical derivation and conversion between the point-based hand model</nobr></div>
<div style="position: absolute; top: 69294px; left: 201px;"><nobr>and the angle-based hand model. The advantage of the point-based hand model is</nobr></div>
<div style="position: absolute; top: 69321px; left: 201px;"><nobr>that it is simpler to use in hand posture recognition since only six data points are used</nobr></div>
<div style="position: absolute; top: 69348px; left: 201px;"><nobr>to classify the postures. It is especially suited to recognizing pointing, grabbing, and</nobr></div>
<div style="position: absolute; top: 69375px; left: 201px;"><nobr>shooting postures used commonly in virtual environments, and also shows promise in</nobr></div>
<div style="position: absolute; top: 69402px; left: 201px;"><nobr>recognizing American Sign Language.</nobr></div>
<div style="position: absolute; top: 69429px; left: 223px;"><nobr>However, the point-based hand model has distinct disadvantages. First, it restricts</nobr></div>
<div style="position: absolute; top: 69456px; left: 201px;"><nobr>the hand posture and gesture space. The matrix computations for converting an angle-</nobr></div>
<div style="position: absolute; top: 69482px; left: 201px;"><nobr>based hand model to a point-based model are complex and thus can increase response</nobr></div>
<div style="position: absolute; top: 69509px; left: 201px;"><nobr>time. Also, error accumulation from the matrix computations can lead to large inaccu-</nobr></div>
<div style="position: absolute; top: 69536px; left: 201px;"><nobr>racies in fingertip positions. Ideally, one would use a point-based hand model directly,</nobr></div>
<div style="position: absolute; top: 69563px; left: 201px;"><nobr>eliminating the conversion from an angle-based model. An area for future research is</nobr></div>
<div style="position: absolute; top: 69590px; left: 201px;"><nobr>thus to develop a data glove that tracks the position and orientation of the fingertips</nobr></div>
<div style="position: absolute; top: 69617px; left: 201px;"><nobr>instead of measuring joint angles, since no current input device to does this.</nobr></div>
<div style="position: absolute; top: 70119px; left: 451px;"><nobr>56</nobr></div>
</span></font>

<div style="position: absolute; top: 70267px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="61"><b>Page 61</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 71235px; left: 201px;"><nobr>Figure A.2: The various motions that the hand and fingers can make using its 23 de-</nobr></div>
<div style="position: absolute; top: 71253px; left: 201px;"><nobr>grees of freedom (from Sturman[101]).</nobr></div>
<div style="position: absolute; top: 71307px; left: 451px;"><nobr>57</nobr></div>
</span></font>

<div style="position: absolute; top: 71455px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="62"><b>Page 62</b></a></font></td></tr></tbody></table></div><font size="2" face="Times"><span style="font-size: 7px; font-family: Times;">
<div style="position: absolute; top: 71802px; left: 244px;"><nobr>Extensor</nobr></div>
<div style="position: absolute; top: 71811px; left: 244px;"><nobr>carpi</nobr></div>
<div style="position: absolute; top: 71821px; left: 244px;"><nobr>ulnaris</nobr></div>
<div style="position: absolute; top: 71880px; left: 265px;"><nobr>Extensor</nobr></div>
<div style="position: absolute; top: 71890px; left: 265px;"><nobr>retinaculum</nobr></div>
<div style="position: absolute; top: 71911px; left: 285px;"><nobr>Ulna</nobr></div>
<div style="position: absolute; top: 71954px; left: 281px;"><nobr>Abductor</nobr></div>
<div style="position: absolute; top: 71779px; left: 469px;"><nobr>Extensor digitorum</nobr></div>
<div style="position: absolute; top: 71789px; left: 469px;"><nobr>communis</nobr></div>
<div style="position: absolute; top: 71819px; left: 479px;"><nobr>Extensor pollicis</nobr></div>
<div style="position: absolute; top: 71829px; left: 479px;"><nobr>longus</nobr></div>
<div style="position: absolute; top: 71884px; left: 506px;"><nobr>Abductor pollicis</nobr></div>
<div style="position: absolute; top: 71894px; left: 506px;"><nobr>longus</nobr></div>
<div style="position: absolute; top: 71935px; left: 565px;"><nobr>Extensor pollicis</nobr></div>
<div style="position: absolute; top: 71945px; left: 565px;"><nobr>brevis tendon</nobr></div>
<div style="position: absolute; top: 71990px; left: 593px;"><nobr>Extensor pollicis</nobr></div>
<div style="position: absolute; top: 72000px; left: 593px;"><nobr>longus tendon</nobr></div>
<div style="position: absolute; top: 72112px; left: 539px;"><nobr>First dorsal</nobr></div>
<div style="position: absolute; top: 72122px; left: 539px;"><nobr>interosseous muscle</nobr></div>
<div style="position: absolute; top: 72145px; left: 550px;"><nobr>Tendons of extensor</nobr></div>
<div style="position: absolute; top: 72155px; left: 550px;"><nobr>digitorum muscle</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 72365px; left: 201px;"><nobr>Figure A.3: The various muscles and tendons of the hand and wrist (from Kadous[48]).</nobr></div>
<div style="position: absolute; top: 72495px; left: 451px;"><nobr>58</nobr></div>
</span></font>

<div style="position: absolute; top: 72643px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="63"><b>Page 63</b></a></font></td></tr></tbody></table></div><font size="4" face="Times"><span style="font-size: 28px; font-family: Times;">
<div style="position: absolute; top: 72946px; left: 201px;"><nobr><b>Appendix B</b></nobr></div>
</span></font>
<font size="5" face="Times"><span style="font-size: 35px; font-family: Times;">
<div style="position: absolute; top: 73038px; left: 201px;"><nobr><b>Hand Posture and Gesture</b></nobr></div>
<div style="position: absolute; top: 73104px; left: 201px;"><nobr><b>Classification</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 73213px; left: 201px;"><nobr>Hand posture and gesture help to augment spoken communication and also provide</nobr></div>
<div style="position: absolute; top: 73240px; left: 201px;"><nobr>communication without speech. With an understanding and classification of how hu-</nobr></div>
<div style="position: absolute; top: 73267px; left: 201px;"><nobr>mans communicate with their hands, researchers can use hand postures and gestures</nobr></div>
<div style="position: absolute; top: 73294px; left: 201px;"><nobr>more effectively as an interface to computer applications. Many hand posture and ges-</nobr></div>
<div style="position: absolute; top: 73321px; left: 201px;"><nobr>ture classification systems and taxonomies have been developed; [29][49][72]; this</nobr></div>
<div style="position: absolute; top: 73348px; left: 201px;"><nobr>appendix describes Sturman’s Whole Hand Input Taxonomy[100], Nespoulous and</nobr></div>
<div style="position: absolute; top: 73374px; left: 201px;"><nobr>Lecours’ Gesture taxonomy[80], and the MIT Advanced Human Interface Group’s</nobr></div>
<div style="position: absolute; top: 73401px; left: 201px;"><nobr>(AHIG) Gesture Classification System[117].</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 73459px; left: 201px;"><nobr><b>B.1 Sturman’s Whole Hand Input Taxonomy</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 73508px; left: 201px;"><nobr>Sturman’s Whole Hand Input Taxonomy[100] is designed as a mapping between cate-</nobr></div>
<div style="position: absolute; top: 73535px; left: 201px;"><nobr>gories of hand actions and their interpretations. According to Sturman, “Hand actions</nobr></div>
<div style="position: absolute; top: 73562px; left: 201px;"><nobr>are defined as position, motion, and forces generated by the hand. The interpretation of</nobr></div>
<div style="position: absolute; top: 73588px; left: 201px;"><nobr>hand actions are the functional interpretation made by the user and/or the applications</nobr></div>
<div style="position: absolute; top: 73615px; left: 201px;"><nobr>of the hand actions.” Hand actions fall into two categories: continuous features and</nobr></div>
<div style="position: absolute; top: 73683px; left: 451px;"><nobr>59</nobr></div>
</span></font>

<div style="position: absolute; top: 73831px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="64"><b>Page 64</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 74020px; left: 201px;"><nobr>discrete features. Continuous features are based on the degrees of freedom of the hand</nobr></div>
<div style="position: absolute; top: 74046px; left: 201px;"><nobr>and include such continuous quantities as fingertip position, joint velocities, and direc-</nobr></div>
<div style="position: absolute; top: 74073px; left: 201px;"><nobr>tion of motion. Hand gestures fall into this category, as do the forces on the pads of</nobr></div>
<div style="position: absolute; top: 74100px; left: 201px;"><nobr>the fingers and palm. Discrete features are based on static values of the features of the</nobr></div>
<div style="position: absolute; top: 74127px; left: 201px;"><nobr>hand. Hand postures, such as a fist or a pointing posture, fall into the discrete feature</nobr></div>
<div style="position: absolute; top: 74154px; left: 201px;"><nobr>category.</nobr></div>
<div style="position: absolute; top: 74181px; left: 223px;"><nobr>Interpretation of hand actions is divided into three categories: direct, mapped, and</nobr></div>
<div style="position: absolute; top: 74208px; left: 201px;"><nobr>symbolic interpretation. In direct interpretation, the user is physically interacting with</nobr></div>
<div style="position: absolute; top: 74234px; left: 201px;"><nobr>the virtual world as if it were the real world; as when users grab a virtual world object</nobr></div>
<div style="position: absolute; top: 74261px; left: 201px;"><nobr>and place it on a virtual table in the same way they would grab a real coffee mug and</nobr></div>
<div style="position: absolute; top: 74288px; left: 201px;"><nobr>place it on a real table. Direct interpretation also includes interaction in which the hand</nobr></div>
<div style="position: absolute; top: 74315px; left: 201px;"><nobr>mimics the actions of the object being controlled. In a mapped interpretation, data from</nobr></div>
<div style="position: absolute; top: 74342px; left: 201px;"><nobr>the hand is mapped to some virtual input device such as a button or slider; as when the</nobr></div>
<div style="position: absolute; top: 74369px; left: 201px;"><nobr>flexion of the index finger to manipulates a slider that changes an interocular distance</nobr></div>
<div style="position: absolute; top: 74396px; left: 201px;"><nobr>parameter for stereoscopic viewing. Finally, in symbolic interpretation, users specify</nobr></div>
<div style="position: absolute; top: 74423px; left: 201px;"><nobr>a hand posture or gesture that is cognitively mapped to some function or task. For</nobr></div>
<div style="position: absolute; top: 74450px; left: 201px;"><nobr>example, a series of hand gestures can signify a token stream used in the recognition</nobr></div>
<div style="position: absolute; top: 74477px; left: 201px;"><nobr>of American Sign Language (ASL) or movement through a virtual environment.</nobr></div>
<div style="position: absolute; top: 74503px; left: 223px;"><nobr>Using the two categories of hand action and the three categories of interpretation,</nobr></div>
<div style="position: absolute; top: 74530px; left: 201px;"><nobr>Sturman derives six categories that classify whole-hand input:</nobr></div>
<div style="position: absolute; top: 74579px; left: 201px;"><nobr><b>Continuous/Direct: </b>Continuous hand data is mapped to a kinematically similar ac-</nobr></div>
<div style="position: absolute; top: 74606px; left: 201px;"><nobr>tion: a graphical hand follows a user’s real hand motion.</nobr></div>
<div style="position: absolute; top: 74654px; left: 201px;"><nobr><b>Continuous/Mapped: </b>Continuoushand data is mapped to some logical input device:</nobr></div>
<div style="position: absolute; top: 74682px; left: 201px;"><nobr>a finger is used to position a mouse cursor.</nobr></div>
<div style="position: absolute; top: 74730px; left: 201px;"><nobr><b>Continuous/Symbolic: </b>The application interprets continuous hand data and maps</nobr></div>
<div style="position: absolute; top: 74758px; left: 201px;"><nobr>it to a particular task: in flying through a virtual environment, the distance between</nobr></div>
<div style="position: absolute; top: 74784px; left: 201px;"><nobr>the two hands determines speed and the vector made by the two hands determines</nobr></div>
<div style="position: absolute; top: 74811px; left: 201px;"><nobr>direction[73].</nobr></div>
<div style="position: absolute; top: 74871px; left: 451px;"><nobr>60</nobr></div>
</span></font>

<div style="position: absolute; top: 75019px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="65"><b>Page 65</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 75207px; left: 201px;"><nobr><b>Discrete/Direct: </b>Discrete hand data or a hand posture is mapped to a directly ma-</nobr></div>
<div style="position: absolute; top: 75234px; left: 201px;"><nobr>nipulative task. Sturman claims that this category is rarely used because it has few</nobr></div>
<div style="position: absolute; top: 75261px; left: 201px;"><nobr>applications.</nobr></div>
<div style="position: absolute; top: 75309px; left: 201px;"><nobr><b>Discrete/Mapped: </b>Discrete hand data is mapped to a discrete activation level: an</nobr></div>
<div style="position: absolute; top: 75337px; left: 201px;"><nobr>object is animated as long as the user makes a fist.</nobr></div>
<div style="position: absolute; top: 75385px; left: 201px;"><nobr><b>Discrete/Symbolic: </b>Discrete hand data is used to generate commands in an applica-</nobr></div>
<div style="position: absolute; top: 75412px; left: 201px;"><nobr>tion: a user makes a halt posture to make an object stop moving.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 75470px; left: 201px;"><nobr><b>B.2 Nespoulous and Lecours’ Gesture Taxonomy</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 75519px; left: 201px;"><nobr>Nespoulous and Lecours’ taxonomy[80] defines and groups different types of gestures</nobr></div>
<div style="position: absolute; top: 75546px; left: 201px;"><nobr>in terms of movement and interpretation (see Figure B.1). This taxonomy is also dis-</nobr></div>
<div style="position: absolute; top: 75573px; left: 201px;"><nobr>cussed in Quek[87]. Gestures are divided into two categories: acts and symbols. Act</nobr></div>
<div style="position: absolute; top: 75600px; left: 201px;"><nobr>gestures are movements that relate directly to the intended interpretation.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 14px; font-family: Times;">
<div style="position: absolute; top: 75690px; left: 336px;"><nobr>Acts</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 11px; font-family: Times;">
<div style="position: absolute; top: 75739px; left: 267px;"><nobr>Mimetic</nobr></div>
<div style="position: absolute; top: 75739px; left: 394px;"><nobr>Deictic</nobr></div>
<div style="position: absolute; top: 75739px; left: 470px;"><nobr>Referential</nobr></div>
<div style="position: absolute; top: 75739px; left: 586px;"><nobr>Modalizing</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 14px; font-family: Times;">
<div style="position: absolute; top: 75690px; left: 505px;"><nobr>Symbols</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 11px; font-family: Times;">
<div style="position: absolute; top: 75779px; left: 418px;"><nobr>Specific</nobr></div>
<div style="position: absolute; top: 75797px; left: 418px;"><nobr>Generic</nobr></div>
<div style="position: absolute; top: 75814px; left: 418px;"><nobr>Metonymic</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 16px; font-family: Times;">
<div style="position: absolute; top: 75642px; left: 415px;"><nobr>Gestures</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 75860px; left: 217px;"><nobr>Figure B.1: Taxonomy of gestures described by Quek (adapted from Quek[87]).</nobr></div>
<div style="position: absolute; top: 75904px; left: 201px;"><nobr>Symbolic gestures are based on movements that require some level of knowledge to</nobr></div>
<div style="position: absolute; top: 75931px; left: 201px;"><nobr>interpret. Symbolic gestures are classified as referential and modalizing. Referential</nobr></div>
<div style="position: absolute; top: 75958px; left: 201px;"><nobr>gestures are gestures that refer to some object or concept; for example, putting a gun</nobr></div>
<div style="position: absolute; top: 75985px; left: 201px;"><nobr>posture up to one’s head often indicates that one has made a foolish mistake. Modaliz-</nobr></div>
<div style="position: absolute; top: 76012px; left: 201px;"><nobr>ing gestures are gestures used in conjunction with another input modality (e.g. speech).</nobr></div>
<div style="position: absolute; top: 76059px; left: 451px;"><nobr>61</nobr></div>
</span></font>

<div style="position: absolute; top: 76207px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="66"><b>Page 66</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 76396px; left: 201px;"><nobr>For example, in asking whether someone had seen a particular person, extending the</nobr></div>
<div style="position: absolute; top: 76422px; left: 201px;"><nobr>hand out at a certain level could indicate the person’s height.</nobr></div>
<div style="position: absolute; top: 76449px; left: 223px;"><nobr>Act gestures are broken up into mimetic and deictic gestures. Mimetic gestures are</nobr></div>
<div style="position: absolute; top: 76476px; left: 201px;"><nobr>those that mimic or pantomime some task to be performed, deictic gestures are gestures</nobr></div>
<div style="position: absolute; top: 76503px; left: 201px;"><nobr>that derive from pointing. They can be further broken up into specific, generic, and</nobr></div>
<div style="position: absolute; top: 76530px; left: 201px;"><nobr>metonymic gestures. All three forms are physically performed in the same fashion but</nobr></div>
<div style="position: absolute; top: 76557px; left: 201px;"><nobr>they are interpreted differently based on context. Specific deictic gestures are made</nobr></div>
<div style="position: absolute; top: 76584px; left: 201px;"><nobr>when the user selects a particular object or location. Generic deictic gestures are used</nobr></div>
<div style="position: absolute; top: 76610px; left: 201px;"><nobr>when pointing to a member of a group of objects with the intention of discussing the</nobr></div>
<div style="position: absolute; top: 76637px; left: 201px;"><nobr>group itself. Metonymic deictic gestures are made when pointingto an object to signify</nobr></div>
<div style="position: absolute; top: 76664px; left: 201px;"><nobr>an entity related to it.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 19px; font-family: Times;">
<div style="position: absolute; top: 76721px; left: 201px;"><nobr><b>B.3 MIT AHIG’s Gesture Classification System</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 76771px; left: 201px;"><nobr>The AHIG gesture classification system was first discussed in Wexelblat[117], is also</nobr></div>
<div style="position: absolute; top: 76798px; left: 201px;"><nobr>indirectly discussed in Cassell[20] and Wilson et al.[118]. AHIG’s classification sys-</nobr></div>
<div style="position: absolute; top: 76824px; left: 201px;"><nobr>tem starts from the idea that previous gesture classification systems, such as Efron’s[29]</nobr></div>
<div style="position: absolute; top: 76851px; left: 201px;"><nobr>and Kendon’s[49], are oriented to the psychological domain and do not necessarily ap-</nobr></div>
<div style="position: absolute; top: 76878px; left: 201px;"><nobr>ply to computer applications. The system has broken five major categories:</nobr></div>
<div style="position: absolute; top: 76920px; left: 231px;"><nobr>Symbolic/modalizing</nobr></div>
<div style="position: absolute; top: 76959px; left: 231px;"><nobr>Pantomimic</nobr></div>
<div style="position: absolute; top: 76998px; left: 231px;"><nobr>Iconic</nobr></div>
<div style="position: absolute; top: 77037px; left: 231px;"><nobr>Deictic/lakoff</nobr></div>
<div style="position: absolute; top: 77076px; left: 231px;"><nobr>Beat/Butterworth’s/Self-adjusters</nobr></div>
<div style="position: absolute; top: 77117px; left: 223px;"><nobr>Symbolic gestures are essentially hand postures used to represent an object or con-</nobr></div>
<div style="position: absolute; top: 77144px; left: 201px;"><nobr>cept, and are always directly mapped to a particular meaning: for instance, the ‘thumbs</nobr></div>
<div style="position: absolute; top: 77171px; left: 201px;"><nobr>up’ posture means that everything is okay. Modalizing gestures (see Appendix B.2</nobr></div>
<div style="position: absolute; top: 77198px; left: 201px;"><nobr>above) are also included in this category.</nobr></div>
<div style="position: absolute; top: 77247px; left: 451px;"><nobr>62</nobr></div>
</span></font>

<div style="position: absolute; top: 77395px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="67"><b>Page 67</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 77584px; left: 223px;"><nobr>Pantomimic gestures involve using the hands to represent a task or interaction with</nobr></div>
<div style="position: absolute; top: 77610px; left: 201px;"><nobr>a physical object. Users making this type of gesture mimic an action they would do</nobr></div>
<div style="position: absolute; top: 77637px; left: 201px;"><nobr>if they were actually interacting in the real world: for example, making a swinging</nobr></div>
<div style="position: absolute; top: 77664px; left: 201px;"><nobr>gesture with one’s hands to indicate hitting a baseball with a bat.</nobr></div>
<div style="position: absolute; top: 77691px; left: 223px;"><nobr>Iconic gestures are gestures that represent an object. The hands become the object</nobr></div>
<div style="position: absolute; top: 77718px; left: 201px;"><nobr>or objects discussed. These gestures usually are performed to act out a particular event</nobr></div>
<div style="position: absolute; top: 77745px; left: 201px;"><nobr>in which the representative object is the focal point such as someone pretending to</nobr></div>
<div style="position: absolute; top: 77772px; left: 201px;"><nobr>drive a car.</nobr></div>
<div style="position: absolute; top: 77798px; left: 223px;"><nobr>Deictic gesture or pointing gestures are used to indicate a particular object, as dis-</nobr></div>
<div style="position: absolute; top: 77825px; left: 201px;"><nobr>cussed in appendix B.2 above. The other type of gesture included in this category</nobr></div>
<div style="position: absolute; top: 77852px; left: 201px;"><nobr>are Lakoff gestures[57], associated verbal utterances that specify a particular metaphor</nobr></div>
<div style="position: absolute; top: 77879px; left: 201px;"><nobr>such as happiness or anger. A gesture usually accompanies these utterances to show</nobr></div>
<div style="position: absolute; top: 77906px; left: 201px;"><nobr>the directionality of the metaphor.</nobr></div>
<div style="position: absolute; top: 77933px; left: 223px;"><nobr>The last category contains three types of gestures: beats, Butterworth’s, and self-</nobr></div>
<div style="position: absolute; top: 77960px; left: 201px;"><nobr>adjusters. Beats are gestures used for emphasis, especially when used with speech.</nobr></div>
<div style="position: absolute; top: 77987px; left: 201px;"><nobr>Beat gestures can help speakers emphasize particular words or concepts and also help</nobr></div>
<div style="position: absolute; top: 78014px; left: 201px;"><nobr>direct the listener’s attention. Butterworth gestures[19] are similar to beats except they</nobr></div>
<div style="position: absolute; top: 78041px; left: 201px;"><nobr>are primarily used to mark unimportant events. The classic example of a Butterworth</nobr></div>
<div style="position: absolute; top: 78067px; left: 201px;"><nobr>gesture is ‘hand waving’ as a placeholder for speaking when one is still thinking about</nobr></div>
<div style="position: absolute; top: 78094px; left: 201px;"><nobr>how to say something. Finally, self-adjusters are gestures people make when they</nobr></div>
<div style="position: absolute; top: 78121px; left: 201px;"><nobr>fidget: for example, when one taps a finger or moves a foot rapidly.</nobr></div>
<div style="position: absolute; top: 78435px; left: 451px;"><nobr>63</nobr></div>
</span></font>

<div style="position: absolute; top: 78583px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="68"><b>Page 68</b></a></font></td></tr></tbody></table></div><font size="5" face="Times"><span style="font-size: 35px; font-family: Times;">
<div style="position: absolute; top: 78891px; left: 201px;"><nobr><b>Bibliography</b></nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79000px; left: 216px;"><nobr>[1] Agarwal, R., and J. Sklansky. Estimating Optical Flow from Clustered Trajecto-</nobr></div>
<div style="position: absolute; top: 79027px; left: 240px;"><nobr>ries in Velocity-Time. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79027px; left: 388px;"><nobr>Proceedings of the 11thIAPR InternationalConference</nobr></div>
<div style="position: absolute; top: 79054px; left: 240px;"><nobr>on Pattern Recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79054px; left: 387px;"><nobr>Vol. #1. Conference A: Computer Vision and Applica-</nobr></div>
<div style="position: absolute; top: 79081px; left: 240px;"><nobr>tions, 215-219, 1992.</nobr></div>
<div style="position: absolute; top: 79117px; left: 216px;"><nobr>[2] Aha, David W., Dennis Kibler, and Marc K. Albert. Instance-based Learning</nobr></div>
<div style="position: absolute; top: 79144px; left: 240px;"><nobr>Algorithms.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79144px; left: 316px;"><nobr>Machine Learning</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79144px; left: 426px;"><nobr>, 6, 37-66, 1991.</nobr></div>
<div style="position: absolute; top: 79180px; left: 216px;"><nobr>[3] American Academy of Orthopedic Surgeons.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79180px; left: 514px;"><nobr>Joint Motion: Method of Measur-</nobr></div>
<div style="position: absolute; top: 79207px; left: 240px;"><nobr>ing and Recording</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79207px; left: 351px;"><nobr>. Churchill Livingstone, New York, 1988.</nobr></div>
<div style="position: absolute; top: 79244px; left: 216px;"><nobr>[4] American Society for Surgery of the Hand.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79244px; left: 505px;"><nobr>The Hand: Examination and Diag-</nobr></div>
<div style="position: absolute; top: 79271px; left: 240px;"><nobr>nosis</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79271px; left: 271px;"><nobr>. Churchill Livingstone, New York, 1978.</nobr></div>
<div style="position: absolute; top: 79308px; left: 216px;"><nobr>[5] Anderson, James A.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79307px; left: 374px;"><nobr>An Introduction to Neural Networks.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79308px; left: 613px;"><nobr>Bradford Books,</nobr></div>
<div style="position: absolute; top: 79334px; left: 240px;"><nobr>Boston, 1995.</nobr></div>
<div style="position: absolute; top: 79371px; left: 216px;"><nobr>[6] Ascension Technology Corporation.The Flock of Birds Installation and Operation</nobr></div>
<div style="position: absolute; top: 79398px; left: 240px;"><nobr>Guide. Burlington, Vermont, 1996.</nobr></div>
<div style="position: absolute; top: 79434px; left: 216px;"><nobr>[7] Auer, T., A. Pinz, and M. Gervautz. Tracking in a Multi-UserAugmented Reality</nobr></div>
<div style="position: absolute; top: 79461px; left: 240px;"><nobr>System. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79461px; left: 309px;"><nobr>Proceedings of the IASTED International Conference on Computer</nobr></div>
<div style="position: absolute; top: 79488px; left: 240px;"><nobr>Graphics and Imaging</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79488px; left: 374px;"><nobr>, 249-253, 1998.</nobr></div>
<div style="position: absolute; top: 79525px; left: 216px;"><nobr>[8] Banarse, D. S. Hand Posture Recognition with the Neocognitron Network.</nobr></div>
<div style="position: absolute; top: 79551px; left: 240px;"><nobr>School of Electronic Engineering and Computer Systems, University College</nobr></div>
<div style="position: absolute; top: 79578px; left: 240px;"><nobr>of North Wales, Bangor, 1993.</nobr></div>
<div style="position: absolute; top: 79623px; left: 451px;"><nobr>64</nobr></div>
</span></font>

<div style="position: absolute; top: 79771px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="69"><b>Page 69</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79960px; left: 216px;"><nobr>[9] Baudel, Thomas, and Michel Beaudouin-Lafon. Charade: Remote Control of</nobr></div>
<div style="position: absolute; top: 79986px; left: 240px;"><nobr>Objects Using Free-Hand Gestures.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 79986px; left: 456px;"><nobr>Communications of the ACM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 79986px; left: 633px;"><nobr>, 36(7):28-35,</nobr></div>
<div style="position: absolute; top: 80013px; left: 240px;"><nobr>1993.</nobr></div>
<div style="position: absolute; top: 80052px; left: 208px;"><nobr>[10] Blake, Andrew, and Michael Isard. 3D Position, Attitude and Shape Input Using</nobr></div>
<div style="position: absolute; top: 80079px; left: 240px;"><nobr>Video Tracking of Hands and Lips. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80079px; left: 482px;"><nobr>Proceedings of SIGGRAPH’94</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80079px; left: 673px;"><nobr>, ACM</nobr></div>
<div style="position: absolute; top: 80106px; left: 240px;"><nobr>Press, 185-192, 1994.</nobr></div>
<div style="position: absolute; top: 80145px; left: 208px;"><nobr>[11] Birk, Henrik, Thomas B. Moeslund, and Claus B. Madsen. Real-Time Recogni-</nobr></div>
<div style="position: absolute; top: 80172px; left: 240px;"><nobr>tion of Hand Alphabet Gestures Using Principal Component Analysis. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80171px; left: 690px;"><nobr>Pro-</nobr></div>
<div style="position: absolute; top: 80198px; left: 240px;"><nobr>ceedings of The 10th Scandinavian Conference on Image Analysis</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80199px; left: 639px;"><nobr>, 1997.</nobr></div>
<div style="position: absolute; top: 80237px; left: 208px;"><nobr>[12] Birk, Henrik and Thomas B. Moeslund. Recognizing Gestures from the</nobr></div>
<div style="position: absolute; top: 80264px; left: 240px;"><nobr>Hand Alphabet Using Principal Component Analysis. Master’s thesis, Aalborg</nobr></div>
<div style="position: absolute; top: 80291px; left: 240px;"><nobr>University, 1996.</nobr></div>
<div style="position: absolute; top: 80330px; left: 208px;"><nobr>[13] Bolt, R.A. Put That There: Voice and Gesture at the Graphics Interface. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80330px; left: 690px;"><nobr>Pro-</nobr></div>
<div style="position: absolute; top: 80357px; left: 240px;"><nobr>ceedings of SIGGRAPH’80</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80357px; left: 408px;"><nobr>, ACM Press, 262-270, 1980.</nobr></div>
<div style="position: absolute; top: 80396px; left: 208px;"><nobr>[14] Brand, Matthew, and Irfan Essa. Causal Analysis for Visual Gesture Understand-</nobr></div>
<div style="position: absolute; top: 80422px; left: 240px;"><nobr>ing. MIT Media Laboratory Perceptual Computing Section Technical Report</nobr></div>
<div style="position: absolute; top: 80449px; left: 240px;"><nobr>No. 327, 1995.</nobr></div>
<div style="position: absolute; top: 80488px; left: 208px;"><nobr>[15] Brand, Matthew. Explanation-Mediated Vision: Making Sense of the World with</nobr></div>
<div style="position: absolute; top: 80515px; left: 240px;"><nobr>Causal Analysis. Ph.D dissertation, Northwestern Univeristy, 1994.</nobr></div>
<div style="position: absolute; top: 80554px; left: 208px;"><nobr>[16] Brand, Matthew, Lawrence Birnbaum, and Paul Cooper. Sensible Scenes: Visual</nobr></div>
<div style="position: absolute; top: 80581px; left: 240px;"><nobr>Understanding of Complex Structures Through Causal Analysis. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80581px; left: 643px;"><nobr>Proceedings</nobr></div>
<div style="position: absolute; top: 80608px; left: 240px;"><nobr>of the 1993 AAAI Conference</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80608px; left: 422px;"><nobr>, 49-56, 1993.</nobr></div>
<div style="position: absolute; top: 80647px; left: 208px;"><nobr>[17] Broomhead, D., and D. Lowe. Multivariable Functional Interpolation and Adap-</nobr></div>
<div style="position: absolute; top: 80674px; left: 240px;"><nobr>tive Networks.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80674px; left: 331px;"><nobr>Complex Systems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80674px; left: 439px;"><nobr>, 2:321-355, 1988.</nobr></div>
<div style="position: absolute; top: 80713px; left: 208px;"><nobr>[18] Bryson, Steve. Virtual Reality in Scientific Visualization.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 80712px; left: 596px;"><nobr>Communications of</nobr></div>
<div style="position: absolute; top: 80739px; left: 240px;"><nobr>the ACM</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 80739px; left: 296px;"><nobr>, 39(5):62-71, 1996.</nobr></div>
<div style="position: absolute; top: 80811px; left: 451px;"><nobr>65</nobr></div>
</span></font>

<div style="position: absolute; top: 80959px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="70"><b>Page 70</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81148px; left: 208px;"><nobr>[19] Butterworth, B., and G. Beattie. Gesture and Silence as Indicators of Planning</nobr></div>
<div style="position: absolute; top: 81174px; left: 240px;"><nobr>in Speech. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 81174px; left: 325px;"><nobr>Recent Advances in the Psychology of Language</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81174px; left: 623px;"><nobr>, Campbell and</nobr></div>
<div style="position: absolute; top: 81201px; left: 240px;"><nobr>Smith (eds.), Plenum Press, New York, 1978.</nobr></div>
<div style="position: absolute; top: 81240px; left: 208px;"><nobr>[20] Cassell, Justine. A Framework for Gesture Generation and Interpretation. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 81267px; left: 240px;"><nobr>Computer Vision in Human-Machine Interaction.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81267px; left: 548px;"><nobr>R. Cipolla and A. Pentland</nobr></div>
<div style="position: absolute; top: 81294px; left: 240px;"><nobr>(eds.), Cambridge University Press, forthcoming.</nobr></div>
<div style="position: absolute; top: 81333px; left: 208px;"><nobr>[21] Charniak, Eugene.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 81332px; left: 354px;"><nobr>Statistical Language Learning.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81333px; left: 539px;"><nobr>MIT Press, Cambridge, 1993.</nobr></div>
<div style="position: absolute; top: 81371px; left: 208px;"><nobr>[22] Cootes, T.F., C.J. Taylor, D.H. Cooper, and J. Graham. Active Shape Models –</nobr></div>
<div style="position: absolute; top: 81398px; left: 240px;"><nobr>Their Training and Applications.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 81398px; left: 443px;"><nobr>Computer Vision and Image Understanding</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81398px; left: 712px;"><nobr>,</nobr></div>
<div style="position: absolute; top: 81425px; left: 240px;"><nobr>61(2), January, 1995.</nobr></div>
<div style="position: absolute; top: 81464px; left: 208px;"><nobr>[23] Cootes, T.F., and C.J. Taylor. Active Shape Models – ‘Smart Snakes’. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 81464px; left: 690px;"><nobr>Pro-</nobr></div>
<div style="position: absolute; top: 81491px; left: 240px;"><nobr>ceedings of the British Machine Vision Conference</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 81491px; left: 550px;"><nobr>, Springer-Verlag, 266-275,</nobr></div>
<div style="position: absolute; top: 81518px; left: 240px;"><nobr>1992.</nobr></div>
<div style="position: absolute; top: 81557px; left: 208px;"><nobr>[24] Davis, James William. Appearance-Based Motion Recognition of Human Actions.</nobr></div>
<div style="position: absolute; top: 81584px; left: 240px;"><nobr>Master’s thesis, Massachusetts Institute of Technology, 1996.</nobr></div>
<div style="position: absolute; top: 81623px; left: 208px;"><nobr>[25] Davis, James, and Mubarak Shah. Gesture Recognition. Technical Report, De-</nobr></div>
<div style="position: absolute; top: 81649px; left: 240px;"><nobr>partment of Computer Science, University of Central Florida, CS-TR-93-11,</nobr></div>
<div style="position: absolute; top: 81676px; left: 240px;"><nobr>1993.</nobr></div>
<div style="position: absolute; top: 81715px; left: 208px;"><nobr>[26] Defanti, Thomas, and Daniel Sandin. Final Report to the National Endowment</nobr></div>
<div style="position: absolute; top: 81742px; left: 240px;"><nobr>of the Arts. US NEA R60-34-163, University of Illinois at Chicago Circle,</nobr></div>
<div style="position: absolute; top: 81769px; left: 240px;"><nobr>Chicago, Illinois, 1977.</nobr></div>
<div style="position: absolute; top: 81808px; left: 208px;"><nobr>[27] Darrell, Trevor J., and Alex P. Pentland. Recognition of Space-Time Gestures</nobr></div>
<div style="position: absolute; top: 81835px; left: 240px;"><nobr>Using a Distributed Representation. MIT Media Laboratory Vision and Model-</nobr></div>
<div style="position: absolute; top: 81862px; left: 240px;"><nobr>ing Group Technical Report No. 197, 1993.</nobr></div>
<div style="position: absolute; top: 81901px; left: 208px;"><nobr>[28] Dorner, Brigitte. Chasing the Colour Glove: Visual Hand Tracking. Master’s</nobr></div>
<div style="position: absolute; top: 81927px; left: 240px;"><nobr>thesis, Simon Fraser University, 1994.</nobr></div>
<div style="position: absolute; top: 81999px; left: 451px;"><nobr>66</nobr></div>
</span></font>

<div style="position: absolute; top: 82147px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="71"><b>Page 71</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 82336px; left: 208px;"><nobr>[29] Efron, D.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 82335px; left: 299px;"><nobr>Gesture and Environments.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 82336px; left: 462px;"><nobr>King’s CrownPress, MorningsideHeights,</nobr></div>
<div style="position: absolute; top: 82362px; left: 240px;"><nobr>New York, 1941.</nobr></div>
<div style="position: absolute; top: 82401px; left: 208px;"><nobr>[30] Eglowstein, Howard. Reach Out and Touch Your Data.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 82401px; left: 590px;"><nobr>Byte</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 82401px; left: 618px;"><nobr>, July, 283-290,</nobr></div>
<div style="position: absolute; top: 82428px; left: 240px;"><nobr>1990.</nobr></div>
<div style="position: absolute; top: 82467px; left: 208px;"><nobr>[31] Encarnaç˜ao, M. A Survey on Input Technology for the Virtual Table Interface</nobr></div>
<div style="position: absolute; top: 82494px; left: 240px;"><nobr>Device. Technical Report, Fraunhofer Center for Research in Computer Graph-</nobr></div>
<div style="position: absolute; top: 82521px; left: 240px;"><nobr>ics, Inc, 1997.</nobr></div>
<div style="position: absolute; top: 82559px; left: 208px;"><nobr>[32] Everitt, B.S.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 82559px; left: 318px;"><nobr>Cluster Analysis.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 82559px; left: 425px;"><nobr>John Wiley and Sons, New York, 1974.</nobr></div>
<div style="position: absolute; top: 82598px; left: 208px;"><nobr>[33] Fakespace. Pinch Glove System Installation Guide and User Handbook, Moun-</nobr></div>
<div style="position: absolute; top: 82625px; left: 240px;"><nobr>tain View, California, 1997.</nobr></div>
<div style="position: absolute; top: 82664px; left: 208px;"><nobr>[34] Fels, Sidney, and Geoffrey Hinton. Glove-TalkII: An Adaptive Gesture-to-</nobr></div>
<div style="position: absolute; top: 82691px; left: 240px;"><nobr>Format Interface. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 82691px; left: 364px;"><nobr>Proceedings of CHI’95 Human Factors in Computing Sys-</nobr></div>
<div style="position: absolute; top: 82718px; left: 240px;"><nobr>tems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 82718px; left: 269px;"><nobr>, ACM Press, 456-463, 1995.</nobr></div>
<div style="position: absolute; top: 82757px; left: 208px;"><nobr>[35] Fels, Sidney. Glove-TalkII: Mapping Hand Gestures to Speech Using Neural</nobr></div>
<div style="position: absolute; top: 82784px; left: 240px;"><nobr>Networks – An Approach to Building Adaptive Interfaces. Ph.D. dissertation,</nobr></div>
<div style="position: absolute; top: 82811px; left: 240px;"><nobr>University of Toronto, 1994.</nobr></div>
<div style="position: absolute; top: 82849px; left: 208px;"><nobr>[36] Fifth Dimension Technologies. http://www.5dt.com/products.html, 1999.</nobr></div>
<div style="position: absolute; top: 82888px; left: 208px;"><nobr>[37] Freeman, William T., and Craig D. Weissman. Television Control by Hand Ges-</nobr></div>
<div style="position: absolute; top: 82915px; left: 240px;"><nobr>tures. Technical Report, Mitsubishi Electronic Research Laboratories, TR-94-</nobr></div>
<div style="position: absolute; top: 82942px; left: 240px;"><nobr>24, 1994.</nobr></div>
<div style="position: absolute; top: 82981px; left: 208px;"><nobr>[38] Fukushima, Kunihiko. Analysis of the Process of Visual Pattern Recognition by</nobr></div>
<div style="position: absolute; top: 83008px; left: 240px;"><nobr>the Neocognitron.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83008px; left: 352px;"><nobr>Neural Networks</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83008px; left: 454px;"><nobr>, 2:413-420, 1989.</nobr></div>
<div style="position: absolute; top: 83047px; left: 208px;"><nobr>[39] General Reality Company. GloveGRASP User’s Guide. San Jose, California,</nobr></div>
<div style="position: absolute; top: 83074px; left: 240px;"><nobr>1996.</nobr></div>
<div style="position: absolute; top: 83112px; left: 208px;"><nobr>[40] Glassner, Andrew.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83112px; left: 353px;"><nobr>Principles of DigitalImage Synthesis</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83112px; left: 574px;"><nobr>. Morgan Kaufman, San</nobr></div>
<div style="position: absolute; top: 83139px; left: 240px;"><nobr>Francisco, 1995.</nobr></div>
<div style="position: absolute; top: 83187px; left: 451px;"><nobr>67</nobr></div>
</span></font>

<div style="position: absolute; top: 83335px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="72"><b>Page 72</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83524px; left: 208px;"><nobr>[41] Grimes, G. Digital Data Entry Glove Interface Device. Bell Telephone Labora-</nobr></div>
<div style="position: absolute; top: 83550px; left: 240px;"><nobr>tories, Murray Hill, New Jersey. US Patent Number 4,414,537, 1983.</nobr></div>
<div style="position: absolute; top: 83589px; left: 208px;"><nobr>[42] Grimson, W.E.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83589px; left: 334px;"><nobr>Object Recognition by Computer: The Role of Geometric Con-</nobr></div>
<div style="position: absolute; top: 83616px; left: 240px;"><nobr>strants.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83616px; left: 287px;"><nobr>MIT Press, Boston, 1990.</nobr></div>
<div style="position: absolute; top: 83655px; left: 208px;"><nobr>[43] Hand, Chris, Ian Sexton, and Michael Mullan. A Linguistic Approach to the</nobr></div>
<div style="position: absolute; top: 83682px; left: 240px;"><nobr>Recognition of Hand Gestures. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83682px; left: 446px;"><nobr>Proceedings of the Designing Future Interac-</nobr></div>
<div style="position: absolute; top: 83708px; left: 240px;"><nobr>tion Conference</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83709px; left: 336px;"><nobr>, University of Warwick, UK, 1994.</nobr></div>
<div style="position: absolute; top: 83747px; left: 208px;"><nobr>[44] Heap, A. J., and F. Samaria. Real-Time Hand Tracking and Gesture Recognition</nobr></div>
<div style="position: absolute; top: 83774px; left: 240px;"><nobr>Using Smart Snakes. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83774px; left: 392px;"><nobr>Proceedings of Interface to Real and Virtual Worlds</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83774px; left: 712px;"><nobr>,</nobr></div>
<div style="position: absolute; top: 83801px; left: 240px;"><nobr>Montpellier, June 1995.</nobr></div>
<div style="position: absolute; top: 83840px; left: 208px;"><nobr>[45] Huang, X. D., Y. Ariki, and M. A. Jack.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83840px; left: 498px;"><nobr>Hidden Markov Models for Speech</nobr></div>
<div style="position: absolute; top: 83867px; left: 240px;"><nobr>Recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83867px; left: 321px;"><nobr>Edinburgh University Press, Edinburgh, 1990.</nobr></div>
<div style="position: absolute; top: 83906px; left: 208px;"><nobr>[46] InterSense. http://www.isense.com, 1999.</nobr></div>
<div style="position: absolute; top: 83945px; left: 208px;"><nobr>[47] Jolliffe, I. T.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 83945px; left: 318px;"><nobr>Principal Component Analysis</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 83945px; left: 501px;"><nobr>. Springer-Verlag, New York, 1986.</nobr></div>
<div style="position: absolute; top: 83984px; left: 208px;"><nobr>[48] Kadous,</nobr></div>
<div style="position: absolute; top: 83984px; left: 308px;"><nobr>Waleed. GRASP: Recognition of Australian Sign Language Using</nobr></div>
<div style="position: absolute; top: 84011px; left: 240px;"><nobr>Instrumented Gloves. Bachelor’s thesis, University of New South Wales, 1995.</nobr></div>
<div style="position: absolute; top: 84049px; left: 208px;"><nobr>[49] Kendon, Adam. Current Issues in the Study of Gesture. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 84049px; left: 588px;"><nobr>The Biological Foun-</nobr></div>
<div style="position: absolute; top: 84076px; left: 240px;"><nobr>dations of Gestures: Motor and Semiotic Aspects.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84076px; left: 563px;"><nobr>Nespoulous, Perron, and</nobr></div>
<div style="position: absolute; top: 84103px; left: 240px;"><nobr>Lecours (eds.), Lawrence Erlbaum Associates, Hillsday, NJ, 1986.</nobr></div>
<div style="position: absolute; top: 84142px; left: 208px;"><nobr>[50] Kessler, G. Drew, Larry H. Hodges, and Neff Walker. Evaluation of the Cyber-</nobr></div>
<div style="position: absolute; top: 84169px; left: 240px;"><nobr>Glove as a Whole Hand Input Device.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 84169px; left: 472px;"><nobr>ACM Transactions on Computer-Human</nobr></div>
<div style="position: absolute; top: 84196px; left: 240px;"><nobr>Interaction</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84196px; left: 306px;"><nobr>, 2(4):263-283, 1995.</nobr></div>
<div style="position: absolute; top: 84235px; left: 208px;"><nobr>[51] Kohler, Marcus. Special Topics of Gesture Recognition Applied to Intelligent</nobr></div>
<div style="position: absolute; top: 84262px; left: 240px;"><nobr>Home Environments. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 84261px; left: 387px;"><nobr>Proceedings of the International Gesture Workshop’97</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84262px; left: 713px;"><nobr>,</nobr></div>
<div style="position: absolute; top: 84289px; left: 240px;"><nobr>Berlin, 285-297, 1997.</nobr></div>
<div style="position: absolute; top: 84375px; left: 451px;"><nobr>68</nobr></div>
</span></font>

<div style="position: absolute; top: 84523px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="73"><b>Page 73</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84712px; left: 208px;"><nobr>[52] Kramer, James. Communication System for Deaf, Deaf-blind, and Non-vocal</nobr></div>
<div style="position: absolute; top: 84738px; left: 240px;"><nobr>Individuals Using Instrumented Gloves. Virtual Technologies, US Patent Num-</nobr></div>
<div style="position: absolute; top: 84765px; left: 240px;"><nobr>ber 5,047,952, 1991.</nobr></div>
<div style="position: absolute; top: 84804px; left: 208px;"><nobr>[53] Kramer, James, and Larry Leifer. The Talking Glove: An Expressive and Re-</nobr></div>
<div style="position: absolute; top: 84831px; left: 240px;"><nobr>ceptive ‘Verbal’ Communication Aid for the Deaf, Deaf-blind, and Non-vocal.</nobr></div>
<div style="position: absolute; top: 84858px; left: 240px;"><nobr>Technical Report, Department of Electrical Engineering, Stanford University,</nobr></div>
<div style="position: absolute; top: 84885px; left: 240px;"><nobr>1989.</nobr></div>
<div style="position: absolute; top: 84924px; left: 208px;"><nobr>[54] Krose, Ben J. A., and P. Patrick van der Smagt.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 84923px; left: 528px;"><nobr>An Introduction to Neural Net-</nobr></div>
<div style="position: absolute; top: 84950px; left: 240px;"><nobr>works.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84951px; left: 284px;"><nobr>University of Amsterdam, 1995.</nobr></div>
<div style="position: absolute; top: 84989px; left: 208px;"><nobr>[55] Krueger, Myron W.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 84989px; left: 361px;"><nobr>Artificial Reality II.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 84989px; left: 484px;"><nobr>Addison-Wesley Publishing Company,</nobr></div>
<div style="position: absolute; top: 85016px; left: 240px;"><nobr>New York, 1991.</nobr></div>
<div style="position: absolute; top: 85055px; left: 208px;"><nobr>[56] Kumo, Yoshinori, Tomoyuki Ishiyama, Kang-Hyun Jo, Nobutaka Shimada and</nobr></div>
<div style="position: absolute; top: 85082px; left: 240px;"><nobr>Yoshiaki Shirai. Vision-Based Human Interface System: Selectively Recogniz-</nobr></div>
<div style="position: absolute; top: 85109px; left: 240px;"><nobr>ing Intentional Hand Gestures. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85109px; left: 457px;"><nobr>Proceedings of the IASTED International</nobr></div>
<div style="position: absolute; top: 85135px; left: 240px;"><nobr>Conference on Computer Graphics and Imaging</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85136px; left: 528px;"><nobr>, 219-223, 1998.</nobr></div>
<div style="position: absolute; top: 85174px; left: 208px;"><nobr>[57] Lakoff, G., and M. Johnson.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85174px; left: 425px;"><nobr>Metaphors We Live By.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85174px; left: 579px;"><nobr>University of Chicago</nobr></div>
<div style="position: absolute; top: 85201px; left: 240px;"><nobr>Press, Chicago, 1980.</nobr></div>
<div style="position: absolute; top: 85240px; left: 208px;"><nobr>[58] LaViola, Joseph. A Multimodal Interface Framework For Using Hand Gestures</nobr></div>
<div style="position: absolute; top: 85267px; left: 240px;"><nobr>and Speech in Virtual Environment Applications. To appear in</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85267px; left: 626px;"><nobr>Proceedings of</nobr></div>
<div style="position: absolute; top: 85294px; left: 240px;"><nobr>the Gesture Workshop’99</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85294px; left: 394px;"><nobr>, Gif-sur-Yvette, 1999.</nobr></div>
<div style="position: absolute; top: 85333px; left: 208px;"><nobr>[59] LaViola, J., and R. Zeleznik. Flex and Pinch: A Case Study of Whole Hand Input</nobr></div>
<div style="position: absolute; top: 85360px; left: 240px;"><nobr>Design for Virtual Environment Interaction. Submitted to</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85360px; left: 583px;"><nobr>IASTED International</nobr></div>
<div style="position: absolute; top: 85387px; left: 240px;"><nobr>Conference on Computer Graphics and Imaging</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85387px; left: 528px;"><nobr>, 1999.</nobr></div>
<div style="position: absolute; top: 85426px; left: 208px;"><nobr>[60] Lee, Christopher, and Yangsheng Xu. Online Interactive Learning of Ges-</nobr></div>
<div style="position: absolute; top: 85452px; left: 240px;"><nobr>tures for Human/Robot Interfaces. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85452px; left: 472px;"><nobr>1996 IEEE International Conference on</nobr></div>
<div style="position: absolute; top: 85479px; left: 240px;"><nobr>Robotics and Automation</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85479px; left: 394px;"><nobr>, vol. 4, 2982-2987, 1996.</nobr></div>
<div style="position: absolute; top: 85563px; left: 451px;"><nobr>69</nobr></div>
</span></font>

<div style="position: absolute; top: 85711px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="74"><b>Page 74</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85900px; left: 208px;"><nobr>[61] Liang, Rung-Huei, and Ming Ouhyoung. A Sign Language Recognition System</nobr></div>
<div style="position: absolute; top: 85926px; left: 240px;"><nobr>Using Hidden Markov Model and Context Sensitive Search. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 85926px; left: 626px;"><nobr>Proceedings of</nobr></div>
<div style="position: absolute; top: 85953px; left: 240px;"><nobr>the ACM Symposium on Virtual Reality Software and Technology’96</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 85953px; left: 673px;"><nobr>, ACM</nobr></div>
<div style="position: absolute; top: 85980px; left: 240px;"><nobr>Press, 59-66, 1996.</nobr></div>
<div style="position: absolute; top: 86019px; left: 208px;"><nobr>[62] Lu, Shan, Seiji Igi, Hideaki Matsuo, and Yuji Nagashima. Towards a Dialogue</nobr></div>
<div style="position: absolute; top: 86046px; left: 240px;"><nobr>System Based on Recognitionand Synthesis of Japanese Sign Language. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86045px; left: 690px;"><nobr>Pro-</nobr></div>
<div style="position: absolute; top: 86072px; left: 240px;"><nobr>ceedings of the International Gesture Workshop’97</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86073px; left: 546px;"><nobr>, Berlin, 259-272, 1997.</nobr></div>
<div style="position: absolute; top: 86112px; left: 208px;"><nobr>[63] Lucente, Mark, Gert-Jan Zwart, and Andrew D. George. Visualization Space: A</nobr></div>
<div style="position: absolute; top: 86139px; left: 240px;"><nobr>Testbed for Deviceless Multimodal User Interface. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86138px; left: 567px;"><nobr>Intelligent Environments</nobr></div>
<div style="position: absolute; top: 86165px; left: 240px;"><nobr>98</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86165px; left: 256px;"><nobr>, AAAI Spring Symposium Series, 87-92, 1998.</nobr></div>
<div style="position: absolute; top: 86204px; left: 208px;"><nobr>[64] Makower, J., M. Parnianpour, and M. Nordin. The Validity Assessment of the</nobr></div>
<div style="position: absolute; top: 86231px; left: 240px;"><nobr>Dextrous Hand Master: A Linkage System for the Measurement of the Joints of</nobr></div>
<div style="position: absolute; top: 86258px; left: 240px;"><nobr>the Hand. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86258px; left: 321px;"><nobr>Abstracts of the First World Congress of Biomechanics</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86258px; left: 656px;"><nobr>. (Volume</nobr></div>
<div style="position: absolute; top: 86285px; left: 240px;"><nobr>#2), La Jolla, California, 338, 1990.</nobr></div>
<div style="position: absolute; top: 86324px; left: 208px;"><nobr>[65] Mann, Steve. Wearable Computing: A First Step Toward Personal Imaging,</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86350px; left: 240px;"><nobr>IEEE Computer</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86351px; left: 337px;"><nobr>, 30(2): 25-32, 1997.</nobr></div>
<div style="position: absolute; top: 86389px; left: 208px;"><nobr>[66] Mapes, Daniel J., and Michael J. Moshell. A Two-Handed Interface for Object</nobr></div>
<div style="position: absolute; top: 86416px; left: 240px;"><nobr>Manipulation in Virtual Environments. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86416px; left: 498px;"><nobr>PRESENSE: Teleoperators and Vir-</nobr></div>
<div style="position: absolute; top: 86443px; left: 240px;"><nobr>tual Environments</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86443px; left: 350px;"><nobr>, 4(4):403-416, 1995.</nobr></div>
<div style="position: absolute; top: 86482px; left: 208px;"><nobr>[67] Marcus, Beth A., and Philip J. Churchill. Sensing Human Hand Motions for</nobr></div>
<div style="position: absolute; top: 86509px; left: 240px;"><nobr>Controlling Dexterous Robots. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86509px; left: 452px;"><nobr>The Second Annual Space Operations Au-</nobr></div>
<div style="position: absolute; top: 86536px; left: 240px;"><nobr>tomation and Robotics Workshop</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86536px; left: 441px;"><nobr>, Wright State University, June 20-23, 1988.</nobr></div>
<div style="position: absolute; top: 86575px; left: 208px;"><nobr>[68] Martin, Jerome, and James L. Crowley. An Appearance-Based Approach to Ges-</nobr></div>
<div style="position: absolute; top: 86602px; left: 240px;"><nobr>ture Recognition. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 86602px; left: 366px;"><nobr>Proceedings of the Ninth International Conference on Im-</nobr></div>
<div style="position: absolute; top: 86628px; left: 240px;"><nobr>age Analysis and Processing</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 86629px; left: 411px;"><nobr>, 340-347, 1997.</nobr></div>
<div style="position: absolute; top: 86667px; left: 208px;"><nobr>[69] Matsuo, Hideaki, Seiji Igi, Shan Lu, Yuji Nagashima, Yuji Takata, and Terutaka</nobr></div>
<div style="position: absolute; top: 86694px; left: 240px;"><nobr>Teshima. The Recognition Algorithm with Non-contact for Japanese Sign Lan-</nobr></div>
<div style="position: absolute; top: 86751px; left: 451px;"><nobr>70</nobr></div>
</span></font>

<div style="position: absolute; top: 86899px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="75"><b>Page 75</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87088px; left: 240px;"><nobr>guage Using Morphological Analysis. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87087px; left: 490px;"><nobr>Proceedings of the International Ges-</nobr></div>
<div style="position: absolute; top: 87114px; left: 240px;"><nobr>ture Workshop’97</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87114px; left: 354px;"><nobr>, Berlin, 273-284, 1997.</nobr></div>
<div style="position: absolute; top: 87151px; left: 208px;"><nobr>[70] Maybeck, Peter S.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87151px; left: 355px;"><nobr>Stochastic Models, Estimation and Control.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87151px; left: 621px;"><nobr>Volume 1, Aca-</nobr></div>
<div style="position: absolute; top: 87178px; left: 240px;"><nobr>demic Press, Inc., 1979.</nobr></div>
<div style="position: absolute; top: 87215px; left: 208px;"><nobr>[71] Mehrotra, Kishan, ChilukuriK. Mohan, and Sanjay Ranka.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87214px; left: 592px;"><nobr>Elements of Artifical</nobr></div>
<div style="position: absolute; top: 87241px; left: 240px;"><nobr>Neural Networks.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87241px; left: 351px;"><nobr>The MIT Press, Boston, 1997.</nobr></div>
<div style="position: absolute; top: 87278px; left: 208px;"><nobr>[72] McNeill, D. and E. Levy. Conceptual Representations in Language Activity and</nobr></div>
<div style="position: absolute; top: 87305px; left: 240px;"><nobr>Gesture. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87305px; left: 310px;"><nobr>Speech, Place, and Action</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87305px; left: 465px;"><nobr>, Jarvella and Klein (eds.), John Wiley and</nobr></div>
<div style="position: absolute; top: 87332px; left: 240px;"><nobr>Sons Ltd., New York, 1982.</nobr></div>
<div style="position: absolute; top: 87369px; left: 208px;"><nobr>[73] Mine, Mark. Moving Objects In Space: Exploiting Proprioception In Virtual</nobr></div>
<div style="position: absolute; top: 87395px; left: 240px;"><nobr>Environment Interaction. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87395px; left: 408px;"><nobr>Proceedings of SIGGRAPH’97</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87395px; left: 595px;"><nobr>, ACM Press, 19-26,</nobr></div>
<div style="position: absolute; top: 87422px; left: 240px;"><nobr>1997.</nobr></div>
<div style="position: absolute; top: 87459px; left: 208px;"><nobr>[74] Mitchell, Tom M.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87459px; left: 350px;"><nobr>Machine Learning.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87459px; left: 467px;"><nobr>McGraw-Hill, Boston, 1997.</nobr></div>
<div style="position: absolute; top: 87496px; left: 208px;"><nobr>[75] Mulder, Axel. Human Movement, Tracking Technology. Technical Report,</nobr></div>
<div style="position: absolute; top: 87523px; left: 240px;"><nobr>School of Kinesiology, Simon Fraser University, 94-1, 1994.</nobr></div>
<div style="position: absolute; top: 87559px; left: 208px;"><nobr>[76] Multigen. SmartScene Video Clip, Discovery Channel’s NextStep program,</nobr></div>
<div style="position: absolute; top: 87586px; left: 240px;"><nobr>1998.</nobr></div>
<div style="position: absolute; top: 87623px; left: 208px;"><nobr>[77] Murakami, Kouichi, and Hitomi Taguchi. Gesture Recognition Using Recurrent</nobr></div>
<div style="position: absolute; top: 87650px; left: 240px;"><nobr>Neural Networks. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87650px; left: 366px;"><nobr>Proceedings of CHI’91 Human Factors in Computing Sys-</nobr></div>
<div style="position: absolute; top: 87677px; left: 240px;"><nobr>tems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87677px; left: 269px;"><nobr>, 237-242, 1991.</nobr></div>
<div style="position: absolute; top: 87713px; left: 208px;"><nobr>[78] Napier, John.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87713px; left: 323px;"><nobr>Hands</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87713px; left: 362px;"><nobr>, Pantheon Books, New York, 1980.</nobr></div>
<div style="position: absolute; top: 87750px; left: 208px;"><nobr>[79] Nam, Yanghee, and KwangYun Wohn. Recognition of Space-Time Hand-</nobr></div>
<div style="position: absolute; top: 87777px; left: 240px;"><nobr>Gestures Using Hidden Markov Model. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87777px; left: 494px;"><nobr>Proceedings of the ACM Symposium</nobr></div>
<div style="position: absolute; top: 87804px; left: 240px;"><nobr>on Virtual Reality Software and Technology’96</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87804px; left: 525px;"><nobr>, ACM Press, 51-58, 1996.</nobr></div>
<div style="position: absolute; top: 87841px; left: 208px;"><nobr>[80] Nespoulous, J., and A. R. Lecours. Gestures, Nature and Function. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 87840px; left: 669px;"><nobr>The Bi-</nobr></div>
<div style="position: absolute; top: 87867px; left: 240px;"><nobr>ological Foundations of Gestures: Motor and Semiotic Aspects.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 87867px; left: 641px;"><nobr>Nespoulous,</nobr></div>
<div style="position: absolute; top: 87894px; left: 240px;"><nobr>Perron, and Lecours (eds.), Lawrence Erlbaum Associates, Hillsday, NJ, 1986.</nobr></div>
<div style="position: absolute; top: 87939px; left: 451px;"><nobr>71</nobr></div>
</span></font>

<div style="position: absolute; top: 88087px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="76"><b>Page 76</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88276px; left: 208px;"><nobr>[81] Newby, Gregory B. Gesture Recognition Using Statistical Similarity. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88275px; left: 690px;"><nobr>Pro-</nobr></div>
<div style="position: absolute; top: 88302px; left: 240px;"><nobr>ceedings of Virtual Reality and Persons with Disabilities</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88302px; left: 579px;"><nobr>, 1993.</nobr></div>
<div style="position: absolute; top: 88341px; left: 208px;"><nobr>[82] Nissho Electronics Corporation. Introduction to SuperGlove. Tokyo, Japan,</nobr></div>
<div style="position: absolute; top: 88368px; left: 240px;"><nobr>1997.</nobr></div>
<div style="position: absolute; top: 88407px; left: 208px;"><nobr>[83] Oviatt, Sharon, and Robert VanGent. Error Resolution During Multimodal</nobr></div>
<div style="position: absolute; top: 88434px; left: 240px;"><nobr>Human-Computer Interaction. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88434px; left: 446px;"><nobr>Proceedings of the International Conference</nobr></div>
<div style="position: absolute; top: 88460px; left: 240px;"><nobr>on Spoken Language Processing</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88461px; left: 435px;"><nobr>, 204-207, 1996.</nobr></div>
<div style="position: absolute; top: 88499px; left: 208px;"><nobr>[84] Papper, M., and M. Gigante. Using Gestures to Control a Virtual Arm. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88499px; left: 674px;"><nobr>Virtual</nobr></div>
<div style="position: absolute; top: 88526px; left: 240px;"><nobr>RealitySystems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88526px; left: 336px;"><nobr>, R. Earnshaw, H. Jones, and M. Gigante (eds.), Academic Press,</nobr></div>
<div style="position: absolute; top: 88553px; left: 240px;"><nobr>London, 1993.</nobr></div>
<div style="position: absolute; top: 88592px; left: 208px;"><nobr>[85] Pausch, Randy. Virtual Reality on Five Dollars a Day. Technical Report CS-91-</nobr></div>
<div style="position: absolute; top: 88619px; left: 240px;"><nobr>21, Department of Computer Science, University of Virginia, 1991.</nobr></div>
<div style="position: absolute; top: 88658px; left: 208px;"><nobr>[86] Polhemus. http://www.polhemus.com, 1999.</nobr></div>
<div style="position: absolute; top: 88697px; left: 208px;"><nobr>[87] Quek, Francis K.H. Toward a Vision-Based Gesture Interface. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88697px; left: 643px;"><nobr>Proceedings</nobr></div>
<div style="position: absolute; top: 88724px; left: 240px;"><nobr>of the ACM Symposium on Virtual Reality Software and Technology’94</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88724px; left: 675px;"><nobr>, ACM</nobr></div>
<div style="position: absolute; top: 88751px; left: 240px;"><nobr>Press, 17-31, 1994.</nobr></div>
<div style="position: absolute; top: 88789px; left: 208px;"><nobr>[88] Rabiner, L. R. A Tutorial on Hidden Markov Models and Selected Applications</nobr></div>
<div style="position: absolute; top: 88816px; left: 240px;"><nobr>in Speech Recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88816px; left: 383px;"><nobr>Proceedings of the IEEE</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88816px; left: 536px;"><nobr>77(2):267-296, 1989.</nobr></div>
<div style="position: absolute; top: 88855px; left: 208px;"><nobr>[89] Rabiner, L. R., and B.H. Juang. An Introduction to Hidden Markov Models.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88882px; left: 240px;"><nobr>IEEE ASSP Magazine</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88882px; left: 375px;"><nobr>, 4-16, January, 1986.</nobr></div>
<div style="position: absolute; top: 88921px; left: 208px;"><nobr>[90] Rangarajan, K., and M. Shah. Establishing Motion Correspondence.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 88921px; left: 667px;"><nobr>CVGIP:</nobr></div>
<div style="position: absolute; top: 88948px; left: 240px;"><nobr>Image Understanding</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 88948px; left: 370px;"><nobr>, 54:56-73, 1991.</nobr></div>
<div style="position: absolute; top: 88987px; left: 208px;"><nobr>[91] Rehag, James M., and Takeo Kanade. DigitEyes: Vision-Based Human Hand</nobr></div>
<div style="position: absolute; top: 89014px; left: 240px;"><nobr>Tracking. Technical Report CMU-CS-93-220, School of Computer Science,</nobr></div>
<div style="position: absolute; top: 89041px; left: 240px;"><nobr>Carnegie Mellon University, 1993.</nobr></div>
<div style="position: absolute; top: 89127px; left: 451px;"><nobr>72</nobr></div>
</span></font>

<div style="position: absolute; top: 89275px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="77"><b>Page 77</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 89464px; left: 208px;"><nobr>[92] Rubine, Dean. Specifing Gestures by Example. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 89463px; left: 582px;"><nobr>Proceedings of SIG-</nobr></div>
<div style="position: absolute; top: 89490px; left: 240px;"><nobr>GRAPH’91</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 89490px; left: 311px;"><nobr>, ACM Press, 329-337, 1991.</nobr></div>
<div style="position: absolute; top: 89529px; left: 208px;"><nobr>[93] Russell, Stuart, and Peter Norvig.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 89529px; left: 448px;"><nobr>Artificial Intelligence: A Modern Approach.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 89556px; left: 240px;"><nobr>Prentice Hall, Englewood Cliffs, NJ, 1995.</nobr></div>
<div style="position: absolute; top: 89595px; left: 208px;"><nobr>[94] Schlenzig, Jennifer, Edward Hunter, and Ramesh Jain. Recursive Spatio-</nobr></div>
<div style="position: absolute; top: 89622px; left: 240px;"><nobr>Temporal Analysis: Understanding Gestures. Technical Report VCL-95-109,</nobr></div>
<div style="position: absolute; top: 89649px; left: 240px;"><nobr>Visual Computing Laboratory, University of California, San Diego, 1995.</nobr></div>
<div style="position: absolute; top: 89687px; left: 208px;"><nobr>[95] Sirovich, I., and M. Kirby. Low-dimensional Procedure for the Characterization</nobr></div>
<div style="position: absolute; top: 89714px; left: 240px;"><nobr>of Human Faces.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 89714px; left: 344px;"><nobr>Journal of the Optical Society of America</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 89714px; left: 589px;"><nobr>, 4(3):519-524,1987.</nobr></div>
<div style="position: absolute; top: 89753px; left: 208px;"><nobr>[96] Starner, Thad, and Alex Pentland. Real-Time American Sign Language Recog-</nobr></div>
<div style="position: absolute; top: 89780px; left: 240px;"><nobr>nition from Video Using Hidden Markov Models. MIT Media Laboratory Per-</nobr></div>
<div style="position: absolute; top: 89807px; left: 240px;"><nobr>ceptual Computing Section Technical Report No. 375, 1996.</nobr></div>
<div style="position: absolute; top: 89846px; left: 208px;"><nobr>[97] Starner, Thad. Visual Recognition of American Sign Language Using Hidden</nobr></div>
<div style="position: absolute; top: 89873px; left: 240px;"><nobr>Markov Models. Master’s thesis, Massachusetts Institute of Technology, 1995.</nobr></div>
<div style="position: absolute; top: 89912px; left: 208px;"><nobr>[98] State, Andrei, Hirota Gentaro, David T. Chen, William F. Garrett, and Mark</nobr></div>
<div style="position: absolute; top: 89939px; left: 240px;"><nobr>A. Livingston. Superior Augmented Reality Registration by Integrating Land-</nobr></div>
<div style="position: absolute; top: 89965px; left: 240px;"><nobr>mark Tracking and Magnetic Tracking. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 89965px; left: 490px;"><nobr>Proceedings of SIGGRAPH’96</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 89965px; left: 676px;"><nobr>, ACM</nobr></div>
<div style="position: absolute; top: 89992px; left: 240px;"><nobr>Press, 429-438, 1996.</nobr></div>
<div style="position: absolute; top: 90031px; left: 208px;"><nobr>[99] Sturman, David J., and David Zeltzer. A Survey of Glove-based Input.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 90031px; left: 683px;"><nobr>IEEE</nobr></div>
<div style="position: absolute; top: 90058px; left: 240px;"><nobr>Computer Graphics and Applications</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90058px; left: 464px;"><nobr>, 14(1):30-39, 1994.</nobr></div>
<div style="position: absolute; top: 90097px; left: 201px;"><nobr>[100] Sturman, David J., and David Zeltzer. A Design Method for ‘Whole-Hand’</nobr></div>
<div style="position: absolute; top: 90124px; left: 240px;"><nobr>Human-Computer Interaction.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 90124px; left: 434px;"><nobr>ACM Transactions on Information Systems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90124px; left: 712px;"><nobr>,</nobr></div>
<div style="position: absolute; top: 90151px; left: 240px;"><nobr>11(3):219-238, 1993.</nobr></div>
<div style="position: absolute; top: 90190px; left: 201px;"><nobr>[101] Sturman, David J. Whole-hand Input. Ph.D dissertation, Massachusetts Institute</nobr></div>
<div style="position: absolute; top: 90217px; left: 240px;"><nobr>of Technology, 1992.</nobr></div>
<div style="position: absolute; top: 90315px; left: 451px;"><nobr>73</nobr></div>
</span></font>

<div style="position: absolute; top: 90463px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="78"><b>Page 78</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90652px; left: 201px;"><nobr>[102] Sturman, David J., David Zeltzer, and Steve Pieper. Hands-on Interaction with</nobr></div>
<div style="position: absolute; top: 90678px; left: 240px;"><nobr>Virtual Environments. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 90678px; left: 393px;"><nobr>Proceedings of the ACM SIGGRAPH Symposium on</nobr></div>
<div style="position: absolute; top: 90705px; left: 240px;"><nobr>User Interface Software and Technology’89</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90705px; left: 502px;"><nobr>, ACM Press, 19-24, 1989.</nobr></div>
<div style="position: absolute; top: 90743px; left: 201px;"><nobr>[103] Su, S. Augustine, and Richard Furuta. A Logical Hand Device in Virtual Envi-</nobr></div>
<div style="position: absolute; top: 90770px; left: 240px;"><nobr>ronments. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 90769px; left: 321px;"><nobr>Proceedings of the ACM Symposium on Virtual Reality Software</nobr></div>
<div style="position: absolute; top: 90796px; left: 240px;"><nobr>and Technology</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90797px; left: 336px;"><nobr>, ACM Press, 33-42, 1994.</nobr></div>
<div style="position: absolute; top: 90834px; left: 201px;"><nobr>[104] Su, S. Augustine. Hand Modeling in Virtual Environment. Master’s Degree</nobr></div>
<div style="position: absolute; top: 90861px; left: 240px;"><nobr>Scholarly Paper, Department of Computer Science, University of Maryland,</nobr></div>
<div style="position: absolute; top: 90888px; left: 240px;"><nobr>College Park, 1993.</nobr></div>
<div style="position: absolute; top: 90926px; left: 201px;"><nobr>[105] Takahashi, Tomoichi, and Fumio Kishino. Hand Gesture Coding Based on Ex-</nobr></div>
<div style="position: absolute; top: 90952px; left: 240px;"><nobr>periments Using a Hand Gesture Interface Device.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 90952px; left: 551px;"><nobr>SIGCHI Bulletin</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 90952px; left: 659px;"><nobr>23(2):67-</nobr></div>
<div style="position: absolute; top: 90979px; left: 240px;"><nobr>73, 1991.</nobr></div>
<div style="position: absolute; top: 91017px; left: 201px;"><nobr>[106] Turk, M., and A. Pentland. Eigenfaces for Recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 91017px; left: 568px;"><nobr>Journal ofNeuroscience</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 91017px; left: 711px;"><nobr>,</nobr></div>
<div style="position: absolute; top: 91044px; left: 240px;"><nobr>3(1):71-86, 1991.</nobr></div>
<div style="position: absolute; top: 91082px; left: 201px;"><nobr>[107] Tung, C. P., and A. C. Kak. Automatic Learning of Assembly Tasks Using a</nobr></div>
<div style="position: absolute; top: 91109px; left: 240px;"><nobr>Dataglove System. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 91109px; left: 376px;"><nobr>Proceedings of the IEEE/RSJ Conference on Intelligent</nobr></div>
<div style="position: absolute; top: 91135px; left: 240px;"><nobr>Robots and Systems</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 91135px; left: 361px;"><nobr>, 1-8, 1995.</nobr></div>
<div style="position: absolute; top: 91173px; left: 201px;"><nobr>[108] Utsumi, Akira, Jun Kurumisawa, Takahiro Otsuka, and Jun Ohya. Direct Ma-</nobr></div>
<div style="position: absolute; top: 91200px; left: 240px;"><nobr>nipulation Scene Creation in 3D. SIGGRAPH’97 Electronic Garden, 1997.</nobr></div>
<div style="position: absolute; top: 91238px; left: 201px;"><nobr>[109] Veltman, S. R., and R. Prasad. Hidden Markov Models Appliedto On-line Hand-</nobr></div>
<div style="position: absolute; top: 91265px; left: 240px;"><nobr>written Isolated Character Recognition.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 91264px; left: 485px;"><nobr>IEEE Transactions on Image Process-</nobr></div>
<div style="position: absolute; top: 91291px; left: 240px;"><nobr>ing</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 91292px; left: 260px;"><nobr>, 314-318, 1994.</nobr></div>
<div style="position: absolute; top: 91329px; left: 201px;"><nobr>[110] Virtual Technologies. http://www.virtex.com/prod CyberGloveTM.html, 1999.</nobr></div>
<div style="position: absolute; top: 91367px; left: 201px;"><nobr>[111] Virtual Technologies. CyberGlove User’s Manual. Palo Alto, California, 1993.</nobr></div>
<div style="position: absolute; top: 91405px; left: 201px;"><nobr>[112] Watson, Richard. A Survey of Gesture Recognition Techniques. Technical Re-</nobr></div>
<div style="position: absolute; top: 91431px; left: 240px;"><nobr>port TCD-CS-93-11, Department of Computer Science, Trinity College Dublin,</nobr></div>
<div style="position: absolute; top: 91458px; left: 240px;"><nobr>1993.</nobr></div>
<div style="position: absolute; top: 91503px; left: 451px;"><nobr>74</nobr></div>
</span></font>

<div style="position: absolute; top: 91651px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="79"><b>Page 79</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 91840px; left: 201px;"><nobr>[113] Welch, Greg, and Gary Bishop. SCAAT: Incremental Tracking with Incomplete</nobr></div>
<div style="position: absolute; top: 91866px; left: 240px;"><nobr>Information. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 91866px; left: 335px;"><nobr>Proceedings of SIGGRAPH’97</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 91866px; left: 523px;"><nobr>, ACM Press, 333-345, 1997.</nobr></div>
<div style="position: absolute; top: 91905px; left: 201px;"><nobr>[114] Welch, Greg, and Gary Bishop. An Introduction to the Kalman Filter. Techni-</nobr></div>
<div style="position: absolute; top: 91932px; left: 240px;"><nobr>cal Report TR 05-041, Department of Computer Science, University of North</nobr></div>
<div style="position: absolute; top: 91959px; left: 240px;"><nobr>Carolina at Chapel Hill, 1995.</nobr></div>
<div style="position: absolute; top: 91998px; left: 201px;"><nobr>[115] Weimer, D. and S. K. Ganapathy. Interaction Techniques Using Hand Tracking</nobr></div>
<div style="position: absolute; top: 92025px; left: 240px;"><nobr>and Speech Recognition. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 92024px; left: 416px;"><nobr>Multimedia Interface Design</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 92025px; left: 592px;"><nobr>, Meera M. Blattner</nobr></div>
<div style="position: absolute; top: 92052px; left: 240px;"><nobr>and Roger B. Dannenberg, (eds.), Addison-Wesley Publishing Company, New</nobr></div>
<div style="position: absolute; top: 92079px; left: 240px;"><nobr>York, 109-126, 1992.</nobr></div>
<div style="position: absolute; top: 92117px; left: 201px;"><nobr>[116] Wexelblat, Alan. An Approach to Natural Gesture in Virtual Environments.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 92144px; left: 240px;"><nobr>ACM Transactions on Computer-Human Interaction</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 92144px; left: 553px;"><nobr>, 2(3):179-200, 1995.</nobr></div>
<div style="position: absolute; top: 92183px; left: 201px;"><nobr>[117] Wexelblat, Alan. A Feature-Based Approach to Continuous-Gesture Analysis.</nobr></div>
<div style="position: absolute; top: 92210px; left: 240px;"><nobr>Master’s thesis, Massachusetts Institute of Technology, 1994.</nobr></div>
<div style="position: absolute; top: 92249px; left: 201px;"><nobr>[118] Wilson, Andrew D., Aaron F. Bobick and Justine Cassell. Recovering the Tem-</nobr></div>
<div style="position: absolute; top: 92276px; left: 240px;"><nobr>poral Structure of Natural Gesture. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 92276px; left: 474px;"><nobr>Proceedings of the Second International</nobr></div>
<div style="position: absolute; top: 92302px; left: 240px;"><nobr>Conference on Automatic Face and Gesture Recognition</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 92302px; left: 579px;"><nobr>, 1996.</nobr></div>
<div style="position: absolute; top: 92341px; left: 201px;"><nobr>[119] Wise, Sam, William Gardner, Eric Sabelman, Erik Valainis, YurikoWong, Karen</nobr></div>
<div style="position: absolute; top: 92368px; left: 240px;"><nobr>Glass, John Drace, and Joseph Rosen. Evaluation of a Fiber Optic Glove for</nobr></div>
<div style="position: absolute; top: 92395px; left: 240px;"><nobr>Semiautomated Goniometric Measurements.</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 92395px; left: 510px;"><nobr>Journal of RehabilitationResearch</nobr></div>
<div style="position: absolute; top: 92422px; left: 240px;"><nobr>and Development</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 92422px; left: 345px;"><nobr>, 27(4): 411-424, 1990.</nobr></div>
<div style="position: absolute; top: 92461px; left: 201px;"><nobr>[120] Youngblut, C., R.E. Johnson, S.H. Nash, R.A. Wienclaw, and C.A. Will. Review</nobr></div>
<div style="position: absolute; top: 92488px; left: 240px;"><nobr>of Virtual Environment Interface Technology. Technical Report IDA Paper P-</nobr></div>
<div style="position: absolute; top: 92515px; left: 240px;"><nobr>3186, Log: H96-001239. Institute for Defense Analysis, 1996.</nobr></div>
<div style="position: absolute; top: 92554px; left: 201px;"><nobr>[121] Zimmerman, Thomas G., Jaron Lanier, Chuck Blanchard, Steve Bryson, and</nobr></div>
<div style="position: absolute; top: 92580px; left: 240px;"><nobr>Young Harvill. A Hand Gesture Interface Device. In</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 13px; font-family: Times;">
<div style="position: absolute; top: 92580px; left: 554px;"><nobr>Proceedings of CHI+GI’87</nobr></div>
<div style="position: absolute; top: 92607px; left: 240px;"><nobr>Human Factors in ComputingSystems and Graphics Interface</nobr></div>
</span></font>
<font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 92607px; left: 607px;"><nobr>, ACM Press, 189-</nobr></div>
<div style="position: absolute; top: 92634px; left: 240px;"><nobr>192, 1987.</nobr></div>
<div style="position: absolute; top: 92691px; left: 451px;"><nobr>75</nobr></div>
</span></font>

<div style="position: absolute; top: 92839px; left: 0pt;"><hr><table border="0" width="100%"><tbody><tr><td align="right" bgcolor="#eeeeee"><font face="arial,sans-serif"><a name="80"><b>Page 80</b></a></font></td></tr></tbody></table></div><font size="3" face="Times"><span style="font-size: 12px; font-family: Times;">
<div style="position: absolute; top: 93028px; left: 201px;"><nobr>[122] Zimmerman, Thomas G. Optical Flex Sensor. US Patent Number 4,542,291,</nobr></div>
<div style="position: absolute; top: 93054px; left: 240px;"><nobr>1985.</nobr></div>
<div style="position: absolute; top: 93879px; left: 451px;"><nobr>76</nobr></div>
</span></font>
</div></body></html>